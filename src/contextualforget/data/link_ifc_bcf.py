"""
IFC-BCF ë§í¬ ìƒì„± ìŠ¤í¬ë¦½íŠ¸ (ê°œì„  ë²„ì „)
GUID ì¶”ì¶œ ê°•í™” + TF-IDF ì˜ë¯¸ì  ë§¤ì¹­ ì¶”ê°€
"""
import argparse
import re
import sys
from pathlib import Path
from typing import List, Dict, Tuple

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# Add src to path for imports
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from contextualforget.core import read_jsonl, write_jsonl


def extract_guid_from_text(text: str) -> List[str]:
    """
    í…ìŠ¤íŠ¸ì—ì„œ GUID íŒ¨í„´ì„ ì¶”ì¶œí•©ë‹ˆë‹¤ (ê°œì„  ë²„ì „).
    
    IFC GUIDëŠ” 22ì Base64 ë³€í˜• ë¬¸ìì—´ì…ë‹ˆë‹¤.
    í—ˆìš© ë¬¸ì: 0-9, A-Z, a-z, _, $
    """
    # IFC GUID íŒ¨í„´: 22ì Base64 ë³€í˜•
    # \bëŠ” í•œêµ­ì–´ì™€ í•¨ê»˜ ì‚¬ìš© ì‹œ ë¬¸ì œê°€ ìˆìœ¼ë¯€ë¡œ ë” ìœ ì—°í•œ íŒ¨í„´ ì‚¬ìš©
    guid_pattern = r'[0-9A-Za-z_$]{22}(?![0-9A-Za-z_$])'
    guids = re.findall(guid_pattern, text)
    return list(set(guids))  # ì¤‘ë³µ ì œê±°

def semantic_match_tfidf(bcf_text: str, ifc_items: Dict[str, Dict]) -> List[Tuple[str, float]]:
    """
    TF-IDF ê¸°ë°˜ ì˜ë¯¸ì  ë§¤ì¹­ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    
    Args:
        bcf_text: BCF ì´ìŠˆì˜ í…ìŠ¤íŠ¸ (title + description)
        ifc_items: GUID â†’ IFC ì—”í‹°í‹° ë§¤í•‘ ë”•ì…”ë„ˆë¦¬
        
    Returns:
        [(guid, similarity_score), ...] íŠœí”Œ ë¦¬ìŠ¤íŠ¸ (ìƒìœ„ 3ê°œ)
    """
    if not bcf_text.strip() or not ifc_items:
        return []
    
    # IFC ì—”í‹°í‹° í…ìŠ¤íŠ¸ ìƒì„±
    ifc_texts = []
    guids = []
    for guid, item in ifc_items.items():
        text = " ".join([
            str(item.get('name', '')),
            str(item.get('type', '')),
            str(item.get('description', ''))
        ])
        if text.strip():
            ifc_texts.append(text)
            guids.append(guid)
    
    if not ifc_texts:
        return []
    
    # TF-IDF ë²¡í„°í™”
    try:
        # min_dfë¥¼ ì‘ì€ ë°ì´í„°ì…‹ì—ì„œë„ ì‘ë™í•˜ë„ë¡ ì¡°ì •
        vectorizer = TfidfVectorizer(
            min_df=1,
            max_df=1.0,  # ëª¨ë“  ë¬¸ì„œ í—ˆìš©
            ngram_range=(1, 2),
            token_pattern=r'(?u)\b\w+\b'  # í•œêµ­ì–´ ì§€ì›
        )
        documents = [bcf_text] + ifc_texts
        tfidf_matrix = vectorizer.fit_transform(documents)
        
        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        similarities = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:]).flatten()
        
        # ìƒìœ„ 3ê°œ ì„ íƒ (ìœ ì‚¬ë„ê°€ 0ë³´ë‹¤ í° ê²½ìš°)
        top_indices = similarities.argsort()[-3:][::-1]
        results = [
            (guids[i], float(similarities[i]))
            for i in top_indices
            if similarities[i] > 0.05  # ì„ê³„ê°’ ë” ë‚®ì¶¤ (0.1 â†’ 0.05)
        ]
        
        # ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ìµœì†Œ 1ê°œëŠ” ë°˜í™˜ (ìœ ì‚¬ë„ê°€ ê°€ì¥ ë†’ì€ ê²ƒ)
        if not results and len(similarities) > 0:
            best_idx = similarities.argmax()
            if similarities[best_idx] > 0:
                results = [(guids[best_idx], float(similarities[best_idx]))]
        
        return results
    except Exception as e:
        print(f"âš ï¸  TF-IDF ë§¤ì¹­ ì‹¤íŒ¨: {e}")
        return []


def semantic_match_keyword(bcf_text: str, ifc_item: Dict) -> float:
    """
    í‚¤ì›Œë“œ ê¸°ë°˜ ì˜ë¯¸ì  ë§¤ì¹­ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤ (fallback).
    
    Args:
        bcf_text: BCF ì´ìŠˆì˜ í…ìŠ¤íŠ¸
        ifc_item: IFC ì—”í‹°í‹°
        
    Returns:
        ë§¤ì¹­ ì ìˆ˜ (0.0 ~ 1.0)
    """
    bcf_lower = bcf_text.lower()
    ifc_name = str(ifc_item.get("name", "")).lower()
    ifc_type = str(ifc_item.get("type", "")).lower()
    
    # í•œì˜ í‚¤ì›Œë“œ ë§¤í•‘
    keywords = {
        "ë²½": ["wall", "ë²½ì²´"],
        "ë¬¸": ["door", "ë¬¸"],
        "ì°½": ["window", "ì°½ë¬¸"],
        "ë°”ë‹¥": ["slab", "floor", "ë°”ë‹¥"],
        "ì²œì¥": ["ceiling", "ì²œì¥"],
        "ê¸°ë‘¥": ["column", "ê¸°ë‘¥"],
        "ë³´": ["beam", "ë³´"],
        "ê³„ë‹¨": ["stair", "ê³„ë‹¨"],
        "ì—˜ë¦¬ë² ì´í„°": ["elevator", "ì—˜ë¦¬ë² ì´í„°"],
        "í™”ì¥ì‹¤": ["toilet", "restroom", "í™”ì¥ì‹¤"],
        "ë¬´ê· ì‹¤": ["clean", "sterile", "ë¬´ê· "],
        "ë°©í™”": ["fire", "ë°©í™”"],
        "í”¼ë‚œ": ["escape", "exit", "í”¼ë‚œ"]
    }
    
    score = 0
    for _category, terms in keywords.items():
        if any(term in bcf_lower for term in terms):
            if any(term in ifc_name or term in ifc_type for term in terms):
                score += 1
    
    # ì •ê·œí™” (ìµœëŒ€ 10ê°œ í‚¤ì›Œë“œ ê°€ì •)
    return min(score / 10.0, 1.0)


def calculate_confidence(match_type: str, score: float = 1.0) -> float:
    """
    ë§¤ì¹­ íƒ€ì…ë³„ ì‹ ë¢°ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    
    Args:
        match_type: 'direct_guid', 'tfidf', 'keyword'
        score: ë§¤ì¹­ ì ìˆ˜ (TF-IDF ìœ ì‚¬ë„ ë˜ëŠ” í‚¤ì›Œë“œ ì ìˆ˜)
        
    Returns:
        ì‹ ë¢°ë„ (0.0 ~ 1.0)
    """
    confidence_map = {
        'direct_guid': 1.0,
        'tfidf_high': min(0.7, score * 0.9),  # ë†’ì€ TF-IDF ì ìˆ˜
        'tfidf_medium': min(0.5, score * 0.7),  # ì¤‘ê°„ TF-IDF ì ìˆ˜
        'keyword': min(0.4, score),  # í‚¤ì›Œë“œ ë§¤ì¹­
    }
    
    if match_type == 'tfidf':
        if score >= 0.5:
            return confidence_map['tfidf_high']
        else:
            return confidence_map['tfidf_medium']
    
    return confidence_map.get(match_type, 0.2)

def main():
    ap = argparse.ArgumentParser(description='IFC-BCF ë§í¬ ìƒì„± (GUID + TF-IDF ë§¤ì¹­)')
    ap.add_argument("--ifc", required=True, help="IFC JSONL íŒŒì¼ ê²½ë¡œ")
    ap.add_argument("--bcf", required=True, help="BCF JSONL íŒŒì¼ ê²½ë¡œ")
    ap.add_argument("--out", required=True, help="ì¶œë ¥ ë§í¬ JSONL íŒŒì¼ ê²½ë¡œ")
    ap.add_argument("--use-tfidf", action="store_true", help="TF-IDF ë§¤ì¹­ ì‚¬ìš©")
    a = ap.parse_args()

    # IFC ë°ì´í„° ë¡œë“œ
    ifc_map = {r["guid"]: r for r in read_jsonl(a.ifc)}
    print(f"âœ… IFC ë°ì´í„° ë¡œë“œ: {len(ifc_map)}ê°œ")
    
    out = []
    bcf_data = list(read_jsonl(a.bcf))
    print(f"âœ… BCF ë°ì´í„° ë¡œë“œ: {len(bcf_data)}ê°œ")
    
    for i, b in enumerate(bcf_data):
        if (i + 1) % 10 == 0:
            print(f"   ì²˜ë¦¬ ì¤‘: {i+1}/{len(bcf_data)}")
        
        # BCF í…ìŠ¤íŠ¸ êµ¬ì„±
        bcf_text = " ".join([
            str(b.get("title", "")),
            str(b.get("description", "")),
            str(b.get("ref", ""))
        ])
        
        # 1. ì§ì ‘ GUID ë§¤ì¹­
        direct_matches = extract_guid_from_text(bcf_text)
        direct_matches = [g for g in direct_matches if g in ifc_map]
        
        match_type = ''
        confidence = 0.2
        final_matches = []
        
        if direct_matches:
            # ì§ì ‘ GUID ë°œê²¬
            final_matches = direct_matches
            match_type = 'direct_guid'
            confidence = calculate_confidence('direct_guid')
        elif a.use_tfidf:
            # 2. TF-IDF ì˜ë¯¸ì  ë§¤ì¹­
            tfidf_matches = semantic_match_tfidf(bcf_text, ifc_map)
            if tfidf_matches:
                final_matches = [guid for guid, score in tfidf_matches]
                avg_score = sum(score for _, score in tfidf_matches) / len(tfidf_matches)
                match_type = 'tfidf'
                confidence = calculate_confidence('tfidf', avg_score)
            else:
                # 3. í‚¤ì›Œë“œ ë§¤ì¹­ (fallback)
                keyword_scores = []
                for guid, ifc_item in ifc_map.items():
                    score = semantic_match_keyword(bcf_text, ifc_item)
                    if score > 0:
                        keyword_scores.append((guid, score))
                
                if keyword_scores:
                    keyword_scores.sort(key=lambda x: x[1], reverse=True)
                    final_matches = [guid for guid, score in keyword_scores[:3]]
                    avg_score = sum(score for _, score in keyword_scores[:3]) / len(keyword_scores[:3])
                    match_type = 'keyword'
                    confidence = calculate_confidence('keyword', avg_score)
        else:
            # TF-IDF ë¯¸ì‚¬ìš©: í‚¤ì›Œë“œ ë§¤ì¹­ë§Œ
            keyword_scores = []
            for guid, ifc_item in ifc_map.items():
                score = semantic_match_keyword(bcf_text, ifc_item)
                if score > 0:
                    keyword_scores.append((guid, score))
            
            if keyword_scores:
                keyword_scores.sort(key=lambda x: x[1], reverse=True)
                final_matches = [guid for guid, score in keyword_scores[:3]]
                avg_score = sum(score for _, score in keyword_scores[:3]) / len(keyword_scores[:3])
                match_type = 'keyword'
                confidence = calculate_confidence('keyword', avg_score)
        
        # ê²°ê³¼ ì €ì¥
        out.append({
            "topic_id": b["topic_id"],
            "guid_matches": final_matches,
            "confidence": confidence,
            "match_type": match_type,
            "evidence": bcf_text[:100] + "..." if len(bcf_text) > 100 else bcf_text
        })
    
    write_jsonl(a.out, out)
    
    # í†µê³„ ì¶œë ¥
    print(f"\nğŸ“Š ë§í¬ ìƒì„± í†µê³„:")
    print(f"   ì´ BCF ì´ìŠˆ: {len(out)}ê°œ")
    
    with_matches = [r for r in out if r['guid_matches']]
    print(f"   ë§¤ì¹­ ì„±ê³µ: {len(with_matches)}ê°œ ({len(with_matches)/len(out)*100:.1f}%)")
    
    avg_confidence = sum(r['confidence'] for r in out) / len(out) if out else 0
    print(f"   í‰ê·  ì‹ ë¢°ë„: {avg_confidence:.3f}")
    
    match_types = {}
    for r in with_matches:
        mt = r.get('match_type', 'unknown')
        match_types[mt] = match_types.get(mt, 0) + 1
    
    print(f"   ë§¤ì¹­ íƒ€ì…:")
    for mt, count in sorted(match_types.items(), key=lambda x: x[1], reverse=True):
        print(f"      {mt}: {count}ê°œ")
    
    print(f"\nâœ… ë§í¬ íŒŒì¼ ì €ì¥: {a.out}")


if __name__ == "__main__":
    main()

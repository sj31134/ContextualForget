"""
Optimization module for ContextualForget

ì´ ëª¨ë“ˆì€ ê·¸ë˜í”„ ì„±ëŠ¥ ìµœì í™”, ì¸ë±ì‹±, ìºì‹±ì„ ë‹´ë‹¹í•©ë‹ˆë‹¤.
"""

import pickle
import time
from datetime import datetime, timedelta
from pathlib import Path
from typing import Any, Dict, List, Optional, Set, Tuple

import networkx as nx
import numpy as np
from dataclasses import dataclass

from ..core.logging import get_logger

logger = get_logger("contextualforget.optimization")


@dataclass
class OptimizationConfig:
    """ìµœì í™” ì„¤ì •"""
    enable_indexing: bool = True
    enable_caching: bool = True
    enable_compression: bool = True
    cache_size: int = 1000
    index_update_interval: int = 300  # 5ë¶„
    compression_level: int = 6


class GraphIndexer:
    """ê·¸ë˜í”„ ì¸ë±ì‹± í´ë˜ìŠ¤"""
    
    def __init__(self, graph: nx.DiGraph):
        self.graph = graph
        self.guid_index = {}
        self.type_index = {}
        self.date_index = {}
        self.author_index = {}
        self.keyword_index = {}
        self._build_indexes()
    
    def _build_indexes(self):
        """ëª¨ë“  ì¸ë±ìŠ¤ êµ¬ì¶•"""
        logger.info("ê·¸ë˜í”„ ì¸ë±ìŠ¤ êµ¬ì¶• ì‹œì‘...")
        
        for node, data in self.graph.nodes(data=True):
            node_id = node
            
            # GUID ì¸ë±ìŠ¤
            if data.get('type') == 'ifc' and 'guid' in data:
                self.guid_index[data['guid']] = node_id
            
            # íƒ€ì… ì¸ë±ìŠ¤
            node_type = data.get('type', 'unknown')
            if node_type not in self.type_index:
                self.type_index[node_type] = []
            self.type_index[node_type].append(node_id)
            
            # ë‚ ì§œ ì¸ë±ìŠ¤
            created_at = data.get('created_at')
            if created_at:
                try:
                    if isinstance(created_at, str):
                        date = datetime.fromisoformat(created_at.replace('Z', '+00:00'))
                    else:
                        date = created_at
                    
                    date_key = date.date()
                    if date_key not in self.date_index:
                        self.date_index[date_key] = []
                    self.date_index[date_key].append(node_id)
                except Exception as e:
                    logger.warning(f"ë‚ ì§œ ì¸ë±ì‹± ì˜¤ë¥˜: {created_at} - {e}")
            
            # ì‘ì„±ì ì¸ë±ìŠ¤
            author = data.get('author')
            if author:
                if author not in self.author_index:
                    self.author_index[author] = []
                self.author_index[author].append(node_id)
            
            # í‚¤ì›Œë“œ ì¸ë±ìŠ¤
            title = data.get('title', '')
            description = data.get('description', '')
            text = f"{title} {description}".lower()
            
            for word in text.split():
                if len(word) > 2:  # 2ê¸€ì ì´ìƒë§Œ ì¸ë±ì‹±
                    if word not in self.keyword_index:
                        self.keyword_index[word] = []
                    self.keyword_index[word].append(node_id)
        
        logger.info(f"ì¸ë±ìŠ¤ êµ¬ì¶• ì™„ë£Œ: GUID({len(self.guid_index)}), íƒ€ì…({len(self.type_index)}), "
                   f"ë‚ ì§œ({len(self.date_index)}), ì‘ì„±ì({len(self.author_index)}), í‚¤ì›Œë“œ({len(self.keyword_index)})")
    
    def find_by_guid(self, guid: str) -> Optional[Tuple]:
        """GUIDë¡œ ë…¸ë“œ ì°¾ê¸°"""
        return self.guid_index.get(guid)
    
    def find_by_type(self, node_type: str) -> List[Tuple]:
        """íƒ€ì…ìœ¼ë¡œ ë…¸ë“œë“¤ ì°¾ê¸°"""
        return self.type_index.get(node_type, [])
    
    def find_by_date_range(self, start_date: datetime, end_date: datetime) -> List[Tuple]:
        """ë‚ ì§œ ë²”ìœ„ë¡œ ë…¸ë“œë“¤ ì°¾ê¸°"""
        result = []
        current_date = start_date.date()
        end_date_only = end_date.date()
        
        while current_date <= end_date_only:
            if current_date in self.date_index:
                result.extend(self.date_index[current_date])
            current_date += timedelta(days=1)
        
        return result
    
    def find_by_author(self, author: str) -> List[Tuple]:
        """ì‘ì„±ìë¡œ ë…¸ë“œë“¤ ì°¾ê¸°"""
        return self.author_index.get(author, [])
    
    def find_by_keywords(self, keywords: List[str]) -> List[Tuple]:
        """í‚¤ì›Œë“œë¡œ ë…¸ë“œë“¤ ì°¾ê¸°"""
        if not keywords:
            return []
        
        # ëª¨ë“  í‚¤ì›Œë“œê°€ í¬í•¨ëœ ë…¸ë“œë“¤ ì°¾ê¸°
        result_sets = []
        for keyword in keywords:
            keyword_lower = keyword.lower()
            matching_nodes = set()
            
            for word, nodes in self.keyword_index.items():
                if keyword_lower in word:
                    matching_nodes.update(nodes)
            
            result_sets.append(matching_nodes)
        
        # êµì§‘í•© ê³„ì‚°
        if result_sets:
            result = result_sets[0]
            for s in result_sets[1:]:
                result = result.intersection(s)
            return list(result)
        
        return []


class GraphCache:
    """ê·¸ë˜í”„ ìºì‹œ í´ë˜ìŠ¤"""
    
    def __init__(self, max_size: int = 1000):
        self.max_size = max_size
        self.cache = {}
        self.access_times = {}
        self.hit_count = 0
        self.miss_count = 0
    
    def get(self, key: str) -> Optional[Any]:
        """ìºì‹œì—ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°"""
        if key in self.cache:
            self.access_times[key] = time.time()
            self.hit_count += 1
            return self.cache[key]
        
        self.miss_count += 1
        return None
    
    def set(self, key: str, value: Any):
        """ìºì‹œì— ë°ì´í„° ì €ì¥"""
        if len(self.cache) >= self.max_size:
            self._evict_oldest()
        
        self.cache[key] = value
        self.access_times[key] = time.time()
    
    def _evict_oldest(self):
        """ê°€ì¥ ì˜¤ë˜ëœ í•­ëª© ì œê±°"""
        if not self.access_times:
            return
        
        oldest_key = min(self.access_times.keys(), key=lambda k: self.access_times[k])
        del self.cache[oldest_key]
        del self.access_times[oldest_key]
    
    def clear(self):
        """ìºì‹œ ì´ˆê¸°í™”"""
        self.cache.clear()
        self.access_times.clear()
        self.hit_count = 0
        self.miss_count = 0
    
    def get_stats(self) -> Dict[str, Any]:
        """ìºì‹œ í†µê³„ ë°˜í™˜"""
        total_requests = self.hit_count + self.miss_count
        hit_rate = self.hit_count / total_requests if total_requests > 0 else 0
        
        return {
            "cache_size": len(self.cache),
            "max_size": self.max_size,
            "hit_count": self.hit_count,
            "miss_count": self.miss_count,
            "hit_rate": hit_rate
        }


class GraphCompressor:
    """ê·¸ë˜í”„ ì••ì¶• í´ë˜ìŠ¤"""
    
    def __init__(self, compression_level: int = 6):
        self.compression_level = compression_level
    
    def compress_graph(self, graph: nx.DiGraph) -> nx.DiGraph:
        """ê·¸ë˜í”„ ì••ì¶•"""
        logger.info("ê·¸ë˜í”„ ì••ì¶• ì‹œì‘...")
        
        compressed_graph = nx.DiGraph()
        
        # ë…¸ë“œ ì••ì¶•
        for node, data in self.graph.nodes(data=True):
            compressed_data = self._compress_node_data(data)
            compressed_graph.add_node(node, **compressed_data)
        
        # ì—£ì§€ ì••ì¶•
        for edge in self.graph.edges(data=True):
            source, target, data = edge
            compressed_data = self._compress_edge_data(data)
            compressed_graph.add_edge(source, target, **compressed_data)
        
        logger.info(f"ê·¸ë˜í”„ ì••ì¶• ì™„ë£Œ: {graph.number_of_nodes()} â†’ {compressed_graph.number_of_nodes()} ë…¸ë“œ")
        return compressed_graph
    
    def _compress_node_data(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """ë…¸ë“œ ë°ì´í„° ì••ì¶•"""
        compressed = {}
        
        # í•„ìˆ˜ í•„ë“œë§Œ ìœ ì§€
        essential_fields = ['type', 'guid', 'name', 'title', 'created_at']
        
        for field in essential_fields:
            if field in data:
                compressed[field] = data[field]
        
        # í…ìŠ¤íŠ¸ í•„ë“œ ì••ì¶•
        if 'description' in data and data['description']:
            description = data['description']
            if len(description) > 200:
                compressed['description'] = description[:200] + "..."
            else:
                compressed['description'] = description
        
        return compressed
    
    def _compress_edge_data(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """ì—£ì§€ ë°ì´í„° ì••ì¶•"""
        compressed = {}
        
        # í•„ìˆ˜ í•„ë“œë§Œ ìœ ì§€
        essential_fields = ['type', 'confidence']
        
        for field in essential_fields:
            if field in data:
                compressed[field] = data[field]
        
        return compressed


class GraphOptimizer:
    """ê·¸ë˜í”„ ìµœì í™” ë©”ì¸ í´ë˜ìŠ¤"""
    
    def __init__(self, config: Optional[OptimizationConfig] = None):
        self.config = config or OptimizationConfig()
        self.indexer: Optional[GraphIndexer] = None
        self.cache = GraphCache(self.config.cache_size)
        self.compressor = GraphCompressor(self.config.compression_level)
        self.optimization_stats = {
            "index_build_time": 0,
            "cache_hit_rate": 0,
            "compression_ratio": 0,
            "last_optimization": None
        }
    
    def optimize_graph(self, graph: nx.DiGraph) -> nx.DiGraph:
        """ê·¸ë˜í”„ ìµœì í™” ìˆ˜í–‰"""
        logger.info("ê·¸ë˜í”„ ìµœì í™” ì‹œì‘...")
        start_time = time.time()
        
        optimized_graph = graph.copy()
        
        # ì¸ë±ì‹±
        if self.config.enable_indexing:
            self._build_indexes(optimized_graph)
        
        # ì••ì¶•
        if self.config.enable_compression:
            optimized_graph = self.compressor.compress_graph(optimized_graph)
        
        # í†µê³„ ì—…ë°ì´íŠ¸
        optimization_time = time.time() - start_time
        self.optimization_stats.update({
            "index_build_time": optimization_time,
            "last_optimization": datetime.now().isoformat()
        })
        
        logger.info(f"ê·¸ë˜í”„ ìµœì í™” ì™„ë£Œ (ì†Œìš”ì‹œê°„: {optimization_time:.2f}ì´ˆ)")
        return optimized_graph
    
    def _build_indexes(self, graph: nx.DiGraph):
        """ì¸ë±ìŠ¤ êµ¬ì¶•"""
        start_time = time.time()
        self.indexer = GraphIndexer(graph)
        build_time = time.time() - start_time
        
        self.optimization_stats["index_build_time"] = build_time
        logger.info(f"ì¸ë±ìŠ¤ êµ¬ì¶• ì™„ë£Œ (ì†Œìš”ì‹œê°„: {build_time:.2f}ì´ˆ)")
    
    def query_with_optimization(self, query_func, query_key: str, *args, **kwargs):
        """ìµœì í™”ëœ ì¿¼ë¦¬ ì‹¤í–‰"""
        # ìºì‹œ í™•ì¸
        if self.config.enable_caching:
            cached_result = self.cache.get(query_key)
            if cached_result is not None:
                return cached_result
        
        # ì¿¼ë¦¬ ì‹¤í–‰
        result = query_func(*args, **kwargs)
        
        # ê²°ê³¼ ìºì‹±
        if self.config.enable_caching:
            self.cache.set(query_key, result)
        
        return result
    
    def get_optimization_stats(self) -> Dict[str, Any]:
        """ìµœì í™” í†µê³„ ë°˜í™˜"""
        cache_stats = self.cache.get_stats()
        
        return {
            **self.optimization_stats,
            "cache_stats": cache_stats,
            "indexer_stats": {
                "guid_index_size": len(self.indexer.guid_index) if self.indexer else 0,
                "type_index_size": len(self.indexer.type_index) if self.indexer else 0,
                "date_index_size": len(self.indexer.date_index) if self.indexer else 0,
                "author_index_size": len(self.indexer.author_index) if self.indexer else 0,
                "keyword_index_size": len(self.indexer.keyword_index) if self.indexer else 0
            }
        }
    
    def save_optimized_graph(self, graph: nx.DiGraph, filepath: str):
        """ìµœì í™”ëœ ê·¸ë˜í”„ ì €ì¥"""
        filepath = Path(filepath)
        filepath.parent.mkdir(parents=True, exist_ok=True)
        
        with filepath.open('wb') as f:
            pickle.dump(graph, f, protocol=pickle.HIGHEST_PROTOCOL)
        
        logger.info(f"ìµœì í™”ëœ ê·¸ë˜í”„ ì €ì¥ ì™„ë£Œ: {filepath}")
    
    def load_optimized_graph(self, filepath: str) -> nx.DiGraph:
        """ìµœì í™”ëœ ê·¸ë˜í”„ ë¡œë“œ"""
        filepath = Path(filepath)
        
        if not filepath.exists():
            raise FileNotFoundError(f"ê·¸ë˜í”„ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {filepath}")
        
        with filepath.open('rb') as f:
            graph = pickle.load(f)
        
        # ì¸ë±ìŠ¤ ì¬êµ¬ì¶•
        if self.config.enable_indexing:
            self._build_indexes(graph)
        
        logger.info(f"ìµœì í™”ëœ ê·¸ë˜í”„ ë¡œë“œ ì™„ë£Œ: {filepath}")
        return graph


def optimize_graph_for_production(graph_path: str, output_path: str, config: Optional[OptimizationConfig] = None):
    """í”„ë¡œë•ì…˜ìš© ê·¸ë˜í”„ ìµœì í™”"""
    logger.info(f"í”„ë¡œë•ì…˜ìš© ê·¸ë˜í”„ ìµœì í™” ì‹œì‘: {graph_path}")
    
    # ê·¸ë˜í”„ ë¡œë“œ
    with open(graph_path, 'rb') as f:
        graph = pickle.load(f)
    
    # ìµœì í™” ìˆ˜í–‰
    optimizer = GraphOptimizer(config)
    optimized_graph = optimizer.optimize_graph(graph)
    
    # ìµœì í™”ëœ ê·¸ë˜í”„ ì €ì¥
    optimizer.save_optimized_graph(optimized_graph, output_path)
    
    # í†µê³„ ì¶œë ¥
    stats = optimizer.get_optimization_stats()
    logger.info(f"ìµœì í™” í†µê³„: {stats}")
    
    return optimizer


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="ContextualForget Graph Optimizer")
    parser.add_argument("--input", required=True, help="Input graph file")
    parser.add_argument("--output", required=True, help="Output graph file")
    parser.add_argument("--config", help="Configuration file (JSON)")
    
    args = parser.parse_args()
    
    # ì„¤ì • ë¡œë“œ
    config = None
    if args.config:
        import json
        with open(args.config) as f:
            config_data = json.load(f)
            config = OptimizationConfig(**config_data)
    
    # ìµœì í™” ìˆ˜í–‰
    optimizer = optimize_graph_for_production(args.input, args.output, config)
    
    print("âœ… ê·¸ë˜í”„ ìµœì í™” ì™„ë£Œ!")
    print(f"ğŸ“Š ìµœì í™” í†µê³„: {optimizer.get_optimization_stats()}")

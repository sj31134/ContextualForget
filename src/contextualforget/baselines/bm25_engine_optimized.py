"""
Optimized BM25 Query Engine with batch processing and memory optimization
"""

import os
import re
import pickle
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional

from whoosh import index
from whoosh.fields import Schema, TEXT, ID, DATETIME, KEYWORD
from whoosh.qparser import QueryParser, MultifieldParser
from whoosh.query import Term, And, Or
from whoosh.analysis import StandardAnalyzer

from .base import BaselineQueryEngine


class BM25QueryEngine(BaselineQueryEngine):
    """Optimized BM25-based query engine with batch processing."""
    
    def __init__(self, name: str = "BM25"):
        super().__init__(name)
        self.ix = None
        self.analyzer = StandardAnalyzer()
        self.schema = Schema(
            doc_id=ID(stored=True, unique=True),
            doc_type=ID(stored=True),
            title=TEXT(stored=True, analyzer=self.analyzer),
            content=TEXT(stored=True, analyzer=self.analyzer),
            author=TEXT(stored=True),
            created=DATETIME(stored=True),
            guid=ID(stored=True),
            entity_type=TEXT(stored=True),
            keywords=KEYWORD(stored=True, commas=True)
        )
    
    def initialize(self, graph_data: dict[str, Any]) -> None:
        """Initialize BM25 engine with optimized indexing."""
        print(f"ğŸ”§ {self.name} ì—”ì§„ ì´ˆê¸°í™” ì¤‘...")
        
        # Create cache directory
        cache_dir = Path("cache/bm25_index")
        cache_dir.mkdir(parents=True, exist_ok=True)
        
        # Check if index exists
        if index.exists_in(str(cache_dir)):
            print("  âœ… ê¸°ì¡´ ì¸ë±ìŠ¤ ë¡œë“œ: BM25")
            self.ix = index.open_dir(str(cache_dir))
        else:
            print("  ğŸ”¨ ìƒˆ ì¸ë±ìŠ¤ ìƒì„±: BM25")
            self.ix = index.create_in(str(cache_dir), self.schema)
            self._build_index_optimized(graph_data)
        
        self.initialized = True
        print(f"  âœ… {self.name} ì—”ì§„ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _build_index_optimized(self, graph_data: dict[str, Any]) -> None:
        """Build BM25 index with optimized batch processing."""
        print("  ğŸ“š ì¸ë±ìŠ¤ êµ¬ì¶• ì¤‘...")
        
        writer = self.ix.writer()
        
        # Handle different data structures
        if 'graph' in graph_data:
            # NetworkX graph object
            graph = graph_data['graph']
            nodes_to_process = list(graph.nodes(data=True))
        elif 'nodes' in graph_data:
            # List of (node, data) tuples
            nodes_to_process = graph_data['nodes']
        else:
            # Direct BCF data list
            nodes_to_process = []
            for item in graph_data.get('bcf_data', []):
                nodes_to_process.append((('BCF', item.get('guid', '')), item))
        
        # Separate BCF and IFC nodes for efficient processing
        bcf_nodes = []
        ifc_nodes = []
        
        for node, data in nodes_to_process:
            if isinstance(node, tuple) and len(node) == 2:
                node_type, node_id = node
                if node_type == "BCF":
                    bcf_nodes.append((node, data))
                elif node_type == "IFC":
                    ifc_nodes.append((node, data))
        
        print(f"    ğŸ“Š BCF ë…¸ë“œ: {len(bcf_nodes)}ê°œ, IFC ë…¸ë“œ: {len(ifc_nodes)}ê°œ")
        
        # Process BCF data first (smaller dataset)
        bcf_count = self._process_bcf_batch(writer, bcf_nodes)
        
        # Process IFC data in batches (larger dataset)
        ifc_count = self._process_ifc_batch(writer, ifc_nodes)
        
        writer.commit()
        print(f"    ğŸ“‹ BCF ë¬¸ì„œ: {bcf_count}ê°œ")
        print(f"    ğŸ“ IFC ë¬¸ì„œ: {ifc_count}ê°œ")
        print(f"    âœ… ì´ {bcf_count + ifc_count}ê°œ ë¬¸ì„œ ì¸ë±ì‹± ì™„ë£Œ")
    
    def _process_bcf_batch(self, writer, bcf_nodes):
        """Process BCF nodes in batch."""
        bcf_count = 0
        for node, data in bcf_nodes:
            try:
                # Extract BCF data
                title = data.get('title', '')
                description = data.get('description', '')
                author = data.get('author', '')
                created = data.get('created', '')
                node_id = node[1]  # Extract node_id from tuple
                
                # Combine title and description
                content = f"{title} {description}".strip()
                
                # Extract keywords (simplified for performance)
                keywords = self._extract_keywords_simple(content)
                
                # Parse creation date
                created_date = None
                if created:
                    try:
                        created_date = datetime.fromisoformat(created.replace('Z', '+00:00'))
                    except Exception:
                        pass
                
                writer.add_document(
                    doc_id=f"bcf_{node_id}",
                    doc_type="BCF",
                    title=title,
                    content=content,
                    author=author,
                    created=created_date,
                    guid=node_id,  # BCFì˜ topic_idë¥¼ guidë¡œ ì‚¬ìš©
                    entity_type="BCF_ISSUE",
                    keywords=",".join(keywords)
                )
                bcf_count += 1
            except Exception as e:
                print(f"    âš ï¸ BCF ë¬¸ì„œ ì¸ë±ì‹± ì˜¤ë¥˜: {e}")
        
        return bcf_count
    
    def _process_ifc_batch(self, writer, ifc_nodes):
        """Process IFC nodes in optimized batches."""
        ifc_count = 0
        batch_size = 1000  # Process in batches of 1000
        
        for i in range(0, len(ifc_nodes), batch_size):
            batch = ifc_nodes[i:i + batch_size]
            
            for node, data in batch:
                try:
                    # Extract IFC data
                    title = data.get('name', data.get('title', ''))
                    entity_type = data.get('entity_type', '')
                    node_id = node[1]  # Extract node_id from tuple
                    guid = data.get('guid', node_id)
                    
                    # Create content from available fields
                    content = f"{title} {entity_type}".strip()
                    
                    # Extract keywords (simplified for performance)
                    keywords = self._extract_keywords_simple(content)
                    
                    writer.add_document(
                        doc_id=f"ifc_{node_id}",
                        doc_type="IFC",
                        title=title,
                        content=content,
                        author="",
                        created=None,
                        guid=guid,
                        entity_type=entity_type,
                        keywords=",".join(keywords)
                    )
                    ifc_count += 1
                except Exception as e:
                    print(f"    âš ï¸ IFC ë¬¸ì„œ ì¸ë±ì‹± ì˜¤ë¥˜: {e}")
            
            # Progress update for large datasets
            if len(ifc_nodes) > 5000:
                progress = (i + batch_size) / len(ifc_nodes) * 100
                print(f"    ğŸ“Š IFC ì¸ë±ì‹± ì§„í–‰ë¥ : {min(progress, 100):.1f}%")
        
        return ifc_count
    
    def _extract_keywords_simple(self, text: str) -> list[str]:
        """Simplified keyword extraction for better performance."""
        if not text:
            return []
        
        # Simple word extraction (Korean + English)
        words = re.findall(r'[ê°€-í£a-zA-Z]+', text.lower())
        
        # Filter out very short words and common stop words
        stop_words = {'the', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'ì´', 'ê·¸', 'ì €', 'ì˜', 'ë¥¼', 'ì„', 'ê°€', 'ì€', 'ëŠ”'}
        keywords = [word for word in words if len(word) > 2 and word not in stop_words]
        
        # Return top 10 keywords to limit memory usage
        return keywords[:10]
    
    def process_query(self, question: str, **kwargs) -> dict[str, Any]:
        """Process a natural language query with optimized search."""
        if not self.initialized:
            return {"error": "Engine not initialized"}
        
        # Extract keywords from question
        keywords = self._extract_keywords_simple(question)
        
        # Determine query type and handle accordingly
        if self._is_guid_query(question):
            return self._handle_guid_query(question)
        elif self._is_temporal_query(question):
            return self._handle_temporal_query(question, keywords)
        elif self._is_author_query(question):
            return self._handle_author_query(question, keywords)
        else:
            return self._handle_general_query(question, keywords)
    
    def _is_guid_query(self, question: str) -> bool:
        """Check if query is asking for GUID information."""
        guid_pattern = r'([A-Za-z0-9]{8}-[A-Za-z0-9]{4}-[A-Za-z0-9]{4}-[A-Za-z0-9]{4}-[A-Za-z0-9]{12})'
        return bool(re.search(guid_pattern, question))
    
    def _is_temporal_query(self, question: str) -> bool:
        """Check if query is temporal (date/time related)."""
        temporal_keywords = ['ìµœê·¼', 'ì´ì „', 'ìƒì„±', 'ë‚ ì§œ', 'ì¼', 'ì£¼', 'ì›”', 'ë…„', 'recent', 'ago', 'created', 'date']
        return any(keyword in question.lower() for keyword in temporal_keywords)
    
    def _is_author_query(self, question: str) -> bool:
        """Check if query is asking about author."""
        author_keywords = ['ì‘ì„±', 'author', 'engineer', 'architect']
        return any(keyword in question.lower() for keyword in author_keywords)
    
    def _handle_guid_query(self, question: str) -> dict[str, Any]:
        """Handle GUID-specific queries."""
        # Extract GUID from question (UUID í˜•ì‹ ì§€ì›)
        guid_pattern = r'([A-Za-z0-9]{8}-[A-Za-z0-9]{4}-[A-Za-z0-9]{4}-[A-Za-z0-9]{4}-[A-Za-z0-9]{12})'
        match = re.search(guid_pattern, question)
        
        if not match:
            return {"error": "GUID not found in question"}
        
        guid = match.group(1)
        
        # Search for element with this GUID
        with self.ix.searcher() as searcher:
            query = Term("guid", guid)
            results = searcher.search(query, limit=1)
            
            if results:
                hit = results[0]
                return {
                    "answer": f"GUID {guid}ëŠ” {hit['entity_type']} íƒ€ì…ì˜ ìš”ì†Œì…ë‹ˆë‹¤.",
                    "confidence": 1.0,
                    "result_count": 1,
                    "entities": [guid],  # ì‹¤ì œ GUID ë°˜í™˜
                    "source": "BM25",
                    "details": {
                        "guid": guid,
                        "entity_type": hit['entity_type'],
                        "name": hit['title']
                    }
                }
            else:
                return {
                    "answer": f"GUID {guid}ì— ëŒ€í•œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                    "confidence": 0.0,
                    "result_count": 0,
                    "entities": [],
                    "source": "BM25"
                }
    
    def _handle_temporal_query(self, question: str, keywords: list[str]) -> dict[str, Any]:
        """Handle temporal queries."""
        with self.ix.searcher() as searcher:
            # Search for recent documents
            query = MultifieldParser(["title", "content"], self.ix.schema).parse(" ".join(keywords))
            results = searcher.search(query, limit=10)
            
            if results:
                answer = f"ìµœê·¼ ê´€ë ¨ ë¬¸ì„œ {len(results)}ê°œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤."
                entities = [hit['guid'] for hit in results if hit.get('guid')]
                return {
                    "answer": answer,
                    "confidence": 0.7,
                    "result_count": len(results),
                    "entities": entities,
                    "source": "BM25",
                    "details": {"temporal_query": True, "keywords": keywords}
                }
            else:
                return {
                    "answer": "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                    "confidence": 0.0,
                    "result_count": 0,
                    "entities": [],
                    "source": "BM25"
                }
    
    def _handle_author_query(self, question: str, keywords: list[str]) -> dict[str, Any]:
        """Handle author-specific queries."""
        with self.ix.searcher() as searcher:
            # Search by author field
            query = MultifieldParser(["author", "title", "content"], self.ix.schema).parse(" ".join(keywords))
            results = searcher.search(query, limit=10)
            
            if results:
                answer = f"ì‘ì„±ì ê´€ë ¨ ë¬¸ì„œ {len(results)}ê°œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤."
                entities = [hit['guid'] for hit in results if hit.get('guid')]
                return {
                    "answer": answer,
                    "confidence": 0.6,
                    "result_count": len(results),
                    "entities": entities,
                    "source": "BM25",
                    "details": {"author_query": True, "keywords": keywords}
                }
            else:
                return {
                    "answer": "ì‘ì„±ì ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                    "confidence": 0.0,
                    "result_count": 0,
                    "entities": [],
                    "source": "BM25"
                }
    
    def _handle_general_query(self, question: str, keywords: list[str]) -> dict[str, Any]:
        """Handle general queries."""
        with self.ix.searcher() as searcher:
            # Multi-field search
            query = MultifieldParser(["title", "content", "keywords"], self.ix.schema).parse(" ".join(keywords))
            results = searcher.search(query, limit=20)
            
            if results:
                # Separate BCF and IFC results
                bcf_results = [hit for hit in results if hit['doc_type'] == 'BCF']
                ifc_results = [hit for hit in results if hit['doc_type'] == 'IFC']
                
                answer = f"ê´€ë ¨ ë¬¸ì„œ {len(results)}ê°œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤. (BCF: {len(bcf_results)}ê°œ, IFC: {len(ifc_results)}ê°œ)"
                
                # Extract entity IDs (ì‹¤ì œ GUID ì¶”ì¶œ)
                entities = []
                for hit in results:
                    if hit.get('doc_type') == 'IFC':
                        entities.append(hit.get('guid', ''))
                    else:
                        entities.append(hit.get('doc_id', ''))
                
                return {
                    "answer": answer,
                    "confidence": 0.6,
                    "result_count": len(results),
                    "entities": entities,
                    "source": "BM25",
                    "details": {
                        "bcf_count": len(bcf_results),
                        "ifc_count": len(ifc_results),
                        "total_results": len(results)
                    }
                }
            else:
                return {
                    "answer": "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                    "confidence": 0.0,
                    "result_count": 0,
                    "entities": [],
                    "source": "BM25"
                }

# ContextualForget ì—°êµ¬ ë…¼ë¬¸ ì‘ì„± ë¡œë“œë§µ

## ğŸ¯ ëª©í‘œ: Top-tier í•™íšŒ Full Paper íˆ¬ê³  (ICSE, ASE, MSR, ECPPM)

**ì´ ê¸°ê°„**: 9-10ì£¼ (2025ë…„ 10ì›” 9ì¼ ~ 12ì›” 15ì¼)  
**íˆ¬ê³  ëª©í‘œ**: ICSE 2026 (Research Track) ë˜ëŠ” ASE 2025

---

## ğŸ“… ì „ì²´ ì¼ì • ê°œìš”

```
Week 1-2  : Phase 1 - í‰ê°€ ì‹œìŠ¤í…œ êµ¬ì¶•
Week 3-4  : Phase 2 - ë¹„êµêµ° êµ¬í˜„
Week 5-6  : Phase 3 - ë§ê° ì •ì±… ì™„ì„±
Week 7-9  : Phase 4 - ì‹¤í—˜ & ë¶„ì„
Week 10   : Phase 4 - ë…¼ë¬¸ ì‘ì„± & íˆ¬ê³ 
```

---

## ğŸ“‹ Phase 1: í‰ê°€ ì‹œìŠ¤í…œ êµ¬ì¶• (Week 1-2)

**ëª©í‘œ**: Gold standard ë°ì´í„°ì…‹ê³¼ ìë™í™”ëœ ë²¤ì¹˜ë§ˆí¬ ì‹œìŠ¤í…œ êµ¬ì¶•

### Week 1 (Oct 9-15)

#### âœ… Task 1.1: Gold Standard QA ë°ì´í„°ì…‹ ìƒì„±
**ì˜ˆìƒ ì‹œê°„**: 3-4ì¼

**ì„¸ë¶€ ì‘ì—…**:
1. **QA ìŒ ì„¤ê³„ (30ê°œ)**
   ```yaml
   ì¹´í…Œê³ ë¦¬ë³„ ë¶„í¬:
   - ë‹¨ìˆœ ì‚¬ì‹¤ ì§ˆì˜ (10ê°œ): "GUID xxxì˜ ì´ë¦„ì€?"
   - ê´€ê³„ ì§ˆì˜ (10ê°œ): "GUID xxxì™€ ì—°ê²°ëœ BCF ì´ìŠˆëŠ”?"
   - ì‹œê°„ ì§ˆì˜ (5ê°œ): "ìµœê·¼ 30ì¼ ì´ë‚´ ìƒì„±ëœ ì´ìŠˆëŠ”?"
   - ë³€ê²½ ì˜í–¥ ì§ˆì˜ (5ê°œ): "L3 Corridorì—ì„œ ë¬´ì—‡ì´ ë°”ë€Œì—ˆë‚˜?"
   ```

2. **ê° QAì— ëŒ€í•œ Gold Answer ì‘ì„±**
   ```json
   {
     "question": "GUID 1kTvXnbbzCWw8lcMd1dR4oì™€ ê´€ë ¨ëœ ìµœê·¼ ì´ìŠˆëŠ”?",
     "gold_answer": "ì´ 2ê°œì˜ ì´ìŠˆ: ë²½ì²´ ë‘ê»˜ ë¶ˆì¼ì¹˜, ì°½í˜¸ ìœ„ì¹˜ í™•ì¸ í•„ìš”",
     "gold_evidence": [
       {"type": "bcf", "id": "topic-001", "guid": "..."},
       {"type": "bcf", "id": "topic-002", "guid": "..."}
     ],
     "difficulty": "medium",
     "category": "relationship"
   }
   ```

3. **ë³µì¡ë„ë³„ ì¶”ê°€ ìƒì„± (20ê°œ)**
   - Easy (10ê°œ): ë‹¨ì¼ ë…¸ë“œ ê²€ìƒ‰
   - Medium (5ê°œ): 1-hop ê´€ê³„
   - Hard (5ê°œ): Multi-hop ì¶”ë¡ 

4. **ë°ì´í„°ì…‹ ê²€ì¦**
   - ë…ë¦½ì ì¸ ê²€ì¦ìê°€ ë‹µë³€ í™•ì¸
   - ëª¨í˜¸í•œ ì§ˆë¬¸ ì œê±°/ìˆ˜ì •

**ì‚°ì¶œë¬¼**:
- `eval/gold_standard_qa.jsonl` (50ê°œ QA ìŒ)
- `eval/dataset_statistics.json` (í†µê³„ ì •ë³´)

---

#### âœ… Task 1.2: í‰ê°€ ë©”íŠ¸ë¦­ êµ¬í˜„
**ì˜ˆìƒ ì‹œê°„**: 3-4ì¼

**ì„¸ë¶€ ì‘ì—…**:
1. **ì •ë‹µ ì •í™•ë„ ë©”íŠ¸ë¦­**
   ```python
   # eval/metrics/answer_metrics.py
   
   def exact_match(predicted: str, gold: str) -> float:
       """ì™„ì „ ì¼ì¹˜"""
       return 1.0 if predicted.strip() == gold.strip() else 0.0
   
   def f1_score(predicted: str, gold: str) -> float:
       """Token-level F1"""
       pred_tokens = set(tokenize(predicted))
       gold_tokens = set(tokenize(gold))
       
       if not pred_tokens or not gold_tokens:
           return 0.0
       
       precision = len(pred_tokens & gold_tokens) / len(pred_tokens)
       recall = len(pred_tokens & gold_tokens) / len(gold_tokens)
       
       if precision + recall == 0:
           return 0.0
       
       return 2 * precision * recall / (precision + recall)
   
   def bleu_score(predicted: str, gold: str) -> float:
       """BLEU score for answer quality"""
       from nltk.translate.bleu_score import sentence_bleu
       return sentence_bleu([tokenize(gold)], tokenize(predicted))
   ```

2. **ê·¼ê±° ì •í™•ë„ ë©”íŠ¸ë¦­**
   ```python
   # eval/metrics/attribution_metrics.py
   
   def attribution_precision(predicted_evidence: List[str], 
                            gold_evidence: List[str]) -> float:
       """ì˜ˆì¸¡ëœ ê·¼ê±° ì¤‘ ì •ë‹µ ê·¼ê±°ì˜ ë¹„ìœ¨"""
       if not predicted_evidence:
           return 0.0
       
       correct = len(set(predicted_evidence) & set(gold_evidence))
       return correct / len(predicted_evidence)
   
   def attribution_recall(predicted_evidence: List[str], 
                         gold_evidence: List[str]) -> float:
       """ì •ë‹µ ê·¼ê±° ì¤‘ ì˜ˆì¸¡ëœ ê·¼ê±°ì˜ ë¹„ìœ¨"""
       if not gold_evidence:
           return 1.0  # No evidence required
       
       correct = len(set(predicted_evidence) & set(gold_evidence))
       return correct / len(gold_evidence)
   
   def attribution_f1(predicted_evidence: List[str], 
                     gold_evidence: List[str]) -> float:
       """Attribution F1 score"""
       precision = attribution_precision(predicted_evidence, gold_evidence)
       recall = attribution_recall(predicted_evidence, gold_evidence)
       
       if precision + recall == 0:
           return 0.0
       
       return 2 * precision * recall / (precision + recall)
   ```

3. **ì„±ëŠ¥ ë©”íŠ¸ë¦­**
   ```python
   # eval/metrics/performance_metrics.py
   
   def measure_latency(query_func: Callable, 
                      query: str, 
                      repetitions: int = 10) -> Dict[str, float]:
       """ì¿¼ë¦¬ ì§€ì—° ì‹œê°„ ì¸¡ì •"""
       latencies = []
       for _ in range(repetitions):
           start = time.time()
           query_func(query)
           latencies.append(time.time() - start)
       
       return {
           'mean': np.mean(latencies),
           'median': np.median(latencies),
           'std': np.std(latencies),
           'p95': np.percentile(latencies, 95),
           'p99': np.percentile(latencies, 99)
       }
   
   def measure_index_size(graph_path: str) -> Dict[str, Any]:
       """ì¸ë±ìŠ¤ í¬ê¸° ì¸¡ì •"""
       import pickle
       with open(graph_path, 'rb') as f:
           graph = pickle.load(f)
       
       return {
           'nodes': graph.number_of_nodes(),
           'edges': graph.number_of_edges(),
           'disk_size_mb': os.path.getsize(graph_path) / 1024 / 1024
       }
   ```

**ì‚°ì¶œë¬¼**:
- `src/contextualforget/eval/metrics/__init__.py`
- `src/contextualforget/eval/metrics/answer_metrics.py`
- `src/contextualforget/eval/metrics/attribution_metrics.py`
- `src/contextualforget/eval/metrics/performance_metrics.py`
- ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ (`tests/test_eval_metrics.py`)

---

### Week 2 (Oct 16-22)

#### âœ… Task 1.3: ìë™í™” ë²¤ì¹˜ë§ˆí¬ ìŠ¤í¬ë¦½íŠ¸
**ì˜ˆìƒ ì‹œê°„**: 5-7ì¼

**ì„¸ë¶€ ì‘ì—…**:
1. **í†µí•© ë²¤ì¹˜ë§ˆí¬ í´ë˜ìŠ¤**
   ```python
   # eval/benchmark.py
   
   class RAGBenchmark:
       """í†µí•© RAG ë²¤ì¹˜ë§ˆí¬ ì‹œìŠ¤í…œ"""
       
       def __init__(self, 
                    gold_qa_path: str,
                    output_dir: str = "results/benchmark"):
           self.qa_pairs = self.load_gold_qa(gold_qa_path)
           self.output_dir = Path(output_dir)
           self.output_dir.mkdir(parents=True, exist_ok=True)
       
       def evaluate_method(self, 
                          method_name: str,
                          query_func: Callable,
                          evidence_func: Callable) -> Dict[str, Any]:
           """ë‹¨ì¼ RAG ë°©ë²• í‰ê°€"""
           
           results = {
               'method': method_name,
               'timestamp': datetime.now().isoformat(),
               'qa_results': [],
               'aggregate_metrics': {}
           }
           
           answer_scores = []
           attribution_scores = []
           latencies = []
           
           for qa in tqdm(self.qa_pairs, desc=f"Evaluating {method_name}"):
               # ì¿¼ë¦¬ ì‹¤í–‰
               start_time = time.time()
               predicted_answer = query_func(qa['question'])
               predicted_evidence = evidence_func(qa['question'])
               latency = time.time() - start_time
               
               # ì •ë‹µ í‰ê°€
               em = exact_match(predicted_answer, qa['gold_answer'])
               f1 = f1_score(predicted_answer, qa['gold_answer'])
               bleu = bleu_score(predicted_answer, qa['gold_answer'])
               
               # ê·¼ê±° í‰ê°€
               attr_p = attribution_precision(
                   predicted_evidence, qa['gold_evidence']
               )
               attr_r = attribution_recall(
                   predicted_evidence, qa['gold_evidence']
               )
               attr_f1 = attribution_f1(
                   predicted_evidence, qa['gold_evidence']
               )
               
               # ê²°ê³¼ ì €ì¥
               qa_result = {
                   'question_id': qa['id'],
                   'question': qa['question'],
                   'predicted_answer': predicted_answer,
                   'gold_answer': qa['gold_answer'],
                   'metrics': {
                       'exact_match': em,
                       'f1': f1,
                       'bleu': bleu,
                       'attribution_precision': attr_p,
                       'attribution_recall': attr_r,
                       'attribution_f1': attr_f1,
                       'latency_ms': latency * 1000
                   }
               }
               results['qa_results'].append(qa_result)
               
               answer_scores.append(f1)
               attribution_scores.append(attr_f1)
               latencies.append(latency)
           
           # ì§‘ê³„ ë©”íŠ¸ë¦­
           results['aggregate_metrics'] = {
               'mean_f1': np.mean(answer_scores),
               'mean_attribution_f1': np.mean(attribution_scores),
               'mean_latency_ms': np.mean(latencies) * 1000,
               'median_latency_ms': np.median(latencies) * 1000,
               'p95_latency_ms': np.percentile(latencies, 95) * 1000
           }
           
           # ê²°ê³¼ ì €ì¥
           output_file = self.output_dir / f"{method_name}_results.json"
           with open(output_file, 'w') as f:
               json.dump(results, f, indent=2, ensure_ascii=False)
           
           return results
       
       def compare_methods(self, results_list: List[Dict]) -> pd.DataFrame:
           """ì—¬ëŸ¬ ë°©ë²• ë¹„êµ"""
           comparison = []
           
           for result in results_list:
               comparison.append({
                   'Method': result['method'],
                   'F1 Score': result['aggregate_metrics']['mean_f1'],
                   'Attribution F1': result['aggregate_metrics']['mean_attribution_f1'],
                   'Latency (ms)': result['aggregate_metrics']['mean_latency_ms']
               })
           
           df = pd.DataFrame(comparison)
           
           # ì‹œê°í™”
           self.plot_comparison(df)
           
           return df
       
       def plot_comparison(self, df: pd.DataFrame):
           """ê²°ê³¼ ì‹œê°í™”"""
           fig, axes = plt.subplots(1, 3, figsize=(15, 5))
           
           # F1 Score
           df.plot(x='Method', y='F1 Score', kind='bar', ax=axes[0])
           axes[0].set_title('Answer F1 Score')
           axes[0].set_ylim(0, 1)
           
           # Attribution F1
           df.plot(x='Method', y='Attribution F1', kind='bar', ax=axes[1])
           axes[1].set_title('Attribution F1 Score')
           axes[1].set_ylim(0, 1)
           
           # Latency
           df.plot(x='Method', y='Latency (ms)', kind='bar', ax=axes[2])
           axes[2].set_title('Query Latency')
           
           plt.tight_layout()
           plt.savefig(self.output_dir / 'comparison.png', dpi=300)
   ```

2. **ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸**
   ```python
   # scripts/run_benchmark.py
   
   def main():
       # ë²¤ì¹˜ë§ˆí¬ ì´ˆê¸°í™”
       benchmark = RAGBenchmark('eval/gold_standard_qa.jsonl')
       
       # í˜„ì¬ Graph-RAG í‰ê°€
       print("Evaluating Graph-RAG...")
       graph_rag_results = benchmark.evaluate_method(
           method_name='Graph-RAG',
           query_func=graph_rag_query,
           evidence_func=graph_rag_evidence
       )
       
       # ê²°ê³¼ ì¶œë ¥
       print("\n=== Graph-RAG Results ===")
       print(f"F1 Score: {graph_rag_results['aggregate_metrics']['mean_f1']:.3f}")
       print(f"Attribution F1: {graph_rag_results['aggregate_metrics']['mean_attribution_f1']:.3f}")
       print(f"Latency: {graph_rag_results['aggregate_metrics']['mean_latency_ms']:.2f} ms")
   ```

**ì‚°ì¶œë¬¼**:
- `eval/benchmark.py`
- `scripts/run_benchmark.py`
- `results/benchmark/` (ê²°ê³¼ ë””ë ‰í† ë¦¬)
- ì‹¤í–‰ ë¬¸ì„œ (`docs/benchmark_guide.md`)

---

## ğŸ“‹ Phase 2: ë¹„êµêµ° êµ¬í˜„ (Week 3-4)

**ëª©í‘œ**: í‚¤ì›Œë“œ RAG ë° ë²¡í„° RAG ë¹„êµêµ° êµ¬í˜„

### Week 3 (Oct 23-29)

#### âœ… Task 2.1: í‚¤ì›Œë“œ RAG (BM25) êµ¬í˜„
**ì˜ˆìƒ ì‹œê°„**: 5-7ì¼

**ì„¸ë¶€ ì‘ì—…**:
1. **BM25 ì¸ë±ìŠ¤ êµ¬ì¶•**
   ```python
   # src/contextualforget/baselines/keyword_rag.py
   
   from rank_bm25 import BM25Okapi
   import nltk
   
   class KeywordRAG:
       """BM25 ê¸°ë°˜ í‚¤ì›Œë“œ RAG"""
       
       def __init__(self, graph_path: str):
           self.graph = self.load_graph(graph_path)
           self.documents = self.extract_documents()
           self.bm25 = self.build_index()
       
       def extract_documents(self) -> List[Dict]:
           """ê·¸ë˜í”„ì—ì„œ ë¬¸ì„œ ì¶”ì¶œ"""
           documents = []
           
           for node_id, data in self.graph.nodes(data=True):
               if data.get('type') == 'ifc':
                   doc = {
                       'id': node_id,
                       'text': f"{data.get('name', '')} {data.get('ifc_type', '')}",
                       'type': 'ifc',
                       'data': data
                   }
               elif data.get('type') == 'bcf':
                   doc = {
                       'id': node_id,
                       'text': f"{data.get('title', '')} {data.get('description', '')}",
                       'type': 'bcf',
                       'data': data
                   }
               else:
                   continue
               
               documents.append(doc)
           
           return documents
       
       def build_index(self) -> BM25Okapi:
           """BM25 ì¸ë±ìŠ¤ êµ¬ì¶•"""
           tokenized_docs = [
               self.tokenize(doc['text']) 
               for doc in self.documents
           ]
           return BM25Okapi(tokenized_docs)
       
       def tokenize(self, text: str) -> List[str]:
           """í…ìŠ¤íŠ¸ í† í°í™”"""
           return nltk.word_tokenize(text.lower())
       
       def query(self, question: str, top_k: int = 5) -> str:
           """ì§ˆì˜ ì²˜ë¦¬"""
           # í† í°í™”
           query_tokens = self.tokenize(question)
           
           # BM25 ê²€ìƒ‰
           scores = self.bm25.get_scores(query_tokens)
           top_indices = np.argsort(scores)[::-1][:top_k]
           
           # ê²°ê³¼ ì¡°í•©
           retrieved_docs = [self.documents[i] for i in top_indices]
           
           # ë‹µë³€ ìƒì„± (ê°„ë‹¨í•œ ì¶”ì¶œ)
           answer = self.generate_answer(question, retrieved_docs)
           
           return answer
       
       def get_evidence(self, question: str, top_k: int = 5) -> List[str]:
           """ê·¼ê±° ë¬¸ì„œ ë°˜í™˜"""
           query_tokens = self.tokenize(question)
           scores = self.bm25.get_scores(query_tokens)
           top_indices = np.argsort(scores)[::-1][:top_k]
           
           return [self.documents[i]['id'] for i in top_indices]
       
       def generate_answer(self, question: str, docs: List[Dict]) -> str:
           """ê²€ìƒ‰ëœ ë¬¸ì„œë¡œë¶€í„° ë‹µë³€ ìƒì„±"""
           # ê°„ë‹¨í•œ ì¶”ì¶œì‹ ë‹µë³€
           if 'guid' in question.lower() or 'ê´€ë ¨' in question:
               # BCF í† í”½ ì¶”ì¶œ
               topics = [d for d in docs if d['type'] == 'bcf']
               if topics:
                   titles = [t['data'].get('title', '') for t in topics]
                   return f"ì´ {len(titles)}ê°œì˜ ì´ìŠˆ: " + ", ".join(titles[:3])
           
           # ê¸°ë³¸ ë‹µë³€
           return docs[0]['text'] if docs else "ë‹µë³€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
   ```

2. **ë²¤ì¹˜ë§ˆí¬ í†µí•©**
   ```python
   # scripts/run_benchmark.py ì—…ë°ì´íŠ¸
   
   # BM25 í‰ê°€ ì¶”ê°€
   print("\nEvaluating BM25 Keyword RAG...")
   keyword_rag = KeywordRAG('data/processed/graph.gpickle')
   keyword_results = benchmark.evaluate_method(
       method_name='Keyword-RAG (BM25)',
       query_func=keyword_rag.query,
       evidence_func=keyword_rag.get_evidence
   )
   ```

**ì‚°ì¶œë¬¼**:
- `src/contextualforget/baselines/keyword_rag.py`
- `tests/test_keyword_rag.py`
- `results/benchmark/keyword_rag_results.json`

---

### Week 4 (Oct 30 - Nov 5)

#### âœ… Task 2.2: ë²¡í„° RAG (Sentence-BERT + FAISS) êµ¬í˜„
**ì˜ˆìƒ ì‹œê°„**: 5-7ì¼

**ì„¸ë¶€ ì‘ì—…**:
1. **ì„ë² ë”© ë° FAISS ì¸ë±ìŠ¤**
   ```python
   # src/contextualforget/baselines/vector_rag.py
   
   from sentence_transformers import SentenceTransformer
   import faiss
   
   class VectorRAG:
       """Sentence-BERT + FAISS ê¸°ë°˜ ë²¡í„° RAG"""
       
       def __init__(self, 
                    graph_path: str,
                    model_name: str = 'paraphrase-multilingual-mpnet-base-v2'):
           self.graph = self.load_graph(graph_path)
           self.model = SentenceTransformer(model_name)
           self.documents = self.extract_documents()
           self.embeddings = None
           self.index = None
           self.build_index()
       
       def extract_documents(self) -> List[Dict]:
           """ê·¸ë˜í”„ì—ì„œ ë¬¸ì„œ ì¶”ì¶œ (KeywordRAGì™€ ë™ì¼)"""
           # ... (KeywordRAGì™€ ë™ì¼í•œ ë¡œì§)
       
       def build_index(self):
           """FAISS ì¸ë±ìŠ¤ êµ¬ì¶•"""
           print("Building vector embeddings...")
           texts = [doc['text'] for doc in self.documents]
           self.embeddings = self.model.encode(texts, show_progress_bar=True)
           
           print("Building FAISS index...")
           dimension = self.embeddings.shape[1]
           self.index = faiss.IndexFlatL2(dimension)
           self.index.add(self.embeddings.astype('float32'))
           
           print(f"Index built: {len(self.documents)} documents, {dimension}D")
       
       def query(self, question: str, top_k: int = 5) -> str:
           """ì§ˆì˜ ì²˜ë¦¬"""
           # ì§ˆë¬¸ ì„ë² ë”©
           query_embedding = self.model.encode([question])
           
           # FAISS ê²€ìƒ‰
           distances, indices = self.index.search(
               query_embedding.astype('float32'), top_k
           )
           
           # ê²°ê³¼ ì¡°í•©
           retrieved_docs = [self.documents[i] for i in indices[0]]
           
           # ë‹µë³€ ìƒì„±
           answer = self.generate_answer(question, retrieved_docs)
           
           return answer
       
       def get_evidence(self, question: str, top_k: int = 5) -> List[str]:
           """ê·¼ê±° ë¬¸ì„œ ë°˜í™˜"""
           query_embedding = self.model.encode([question])
           distances, indices = self.index.search(
               query_embedding.astype('float32'), top_k
           )
           
           return [self.documents[i]['id'] for i in indices[0]]
       
       def generate_answer(self, question: str, docs: List[Dict]) -> str:
           """ê²€ìƒ‰ëœ ë¬¸ì„œë¡œë¶€í„° ë‹µë³€ ìƒì„± (KeywordRAGì™€ ë™ì¼)"""
           # ... (KeywordRAGì™€ ë™ì¼í•œ ë¡œì§)
   ```

2. **ë²¤ì¹˜ë§ˆí¬ í†µí•©**
   ```python
   # scripts/run_benchmark.py ì—…ë°ì´íŠ¸
   
   # Vector RAG í‰ê°€ ì¶”ê°€
   print("\nEvaluating Vector RAG...")
   vector_rag = VectorRAG('data/processed/graph.gpickle')
   vector_results = benchmark.evaluate_method(
       method_name='Vector-RAG (SBERT+FAISS)',
       query_func=vector_rag.query,
       evidence_func=vector_rag.get_evidence
   )
   
   # ì „ì²´ ë¹„êµ
   print("\n=== Comparison ===")
   comparison_df = benchmark.compare_methods([
       graph_rag_results,
       keyword_results,
       vector_results
   ])
   print(comparison_df)
   ```

**ì‚°ì¶œë¬¼**:
- `src/contextualforget/baselines/vector_rag.py`
- `tests/test_vector_rag.py`
- `results/benchmark/vector_rag_results.json`
- `results/benchmark/comparison.png`

---

#### âœ… Task 2.3: RQ1 ì´ˆê¸° ì‹¤í—˜
**ì˜ˆìƒ ì‹œê°„**: 2-3ì¼

**ì„¸ë¶€ ì‘ì—…**:
1. **3ê°€ì§€ ë°©ë²• ë¹„êµ ì‹¤í—˜**
2. **í†µê³„ì  ìœ ì˜ì„± ê²€ì • (t-test)**
3. **ê²°ê³¼ ë¶„ì„ ë° ì‹œê°í™”**
4. **ì˜ˆë¹„ ë…¼ë¬¸ Results ì„¹ì…˜ ì‘ì„±**

**ì‚°ì¶œë¬¼**:
- `results/rq1_preliminary_results.pdf`
- `docs/rq1_analysis.md`

---

## ğŸ“‹ Phase 3: ë§ê° ì •ì±… ì™„ì„± (Week 5-6)

**ëª©í‘œ**: ìš”ì•½ ì••ì¶• ì •ì±… êµ¬í˜„ ë° ë§ê° íš¨ê³¼ ì¸¡ì •

### Week 5 (Nov 6-12)

#### âœ… Task 3.1: LLM ìš”ì•½ ì••ì¶• ì •ì±… êµ¬í˜„
**ì˜ˆìƒ ì‹œê°„**: 5-7ì¼

**ì„¸ë¶€ ì‘ì—…**:
1. **ìš”ì•½ ì •ì±… í´ë˜ìŠ¤**
   ```python
   # src/contextualforget/core/summarization_policy.py
   
   from langchain_ollama import ChatOllama
   
   class SummarizationPolicy(ForgettingPolicy):
       """LLM ê¸°ë°˜ ì´ë²¤íŠ¸ ìš”ì•½ ì••ì¶• ì •ì±…"""
       
       def __init__(self, 
                    model_name: str = "qwen2.5:3b",
                    time_window_days: int = 30,
                    min_events_for_summary: int = 3):
           self.llm = ChatOllama(model=model_name, temperature=0.3)
           self.time_window = time_window_days
           self.min_events = min_events_for_summary
       
       def should_forget(self, event: Dict, current_time: datetime) -> bool:
           """ê°œë³„ ì´ë²¤íŠ¸ëŠ” ì‚­ì œí•˜ì§€ ì•ŠìŒ (ê·¸ë£¹ ë‹¨ìœ„ ìš”ì•½)"""
           return False
       
       def apply(self, graph: nx.DiGraph, current_time: datetime) -> nx.DiGraph:
           """ì˜¤ë˜ëœ ì´ë²¤íŠ¸ë“¤ì„ ìš”ì•½í•˜ì—¬ ì••ì¶•"""
           
           # ì˜¤ë˜ëœ ì´ë²¤íŠ¸ ì¶”ì¶œ
           old_events = self.find_old_events(graph, current_time)
           
           if len(old_events) < self.min_events:
               logger.info("ìš”ì•½í•  ì´ë²¤íŠ¸ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤")
               return graph
           
           # ìœ ì‚¬í•œ ì´ë²¤íŠ¸ ê·¸ë£¹í™”
           event_groups = self.group_by_similarity(old_events)
           
           compression_stats = {
               'original_nodes': len(old_events),
               'summary_nodes': 0,
               'compression_ratio': 0
           }
           
           for group in event_groups:
               if len(group) < self.min_events:
                   continue
               
               # LLMìœ¼ë¡œ ìš”ì•½ ìƒì„±
               summary = self.summarize_events(group)
               
               # ìš”ì•½ ë…¸ë“œ ìƒì„±
               summary_node_id = self.create_summary_node(graph, summary, group)
               compression_stats['summary_nodes'] += 1
               
               # ì›ë³¸ ë…¸ë“œ ì œê±° ë° ì—£ì§€ ì¬ì—°ê²°
               self.replace_nodes_with_summary(graph, group, summary_node_id)
           
           # ì••ì¶•ë¥  ê³„ì‚°
           compression_stats['compression_ratio'] = (
               1 - compression_stats['summary_nodes'] / 
               compression_stats['original_nodes']
           )
           
           logger.info(f"ìš”ì•½ ì••ì¶• ì™„ë£Œ: {compression_stats}")
           return graph
       
       def find_old_events(self, 
                          graph: nx.DiGraph, 
                          current_time: datetime) -> List[tuple]:
           """ì˜¤ë˜ëœ BCF ì´ë²¤íŠ¸ ì°¾ê¸°"""
           cutoff_date = current_time - timedelta(days=self.time_window)
           old_events = []
           
           for node_id, data in graph.nodes(data=True):
               if data.get('type') != 'bcf':
                   continue
               
               created_date = datetime.fromisoformat(
                   data.get('created_at', current_time.isoformat())
               )
               
               if created_date < cutoff_date:
                   old_events.append((node_id, data))
           
           return old_events
       
       def group_by_similarity(self, 
                              events: List[tuple]) -> List[List[tuple]]:
           """ìœ ì‚¬í•œ ì´ë²¤íŠ¸ë“¤ì„ ê·¸ë£¹í™”"""
           # ê°„ë‹¨í•œ êµ¬í˜„: GUID ê¸°ë°˜ ê·¸ë£¹í™”
           # ì‹¤ì œë¡œëŠ” ì„ë² ë”© ìœ ì‚¬ë„ ê¸°ë°˜ í´ëŸ¬ìŠ¤í„°ë§
           
           groups_dict = {}
           
           for node_id, data in events:
               # ì—°ê²°ëœ IFC GUID ê¸°ë°˜ ê·¸ë£¹í™”
               connected_guids = tuple(sorted(data.get('related_guids', [])))
               
               if connected_guids not in groups_dict:
                   groups_dict[connected_guids] = []
               
               groups_dict[connected_guids].append((node_id, data))
           
           return list(groups_dict.values())
       
       def summarize_events(self, event_group: List[tuple]) -> str:
           """LLMìœ¼ë¡œ ì´ë²¤íŠ¸ ê·¸ë£¹ ìš”ì•½"""
           # ì´ë²¤íŠ¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ
           event_texts = []
           for node_id, data in event_group:
               text = f"- [{data.get('created_at', '')}] {data.get('title', '')}: {data.get('description', '')}"
               event_texts.append(text)
           
           # LLM í”„ë¡¬í”„íŠ¸
           prompt = f"""ë‹¤ìŒ BIM í”„ë¡œì íŠ¸ ì´ìŠˆë“¤ì„ í•˜ë‚˜ì˜ ìš”ì•½ë¬¸ìœ¼ë¡œ í†µí•©í•˜ì„¸ìš”.
í•µì‹¬ ì •ë³´(ë‚ ì§œ ë²”ìœ„, ì£¼ìš” ë¬¸ì œ, ê´€ë ¨ ë¶€ì¬)ë¥¼ ìœ ì§€í•˜ë˜ ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.

ì´ë²¤íŠ¸ë“¤:
{chr(10).join(event_texts)}

ìš”ì•½ (50ë‹¨ì–´ ì´ë‚´):"""
           
           # LLM í˜¸ì¶œ
           response = self.llm.invoke(prompt)
           summary = response.content.strip()
           
           return summary
       
       def create_summary_node(self, 
                              graph: nx.DiGraph, 
                              summary: str, 
                              original_events: List[tuple]) -> str:
           """ìš”ì•½ ë…¸ë“œ ìƒì„±"""
           import uuid
           summary_node_id = f"summary:{uuid.uuid4()}"
           
           # ë©”íƒ€ë°ì´í„° í†µí•©
           start_date = min(
               datetime.fromisoformat(data.get('created_at', ''))
               for _, data in original_events
           )
           end_date = max(
               datetime.fromisoformat(data.get('created_at', ''))
               for _, data in original_events
           )
           
           graph.add_node(summary_node_id, **{
               'type': 'summary',
               'text': summary,
               'original_count': len(original_events),
               'date_range_start': start_date.isoformat(),
               'date_range_end': end_date.isoformat(),
               'created_at': datetime.now().isoformat()
           })
           
           return summary_node_id
       
       def replace_nodes_with_summary(self, 
                                     graph: nx.DiGraph, 
                                     original_nodes: List[tuple], 
                                     summary_node_id: str):
           """ì›ë³¸ ë…¸ë“œë¥¼ ìš”ì•½ ë…¸ë“œë¡œ êµì²´í•˜ê³  ì—£ì§€ ì¬ì—°ê²°"""
           # ëª¨ë“  ì›ë³¸ ë…¸ë“œì˜ ì´ì›ƒ ìˆ˜ì§‘
           all_neighbors = set()
           
           for node_id, _ in original_nodes:
               # ì„ í–‰ì (predecessors)
               all_neighbors.update(graph.predecessors(node_id))
               # í›„ì†ì (successors)
               all_neighbors.update(graph.successors(node_id))
           
           # ìš”ì•½ ë…¸ë“œì™€ ì´ì›ƒë“¤ ì—°ê²°
           for neighbor in all_neighbors:
               if neighbor not in [n[0] for n in original_nodes]:
                   graph.add_edge(summary_node_id, neighbor, 
                                relation='summarized')
           
           # ì›ë³¸ ë…¸ë“œ ì œê±°
           for node_id, _ in original_nodes:
               graph.remove_node(node_id)
   ```

2. **í†µí•© ë° í…ŒìŠ¤íŠ¸**
   ```python
   # tests/test_summarization_policy.py
   
   def test_summarization_policy():
       # í…ŒìŠ¤íŠ¸ ê·¸ë˜í”„ ìƒì„±
       graph = create_test_graph_with_old_events()
       
       # ìš”ì•½ ì •ì±… ì ìš©
       policy = SummarizationPolicy(time_window_days=30)
       compressed_graph = policy.apply(graph, datetime.now())
       
       # ê²€ì¦
       assert compressed_graph.number_of_nodes() < graph.number_of_nodes()
       
       # ìš”ì•½ ë…¸ë“œ í™•ì¸
       summary_nodes = [
           n for n, d in compressed_graph.nodes(data=True) 
           if d.get('type') == 'summary'
       ]
       assert len(summary_nodes) > 0
   ```

**ì‚°ì¶œë¬¼**:
- `src/contextualforget/core/summarization_policy.py`
- `tests/test_summarization_policy.py`
- `docs/summarization_guide.md`

---

### Week 6 (Nov 13-19)

#### âœ… Task 3.2: êµ¬ë²„ì „ ì¸ìš©ë¥ /ëª¨ìˆœë¥  ì¸¡ì • ì‹œìŠ¤í…œ
**ì˜ˆìƒ ì‹œê°„**: 3-4ì¼

**ì„¸ë¶€ ì‘ì—…**:
1. **êµ¬ë²„ì „ ì¸ìš©ë¥  ì¸¡ì •**
   ```python
   # eval/metrics/forgetting_metrics.py
   
   def measure_obsolete_citation_rate(graph: nx.DiGraph,
                                      query_results: List[Dict],
                                      current_time: datetime,
                                      ttl_days: int) -> float:
       """êµ¬ë²„ì „ ì¸ìš©ë¥  ì¸¡ì •"""
       cutoff_date = current_time - timedelta(days=ttl_days)
       
       total_citations = 0
       obsolete_citations = 0
       
       for result in query_results:
           for evidence_id in result.get('evidence', []):
               total_citations += 1
               
               # ë…¸ë“œ ë‚ ì§œ í™•ì¸
               node_data = graph.nodes[evidence_id]
               created_date = datetime.fromisoformat(
                   node_data.get('created_at', '')
               )
               
               if created_date < cutoff_date:
                   obsolete_citations += 1
       
       return obsolete_citations / total_citations if total_citations > 0 else 0.0
   
   def measure_contradiction_rate(query_results: List[Dict],
                                 llm_checker: Callable) -> float:
       """ëª¨ìˆœë¥  ì¸¡ì • (LLM ê¸°ë°˜)"""
       total_pairs = 0
       contradictions = 0
       
       # ë™ì¼ GUIDì— ëŒ€í•œ ì—¬ëŸ¬ ë‹µë³€ ë¹„êµ
       guid_answers = {}
       for result in query_results:
           guid = result.get('guid')
           answer = result.get('answer')
           
           if guid not in guid_answers:
               guid_answers[guid] = []
           guid_answers[guid].append(answer)
       
       # ìŒë³„ ëª¨ìˆœ ê²€ì‚¬
       for guid, answers in guid_answers.items():
           for i in range(len(answers)):
               for j in range(i + 1, len(answers)):
                   total_pairs += 1
                   if llm_checker(answers[i], answers[j]):
                       contradictions += 1
       
       return contradictions / total_pairs if total_pairs > 0 else 0.0
   ```

2. **LLM ê¸°ë°˜ ëª¨ìˆœ ê²€ì‚¬ê¸°**
   ```python
   class ContradictionChecker:
       """LLM ê¸°ë°˜ ëª¨ìˆœ ê²€ì‚¬"""
       
       def __init__(self, model_name: str = "qwen2.5:3b"):
           self.llm = ChatOllama(model=model_name, temperature=0.1)
       
       def check_contradiction(self, answer1: str, answer2: str) -> bool:
           """ë‘ ë‹µë³€ì´ ëª¨ìˆœë˜ëŠ”ì§€ í™•ì¸"""
           prompt = f"""ë‹¤ìŒ ë‘ ë‹µë³€ì´ ì„œë¡œ ëª¨ìˆœë˜ëŠ”ì§€ íŒë‹¨í•˜ì„¸ìš”.

ë‹µë³€ 1: {answer1}
ë‹µë³€ 2: {answer2}

ëª¨ìˆœ ì—¬ë¶€ (yes/no):"""
           
           response = self.llm.invoke(prompt)
           return 'yes' in response.content.lower()
   ```

**ì‚°ì¶œë¬¼**:
- `eval/metrics/forgetting_metrics.py`
- `tests/test_forgetting_metrics.py`

---

#### âœ… Task 3.3: ë§ê° ì •ì±… ì¡°í•© ì‹¤í—˜
**ì˜ˆìƒ ì‹œê°„**: 2-3ì¼

**ì„¸ë¶€ ì‘ì—…**:
1. **ì •ì±… ì¡°í•© ì‹¤í—˜ ì„¤ê³„**
   ```python
   # scripts/forgetting_experiments.py
   
   def run_forgetting_experiments():
       """ë‹¤ì–‘í•œ ë§ê° ì •ì±… ì¡°í•© ì‹¤í—˜"""
       
       policies = {
           'None': None,
           'TTL-30': TTLPolicy(ttl_days=30),
           'TTL-60': TTLPolicy(ttl_days=60),
           'Decay': WeightedDecayPolicy(decay_rate=0.1),
           'Summary-30': SummarizationPolicy(time_window_days=30),
           'TTL+Decay': CompositePolicy([
               TTLPolicy(ttl_days=30),
               WeightedDecayPolicy(decay_rate=0.1)
           ]),
           'TTL+Summary': CompositePolicy([
               TTLPolicy(ttl_days=60),
               SummarizationPolicy(time_window_days=30)
           ])
       }
       
       results = []
       
       for policy_name, policy in policies.items():
           print(f"\nTesting {policy_name}...")
           
           # ê·¸ë˜í”„ ë¡œë“œ
           graph = load_base_graph()
           
           # ì •ì±… ì ìš©
           if policy:
               graph = policy.apply(graph, datetime.now())
           
           # ì¸¡ì •
           obsolete_rate = measure_obsolete_citation_rate(graph, ...)
           contradiction_rate = measure_contradiction_rate(...)
           graph_size = graph.number_of_nodes()
           query_latency = measure_average_latency(graph, ...)
           
           results.append({
               'policy': policy_name,
               'obsolete_rate': obsolete_rate,
               'contradiction_rate': contradiction_rate,
               'graph_size': graph_size,
               'query_latency_ms': query_latency
           })
       
       # ê²°ê³¼ ì €ì¥ ë° ì‹œê°í™”
       df = pd.DataFrame(results)
       df.to_csv('results/forgetting_experiments.csv', index=False)
       plot_performance_cost_curve(df)
   ```

2. **ì„±ëŠ¥-ë¹„ìš© ê³¡ì„  ì‹œê°í™”**
   ```python
   def plot_performance_cost_curve(df: pd.DataFrame):
       """ì„±ëŠ¥-ë¹„ìš© íŠ¸ë ˆì´ë“œì˜¤í”„ ì‹œê°í™”"""
       fig, axes = plt.subplots(2, 2, figsize=(12, 10))
       
       # êµ¬ë²„ì „ ì¸ìš©ë¥ 
       df.plot(x='policy', y='obsolete_rate', kind='bar', ax=axes[0,0])
       axes[0,0].set_title('Obsolete Citation Rate')
       
       # ëª¨ìˆœë¥ 
       df.plot(x='policy', y='contradiction_rate', kind='bar', ax=axes[0,1])
       axes[0,1].set_title('Contradiction Rate')
       
       # ê·¸ë˜í”„ í¬ê¸°
       df.plot(x='policy', y='graph_size', kind='bar', ax=axes[1,0])
       axes[1,0].set_title('Graph Size (nodes)')
       
       # ì¿¼ë¦¬ ì§€ì—°
       df.plot(x='policy', y='query_latency_ms', kind='bar', ax=axes[1,1])
       axes[1,1].set_title('Query Latency (ms)')
       
       plt.tight_layout()
       plt.savefig('results/performance_cost_curve.png', dpi=300)
   ```

**ì‚°ì¶œë¬¼**:
- `scripts/forgetting_experiments.py`
- `results/forgetting_experiments.csv`
- `results/performance_cost_curve.png`
- `docs/rq2_analysis.md`

---

## ğŸ“‹ Phase 4: ì‹¤í—˜ & ë…¼ë¬¸ ì‘ì„± (Week 7-10)

### Week 7-8 (Nov 20 - Dec 3)

#### âœ… Task 4.1: ì „ì²´ ì‹¤í—˜ ìˆ˜í–‰
**ì˜ˆìƒ ì‹œê°„**: 10-14ì¼

**ì„¸ë¶€ ì‘ì—…**:
1. **RQ1 ì „ì²´ ì‹¤í—˜** (3ì¼)
   - 3ê°€ì§€ RAG ë°©ë²• ì™„ì „ ë¹„êµ
   - ë‚œì´ë„ë³„, ì¹´í…Œê³ ë¦¬ë³„ ë¶„ì„
   - í†µê³„ì  ìœ ì˜ì„± ê²€ì •

2. **RQ2 ì „ì²´ ì‹¤í—˜** (4ì¼)
   - 7ê°€ì§€ ë§ê° ì •ì±… ì¡°í•© ë¹„êµ
   - ì‹œê°„ ê²½ê³¼ ì‹œë®¬ë ˆì´ì…˜ (30/60/90ì¼)
   - ì„±ëŠ¥-ë¹„ìš© íŠ¸ë ˆì´ë“œì˜¤í”„ ë¶„ì„

3. **RQ3 ì „ì²´ ì‹¤í—˜** (3ì¼)
   - ë³€ê²½ ì˜í–¥ ì§ˆì˜ ì‹¤í—˜
   - íƒìƒ‰ ì‹œê°„ ë¹„êµ (Graph vs Linear scan)
   - ë‹¤ì–‘í•œ í™‰ ìˆ˜ ì‹¤í—˜ (1-hop, 2-hop, 3-hop)

4. **ì¶”ê°€ ë¶„ì„** (2-3ì¼)
   - ì˜¤ë¥˜ ë¶„ì„ (Error Analysis)
   - Ablation Study
   - ìŠ¤ì¼€ì¼ëŸ¬ë¹Œë¦¬í‹° í…ŒìŠ¤íŠ¸

**ì‚°ì¶œë¬¼**:
- `results/rq1_full_results.json`
- `results/rq2_full_results.json`
- `results/rq3_full_results.json`
- ëª¨ë“  ì‹œê°í™” ìë£Œ (`.png`, `.pdf`)

---

### Week 9 (Dec 4-10)

#### âœ… Task 4.2: í†µê³„ ë¶„ì„ ë° ì‹œê°í™”
**ì˜ˆìƒ ì‹œê°„**: 5-7ì¼

**ì„¸ë¶€ ì‘ì—…**:
1. **í†µê³„ì  ìœ ì˜ì„± ê²€ì •**
   ```python
   # scripts/statistical_analysis.py
   
   from scipy import stats
   
   def perform_statistical_tests(results: Dict):
       """í†µê³„ì  ìœ ì˜ì„± ê²€ì •"""
       
       # RQ1: Graph-RAG vs Baselines
       graph_rag_f1 = results['graph_rag']['f1_scores']
       keyword_rag_f1 = results['keyword_rag']['f1_scores']
       vector_rag_f1 = results['vector_rag']['f1_scores']
       
       # t-test
       t_stat_kw, p_value_kw = stats.ttest_ind(graph_rag_f1, keyword_rag_f1)
       t_stat_vec, p_value_vec = stats.ttest_ind(graph_rag_f1, vector_rag_f1)
       
       print(f"Graph-RAG vs Keyword-RAG: t={t_stat_kw:.3f}, p={p_value_kw:.4f}")
       print(f"Graph-RAG vs Vector-RAG: t={t_stat_vec:.3f}, p={p_value_vec:.4f}")
       
       # Effect size (Cohen's d)
       cohens_d_kw = calculate_cohens_d(graph_rag_f1, keyword_rag_f1)
       cohens_d_vec = calculate_cohens_d(graph_rag_f1, vector_rag_f1)
       
       print(f"Effect size (Keyword): d={cohens_d_kw:.3f}")
       print(f"Effect size (Vector): d={cohens_d_vec:.3f}")
   ```

2. **ë…¼ë¬¸ í’ˆì§ˆ Figure ìƒì„±**
   - Publication-ready plots (LaTeX í˜¸í™˜)
   - ìƒ‰ìƒ íŒ”ë ˆíŠ¸ ì¼ê´€ì„±
   - ë†’ì€ í•´ìƒë„ (300+ DPI)

3. **í‘œ ìƒì„±**
   - LaTeX í˜•ì‹ í‘œ
   - ì£¼ìš” ê²°ê³¼ ìš”ì•½ í‘œ

**ì‚°ì¶œë¬¼**:
- `results/statistical_analysis.pdf`
- `paper/figures/` (ëª¨ë“  ë…¼ë¬¸ìš© ê·¸ë¦¼)
- `paper/tables/` (ëª¨ë“  ë…¼ë¬¸ìš© í‘œ)

---

### Week 10 (Dec 11-15)

#### âœ… Task 4.3: ë…¼ë¬¸ ì‘ì„± & íˆ¬ê³ 
**ì˜ˆìƒ ì‹œê°„**: 5-7ì¼

**ì„¸ë¶€ ì‘ì—…**:
1. **ë…¼ë¬¸ êµ¬ì¡°** (ACM/IEEE í˜•ì‹)
   ```
   1. Abstract (150-200 words)
   2. Introduction (2 pages)
      - Motivation
      - Problem Statement
      - Contributions
   3. Background & Related Work (2-3 pages)
      - IFC/BCF in BIM
      - RAG Systems
      - Long-term Memory in AI
   4. Approach (3-4 pages)
      - System Architecture
      - Graph-based Integration
      - Forgetting Mechanisms
      - LLM Integration
   5. Implementation (1-2 pages)
      - Tech Stack
      - Data Processing
      - Real-time Monitoring
   6. Evaluation (4-5 pages)
      - Experimental Setup
      - RQ1 Results: Graph-RAG vs Baselines
      - RQ2 Results: Forgetting Policies
      - RQ3 Results: Change Impact Tracking
      - Discussion
   7. Threats to Validity (0.5-1 page)
   8. Related Work (1-2 pages)
   9. Conclusion & Future Work (0.5-1 page)
   10. References
   
   Total: ~12-15 pages (ACM double column)
   ```

2. **ì§‘í•„ ì¼ì •**
   - Day 1-2: Abstract, Introduction, Approach ì´ˆì•ˆ
   - Day 3-4: Evaluation, Results ì‘ì„±
   - Day 5: Background, Related Work, Conclusion
   - Day 6: ì „ì²´ í¸ì§‘ ë° êµì •
   - Day 7: ìµœì¢… ê²€í†  ë° íˆ¬ê³  ì¤€ë¹„

3. **Artifact ì¤€ë¹„**
   - GitHub ì €ì¥ì†Œ ì •ë¦¬
   - README ìµœì¢… ì—…ë°ì´íŠ¸
   - Replication Package
   - Zenodo DOI ë°œê¸‰

**ì‚°ì¶œë¬¼**:
- `paper/contextualforget_paper.pdf`
- `paper/contextualforget_paper.tex`
- Artifact íŒ¨í‚¤ì§€
- íˆ¬ê³  ì™„ë£Œ!

---

## ğŸ“Š ì£¼ì°¨ë³„ ì²´í¬ë¦¬ìŠ¤íŠ¸

### Week 1 ì²´í¬ë¦¬ìŠ¤íŠ¸
- [ ] Gold QA ë°ì´í„°ì…‹ 50ê°œ ìƒì„±
- [ ] ì •ë‹µ ì •í™•ë„ ë©”íŠ¸ë¦­ êµ¬í˜„
- [ ] ê·¼ê±° ì •í™•ë„ ë©”íŠ¸ë¦­ êµ¬í˜„
- [ ] ì„±ëŠ¥ ë©”íŠ¸ë¦­ êµ¬í˜„
- [ ] ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ì‘ì„±

### Week 2 ì²´í¬ë¦¬ìŠ¤íŠ¸
- [ ] ë²¤ì¹˜ë§ˆí¬ í´ë˜ìŠ¤ êµ¬í˜„
- [ ] ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±
- [ ] Graph-RAG í‰ê°€ ì™„ë£Œ
- [ ] ê²°ê³¼ ì‹œê°í™”
- [ ] ë¬¸ì„œ ì‘ì„±

### Week 3 ì²´í¬ë¦¬ìŠ¤íŠ¸
- [ ] BM25 ì¸ë±ìŠ¤ êµ¬í˜„
- [ ] í‚¤ì›Œë“œ RAG ì¿¼ë¦¬ ë¡œì§
- [ ] ë²¤ì¹˜ë§ˆí¬ í†µí•©
- [ ] í…ŒìŠ¤íŠ¸ ì‘ì„±
- [ ] ê²°ê³¼ ë¶„ì„

### Week 4 ì²´í¬ë¦¬ìŠ¤íŠ¸
- [ ] Sentence-BERT ì„ë² ë”©
- [ ] FAISS ì¸ë±ìŠ¤ êµ¬ì¶•
- [ ] ë²¡í„° RAG ì¿¼ë¦¬ ë¡œì§
- [ ] 3ê°€ì§€ ë°©ë²• ë¹„êµ
- [ ] RQ1 ì˜ˆë¹„ ê²°ê³¼

### Week 5 ì²´í¬ë¦¬ìŠ¤íŠ¸
- [ ] ìš”ì•½ ì •ì±… í´ë˜ìŠ¤ êµ¬í˜„
- [ ] LLM ìš”ì•½ ë¡œì§
- [ ] ë…¸ë“œ êµì²´ ë¡œì§
- [ ] í†µí•© í…ŒìŠ¤íŠ¸
- [ ] ë¬¸ì„œ ì‘ì„±

### Week 6 ì²´í¬ë¦¬ìŠ¤íŠ¸
- [ ] êµ¬ë²„ì „ ì¸ìš©ë¥  ì¸¡ì •
- [ ] ëª¨ìˆœë¥  ì¸¡ì •
- [ ] ì •ì±… ì¡°í•© ì‹¤í—˜
- [ ] ì„±ëŠ¥-ë¹„ìš© ê³¡ì„  ìƒì„±
- [ ] RQ2 ê²°ê³¼ ë¶„ì„

### Week 7-8 ì²´í¬ë¦¬ìŠ¤íŠ¸
- [ ] RQ1 ì „ì²´ ì‹¤í—˜
- [ ] RQ2 ì „ì²´ ì‹¤í—˜
- [ ] RQ3 ì „ì²´ ì‹¤í—˜
- [ ] ì˜¤ë¥˜ ë¶„ì„
- [ ] Ablation Study

### Week 9 ì²´í¬ë¦¬ìŠ¤íŠ¸
- [ ] í†µê³„ì  ìœ ì˜ì„± ê²€ì •
- [ ] Publication figure ìƒì„±
- [ ] LaTeX í‘œ ìƒì„±
- [ ] ê²°ê³¼ ë¬¸ì„œ ì‘ì„±

### Week 10 ì²´í¬ë¦¬ìŠ¤íŠ¸
- [ ] ë…¼ë¬¸ ì´ˆì•ˆ ì‘ì„±
- [ ] ì „ì²´ í¸ì§‘
- [ ] Artifact ì¤€ë¹„
- [ ] ìµœì¢… ê²€í† 
- [ ] íˆ¬ê³ !

---

## ğŸ¯ ì„±ê³µ ê¸°ì¤€ (Success Criteria)

### Phase 1 ì™„ë£Œ ê¸°ì¤€
- âœ… 50ê°œ ì´ìƒì˜ Gold QA ìŒ
- âœ… 5ê°œ ì´ìƒì˜ í‰ê°€ ë©”íŠ¸ë¦­ êµ¬í˜„
- âœ… ìë™í™”ëœ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ ê°€ëŠ¥
- âœ… Graph-RAG ë² ì´ìŠ¤ë¼ì¸ ê²°ê³¼

### Phase 2 ì™„ë£Œ ê¸°ì¤€
- âœ… 2ê°œ ì´ìƒì˜ ë¹„êµêµ° êµ¬í˜„
- âœ… 3ê°€ì§€ ë°©ë²• ë¹„êµ ì™„ë£Œ
- âœ… í†µê³„ì  ìœ ì˜ì„± í™•ì¸ (p < 0.05)
- âœ… RQ1 ì˜ˆë¹„ ë‹µë³€ ê°€ëŠ¥

### Phase 3 ì™„ë£Œ ê¸°ì¤€
- âœ… ìš”ì•½ ì••ì¶• ì •ì±… ì‘ë™
- âœ… êµ¬ë²„ì „ ì¸ìš©ë¥  ì¸¡ì • ê°€ëŠ¥
- âœ… 7ê°€ì§€ ì •ì±… ì¡°í•© ë¹„êµ
- âœ… ì„±ëŠ¥-ë¹„ìš© ê³¡ì„  ë„ì¶œ

### Phase 4 ì™„ë£Œ ê¸°ì¤€
- âœ… 3ê°œ RQ ëª¨ë‘ ë‹µë³€ ê°€ëŠ¥
- âœ… í†µê³„ì ìœ¼ë¡œ ìœ ì˜ë¯¸í•œ ê²°ê³¼
- âœ… 12-15í˜ì´ì§€ ë…¼ë¬¸ ì™„ì„±
- âœ… Replication package ì¤€ë¹„

---

## ğŸ“š í•„ìš” ë¦¬ì†ŒìŠ¤

### ì†Œí”„íŠ¸ì›¨ì–´
- Python 3.11+
- ì¶”ê°€ íŒ¨í‚¤ì§€:
  ```bash
  pip install rank-bm25 sentence-transformers faiss-cpu nltk scipy pandas matplotlib seaborn
  ```

### í•˜ë“œì›¨ì–´
- í˜„ì¬ M4 Pro (24GB RAM) ì¶©ë¶„
- FAISS ì¸ë±ì‹± ì‹œ ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ í•„ìš”

### ë°ì´í„°
- í˜„ì¬ 6ê°œ ê±´ë¬¼, 87ê°œ ë…¸ë“œë¡œ ì‹œì‘
- Phase 1ì—ì„œ ë” ë§ì€ ìƒ˜í”Œ í•„ìš” ì‹œ ì¶”ê°€ ìƒì„±

### ì‹œê°„
- ì£¼ë‹¹ 30-40ì‹œê°„ ì‘ì—… í•„ìš”
- ì´ 270-360ì‹œê°„ (9-10ì£¼ Ã— 30-40ì‹œê°„)

---

## ğŸš¨ ìœ„í—˜ ìš”ì†Œ ë° ëŒ€ì‘ì±…

### ìœ„í—˜ 1: í‰ê°€ ë°ì´í„°ì…‹ í’ˆì§ˆ
**ìœ„í—˜**: Gold QAê°€ ë„ˆë¬´ ì‰½ê±°ë‚˜ í¸í–¥ë¨
**ëŒ€ì‘**: 
- ë…ë¦½ ê²€ì¦ì í™•ë³´
- ë‹¤ì–‘í•œ ë‚œì´ë„ ê· í˜•
- Pilot studyë¡œ ì‚¬ì „ ê²€ì¦

### ìœ„í—˜ 2: ë¹„êµêµ° ì„±ëŠ¥ ì €ì¡°
**ìœ„í—˜**: ëª¨ë“  ë°©ë²•ì´ ë¹„ìŠ·í•œ ì„±ëŠ¥
**ëŒ€ì‘**:
- ë‚œì´ë„ ë†’ì€ ì§ˆë¬¸ ì¶”ê°€
- Multi-hop ì¶”ë¡  ì§ˆë¬¸ í¬í•¨
- ì‹¤ì œ BIM ì „ë¬¸ê°€ ì§ˆë¬¸ ìˆ˜ì§‘

### ìœ„í—˜ 3: ìš”ì•½ ì••ì¶• í’ˆì§ˆ
**ìœ„í—˜**: LLM ìš”ì•½ì´ ì •ë³´ ì†ì‹¤
**ëŒ€ì‘**:
- ìš”ì•½ í’ˆì§ˆ ë©”íŠ¸ë¦­ ì¶”ê°€
- ì‚¬ëŒ í‰ê°€ (Human Evaluation)
- ì—¬ëŸ¬ LLM ëª¨ë¸ ë¹„êµ

### ìœ„í—˜ 4: ì‹œê°„ ë¶€ì¡±
**ìœ„í—˜**: 10ì£¼ ë‚´ ì™„ë£Œ ë¶ˆê°€
**ëŒ€ì‘**:
- ì£¼ì°¨ë³„ ë§ˆì¼ìŠ¤í†¤ ì—„ê²© ê´€ë¦¬
- í•„ìš” ì‹œ Phase 4 ì¶•ì†Œ
- Workshop ë¨¼ì €, Full paper ì´í›„

---

## ğŸ“ˆ ì§„í–‰ ìƒí™© ì¶”ì 

**í˜„ì¬ ìƒíƒœ**: Phase 0 ì™„ë£Œ (ì‹œìŠ¤í…œ í”„ë¡œí† íƒ€ì…)
**ë‹¤ìŒ ë‹¨ê³„**: Phase 1 Task 1.1 ì‹œì‘

### ì£¼ê°„ ë³´ê³  í˜•ì‹
```markdown
## Week X ì§„í–‰ ë³´ê³  (MM/DD - MM/DD)

### ì™„ë£Œí•œ ì‘ì—…
- [ ] Task X.Y: ...

### ì§„í–‰ ì¤‘ì¸ ì‘ì—…
- [ ] Task X.Z: ...

### ë‹¤ìŒ ì£¼ ê³„íš
- [ ] Task A.B: ...

### ì´ìŠˆ ë° ì°¨ë‹¨ ìš”ì†Œ
- ì—†ìŒ / [ì´ìŠˆ ì„¤ëª…]

### ì¼ì • ìƒíƒœ
- âœ… ì¼ì • ì¤€ìˆ˜ / âš ï¸ 1-2ì¼ ì§€ì—° / ğŸš¨ 3ì¼+ ì§€ì—°
```

---

## âœ… ì¦‰ì‹œ ì‹œì‘ ê°€ëŠ¥í•œ ì‘ì—…

### ë‚´ì¼ (Oct 10) ì‹œì‘:
1. **QA ë°ì´í„°ì…‹ ì„¤ê³„ ë¬¸ì„œ ì‘ì„±**
   - ì¹´í…Œê³ ë¦¬ ì •ì˜
   - ì§ˆë¬¸ í…œí”Œë¦¿ ì‘ì„±
   - ì²« 10ê°œ QA ìŒ ì‘ì„±

2. **í‰ê°€ ë©”íŠ¸ë¦­ ê¸°ë³¸ êµ¬ì¡° ì½”ë”©**
   - `eval/metrics/__init__.py` ìƒì„±
   - `answer_metrics.py` ìŠ¤ì¼ˆë ˆí†¤ ì½”ë“œ

3. **í”„ë¡œì íŠ¸ êµ¬ì¡° ì •ë¦¬**
   ```bash
   mkdir -p eval/metrics
   mkdir -p src/contextualforget/baselines
   mkdir -p paper/figures
   mkdir -p paper/tables
   ```

**ì²« ì£¼ ëª©í‘œ**: Gold QA 30ê°œ + í‰ê°€ ë©”íŠ¸ë¦­ 2ê°œ êµ¬í˜„ ì™„ë£Œ!

---

ëª¨ë“  ê³„íšì´ ìˆ˜ë¦½ë˜ì—ˆìŠµë‹ˆë‹¤! ğŸš€ 
ë‚´ì¼ë¶€í„° Phase 1.1ì„ ì‹œì‘í•˜ì‹œê² ìŠµë‹ˆê¹Œ?


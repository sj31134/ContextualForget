#!/usr/bin/env python3
"""
3ê°œ ì‹œìŠ¤í…œ ë¹„êµ ì‹¤í—˜ ìŠ¤í¬ë¦½íŠ¸
- Graph-RAG vs BM25 vs Vector RAG
"""

import sys
import json
import pickle
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any, Tuple

# Add src to path
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

from contextualforget.baselines import BM25QueryEngine, VectorQueryEngine
from contextualforget.query import AdvancedQueryEngine
from contextualforget.llm import NaturalLanguageProcessor, LLMQueryEngine
from metrics import EvaluationMetrics


class SystemComparisonRunner:
    """3ê°œ ì‹œìŠ¤í…œ ë¹„êµ ì‹¤í—˜ ì‹¤í–‰ê¸°"""
    
    def __init__(self, base_dir: Path):
        self.base_dir = Path(base_dir)
        self.eval_dir = self.base_dir / "eval"
        self.graph_path = self.base_dir / "data" / "processed" / "graph.gpickle"
        
        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.graph = self._load_graph()
        self.graph_rag_engine = None
        self.bm25_engine = None
        self.vector_engine = None
        self.metrics = EvaluationMetrics()
        
        # ê²°ê³¼ ì €ì¥
        self.results = {
            "timestamp": datetime.now().isoformat(),
            "systems": ["Graph-RAG", "BM25", "Vector"],
            "graph_stats": {},
            "comparison_results": [],
            "summary": {}
        }
    
    def _load_graph(self):
        """ê·¸ë˜í”„ ë¡œë“œ"""
        if not self.graph_path.exists():
            raise FileNotFoundError(f"ê·¸ë˜í”„ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {self.graph_path}")
        
        with open(self.graph_path, 'rb') as f:
            graph = pickle.load(f)
        
        print(f"âœ… ê·¸ë˜í”„ ë¡œë“œ: {graph.number_of_nodes()}ê°œ ë…¸ë“œ, {graph.number_of_edges()}ê°œ ì—£ì§€")
        return graph
    
    def initialize_engines(self):
        """ëª¨ë“  ì—”ì§„ ì´ˆê¸°í™”"""
        print("\nğŸ”§ ëª¨ë“  ì—”ì§„ ì´ˆê¸°í™” ì¤‘...")
        
        # Prepare graph data
        graph_data = {"nodes": list(self.graph.nodes(data=True))}
        
        # 1. Graph-RAG ì—”ì§„ ì´ˆê¸°í™”
        print("  ğŸ“Š Graph-RAG ì—”ì§„ ì´ˆê¸°í™”...")
        self.graph_rag_engine = AdvancedQueryEngine(self.graph)
        try:
            nlp_processor = NaturalLanguageProcessor(model_name="qwen2.5:3b", use_llm=True)
            self.graph_rag_engine = LLMQueryEngine(self.graph_rag_engine, nlp_processor)
            print("    âœ… Graph-RAG ì—”ì§„ ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            print(f"    âš ï¸ Graph-RAG LLM ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            print("    ğŸ”„ ì •ê·œì‹ ê¸°ë°˜ í´ë°± ëª¨ë“œë¡œ ì „í™˜")
            nlp_processor = NaturalLanguageProcessor(model_name="qwen2.5:3b", use_llm=False)
            self.graph_rag_engine = LLMQueryEngine(self.graph_rag_engine, nlp_processor)
        
        # 2. BM25 ì—”ì§„ ì´ˆê¸°í™”
        print("  ğŸ” BM25 ì—”ì§„ ì´ˆê¸°í™”...")
        self.bm25_engine = BM25QueryEngine()
        self.bm25_engine.initialize(graph_data)
        print("    âœ… BM25 ì—”ì§„ ì´ˆê¸°í™” ì™„ë£Œ")
        
        # 3. Vector RAG ì—”ì§„ ì´ˆê¸°í™”
        print("  ğŸ§  Vector RAG ì—”ì§„ ì´ˆê¸°í™”...")
        self.vector_engine = VectorQueryEngine()
        self.vector_engine.initialize(graph_data)
        print("    âœ… Vector RAG ì—”ì§„ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def load_gold_standard(self) -> List[Dict[str, Any]]:
        """Gold Standard QA ë¡œë“œ"""
        gold_path = self.eval_dir / "gold.jsonl"
        
        if not gold_path.exists():
            raise FileNotFoundError(f"Gold Standard íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {gold_path}")
        
        qa_pairs = []
        with open(gold_path, 'r', encoding='utf-8') as f:
            for line in f:
                qa_pairs.append(json.loads(line))
        
        print(f"âœ… Gold Standard ë¡œë“œ: {len(qa_pairs)}ê°œ QA ìŒ")
        return qa_pairs
    
    def run_single_qa_comparison(self, qa: Dict[str, Any]) -> Dict[str, Any]:
        """ë‹¨ì¼ QAì— ëŒ€í•´ 3ê°œ ì‹œìŠ¤í…œ ë¹„êµ ì‹¤í–‰"""
        qa_id = qa.get('qa_id', 'unknown')
        question = qa.get('question', '')
        ground_truth = qa.get('ground_truth', {})
        
        results = {
            "qa_id": qa_id,
            "question": question,
            "ground_truth": ground_truth,
            "systems": {}
        }
        
        # 1. Graph-RAG ì‹¤í–‰
        try:
            graph_result = self.graph_rag_engine.process_natural_query(question)
            graph_answer = graph_result.get('answer', '')
            graph_confidence = graph_result.get('confidence', 0.0)
            graph_metrics = self.metrics.calculate_answer_accuracy(graph_answer, ground_truth)
            
            results["systems"]["Graph-RAG"] = {
                "answer": graph_answer,
                "confidence": graph_confidence,
                "metrics": graph_metrics,
                "status": "success"
            }
        except Exception as e:
            results["systems"]["Graph-RAG"] = {
                "answer": "",
                "confidence": 0.0,
                "metrics": {"exact_match": 0.0, "f1_score": 0.0, "semantic_match": 0.0},
                "status": "error",
                "error": str(e)
            }
        
        # 2. BM25 ì‹¤í–‰
        try:
            bm25_result = self.bm25_engine.process_query(question)
            bm25_answer = bm25_result.get('answer', '')
            bm25_confidence = bm25_result.get('confidence', 0.0)
            bm25_metrics = self._calculate_baseline_metrics(bm25_answer, ground_truth)
            
            results["systems"]["BM25"] = {
                "answer": bm25_answer,
                "confidence": bm25_confidence,
                "metrics": bm25_metrics,
                "status": "success"
            }
        except Exception as e:
            results["systems"]["BM25"] = {
                "answer": "",
                "confidence": 0.0,
                "metrics": {"exact_match": 0.0, "f1_score": 0.0, "semantic_match": 0.0},
                "status": "error",
                "error": str(e)
            }
        
        # 3. Vector RAG ì‹¤í–‰
        try:
            vector_result = self.vector_engine.process_query(question)
            vector_answer = vector_result.get('answer', '')
            vector_confidence = vector_result.get('confidence', 0.0)
            vector_metrics = self._calculate_baseline_metrics(vector_answer, ground_truth)
            
            results["systems"]["Vector"] = {
                "answer": vector_answer,
                "confidence": vector_confidence,
                "metrics": vector_metrics,
                "status": "success"
            }
        except Exception as e:
            results["systems"]["Vector"] = {
                "answer": "",
                "confidence": 0.0,
                "metrics": {"exact_match": 0.0, "f1_score": 0.0, "semantic_match": 0.0},
                "status": "error",
                "error": str(e)
            }
        
        return results
    
    def _calculate_baseline_metrics(self, predicted_answer: str, ground_truth: Dict[str, Any]) -> Dict[str, float]:
        """ë² ì´ìŠ¤ë¼ì¸ ì‹œìŠ¤í…œ ë‹µë³€ì— ëŒ€í•œ ë©”íŠ¸ë¦­ ê³„ì‚°"""
        if not predicted_answer or not ground_truth:
            return {"exact_match": 0.0, "f1_score": 0.0, "semantic_match": 0.0}
        
        # Ground truthì—ì„œ ë‹µë³€ ì¶”ì¶œ
        gt_answer = ground_truth.get('answer', '')
        if isinstance(gt_answer, list):
            gt_answer = ' '.join(str(item) for item in gt_answer)
        elif isinstance(gt_answer, dict):
            gt_answer = str(gt_answer)
        
        # Exact Match
        exact_match = 1.0 if predicted_answer.strip() == gt_answer.strip() else 0.0
        
        # F1 Score (ê°„ë‹¨í•œ í† í° ê¸°ë°˜)
        pred_tokens = set(predicted_answer.lower().split())
        gt_tokens = set(gt_answer.lower().split())
        
        if not gt_tokens:
            f1_score = 1.0 if not pred_tokens else 0.0
        else:
            intersection = pred_tokens & gt_tokens
            precision = len(intersection) / len(pred_tokens) if pred_tokens else 0.0
            recall = len(intersection) / len(gt_tokens)
            f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0
        
        # Semantic Match (ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜)
        semantic_score = 0.0
        if pred_tokens and gt_tokens:
            # ê³µí†µ í‚¤ì›Œë“œ ë¹„ìœ¨
            common_ratio = len(intersection) / len(gt_tokens)
            semantic_score = min(common_ratio, 1.0)
        
        return {
            "exact_match": exact_match,
            "f1_score": f1_score,
            "semantic_match": semantic_score
        }
    
    def run_comparison(self):
        """ë¹„êµ ì‹¤í—˜ ì‹¤í–‰"""
        print("\nğŸš€ 3ê°œ ì‹œìŠ¤í…œ ë¹„êµ ì‹¤í—˜ ì‹œì‘")
        print("=" * 60)
        
        # Gold Standard ë¡œë“œ
        qa_pairs = self.load_gold_standard()
        
        # ë¹„êµ ì‹¤í—˜ ì‹¤í–‰
        print(f"\nğŸ¯ ë¹„êµ ì‹¤í—˜ ì‹¤í–‰ ì‹œì‘ ({len(qa_pairs)}ê°œ QA)")
        
        for i, qa in enumerate(qa_pairs, 1):
            print(f"  ì§„í–‰ë¥ : {i}/{len(qa_pairs)} - {qa.get('qa_id', 'unknown')}")
            
            result = self.run_single_qa_comparison(qa)
            self.results["comparison_results"].append(result)
        
        print(f"âœ… ë¹„êµ ì‹¤í—˜ ì™„ë£Œ: {len(self.results['comparison_results'])}ê°œ ê²°ê³¼")
        
        # ê²°ê³¼ ë¶„ì„
        self._analyze_results()
        
        # ë³´ê³ ì„œ ìƒì„±
        self._generate_report()
    
    def _analyze_results(self):
        """ê²°ê³¼ ë¶„ì„"""
        print("\nğŸ“Š ê²°ê³¼ ë¶„ì„ ì¤‘...")
        
        results = self.results["comparison_results"]
        
        # ì‹œìŠ¤í…œë³„ í†µê³„
        system_stats = {
            "Graph-RAG": {"total": 0, "success": 0, "exact_matches": 0, "f1_scores": [], "semantic_scores": []},
            "BM25": {"total": 0, "success": 0, "exact_matches": 0, "f1_scores": [], "semantic_scores": []},
            "Vector": {"total": 0, "success": 0, "exact_matches": 0, "f1_scores": [], "semantic_scores": []}
        }
        
        # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„
        category_stats = {
            "entity_search": {"Graph-RAG": [], "BM25": [], "Vector": []},
            "temporal": {"Graph-RAG": [], "BM25": [], "Vector": []},
            "forgetting": {"Graph-RAG": [], "BM25": [], "Vector": []},
            "complex": {"Graph-RAG": [], "BM25": [], "Vector": []}
        }
        
        for result in results:
            qa_id = result["qa_id"]
            
            # ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ
            category = "unknown"
            if "entity" in qa_id:
                category = "entity_search"
            elif "temporal" in qa_id:
                category = "temporal"
            elif "forgetting" in qa_id:
                category = "forgetting"
            elif "complex" in qa_id:
                category = "complex"
            
            # ê° ì‹œìŠ¤í…œë³„ í†µê³„ ê³„ì‚°
            for system_name in ["Graph-RAG", "BM25", "Vector"]:
                system_result = result["systems"][system_name]
                system_stats[system_name]["total"] += 1
                
                if system_result["status"] == "success":
                    system_stats[system_name]["success"] += 1
                    metrics = system_result["metrics"]
                    
                    if metrics["exact_match"] > 0:
                        system_stats[system_name]["exact_matches"] += 1
                    
                    system_stats[system_name]["f1_scores"].append(metrics["f1_score"])
                    system_stats[system_name]["semantic_scores"].append(metrics["semantic_match"])
                    
                    # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„
                    if category in category_stats:
                        category_stats[category][system_name].append(metrics["f1_score"])
        
        # ìš”ì•½ í†µê³„ ê³„ì‚°
        summary = {
            "total_qa": len(results),
            "systems": {}
        }
        
        for system_name, stats in system_stats.items():
            total = stats["total"]
            success = stats["success"]
            
            summary["systems"][system_name] = {
                "success_rate": success / total if total > 0 else 0,
                "exact_match_rate": stats["exact_matches"] / total if total > 0 else 0,
                "average_f1": sum(stats["f1_scores"]) / len(stats["f1_scores"]) if stats["f1_scores"] else 0,
                "average_semantic": sum(stats["semantic_scores"]) / len(stats["semantic_scores"]) if stats["semantic_scores"] else 0,
                "total_qa": total,
                "successful_qa": success
            }
        
        # ì¹´í…Œê³ ë¦¬ë³„ ìš”ì•½
        summary["categories"] = {}
        for category, systems in category_stats.items():
            summary["categories"][category] = {}
            for system_name, scores in systems.items():
                summary["categories"][category][system_name] = {
                    "count": len(scores),
                    "average_f1": sum(scores) / len(scores) if scores else 0
                }
        
        self.results["summary"] = summary
    
    def _generate_report(self):
        """ë³´ê³ ì„œ ìƒì„±"""
        print("\nğŸ“ ë³´ê³ ì„œ ìƒì„± ì¤‘...")
        
        # JSON ë³´ê³ ì„œ
        json_path = self.eval_dir / "comparison_report.json"
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, ensure_ascii=False, indent=2)
        print(f"  âœ… JSON ë³´ê³ ì„œ: {json_path}")
        
        # Markdown ë³´ê³ ì„œ
        md_path = self.eval_dir / "COMPARISON_REPORT.md"
        self._generate_markdown_report(md_path)
        print(f"  âœ… Markdown ë³´ê³ ì„œ: {md_path}")
        
        # ìµœì¢… ê²°ê³¼ ì¶œë ¥
        summary = self.results["summary"]
        print(f"\n======================================================================")
        print(f"âœ… 3ê°œ ì‹œìŠ¤í…œ ë¹„êµ ì‹¤í—˜ ì™„ë£Œ!")
        print(f"======================================================================")
        
        print(f"\nğŸ“Š ìµœì¢… ê²°ê³¼ ìš”ì•½:")
        for system_name, stats in summary["systems"].items():
            print(f"\nğŸ”¹ {system_name}:")
            print(f"  - ì„±ê³µë¥ : {stats['success_rate']:.3f}")
            print(f"  - ì •í™•ë„: {stats['exact_match_rate']:.3f}")
            print(f"  - í‰ê·  F1: {stats['average_f1']:.3f}")
            print(f"  - ì„±ê³µí•œ QA: {stats['successful_qa']}/{stats['total_qa']}")
        
        print(f"\nğŸ“‚ ìƒì„±ëœ íŒŒì¼:")
        print(f"  - {json_path}")
        print(f"  - {md_path}")
    
    def _generate_markdown_report(self, output_path: Path):
        """Markdown ë³´ê³ ì„œ ìƒì„±"""
        summary = self.results["summary"]
        
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write("# 3ê°œ ì‹œìŠ¤í…œ ë¹„êµ ì‹¤í—˜ ë³´ê³ ì„œ\n\n")
            f.write(f"**ì‹¤í–‰ ì¼ì‹œ**: {self.results['timestamp']}\n\n")
            
            # ê·¸ë˜í”„ í†µê³„
            f.write("## ğŸ“Š ê·¸ë˜í”„ í†µê³„\n\n")
            f.write(f"- **ë…¸ë“œ ìˆ˜**: {self.graph.number_of_nodes()}ê°œ\n")
            f.write(f"- **ì—£ì§€ ìˆ˜**: {self.graph.number_of_edges()}ê°œ\n")
            f.write(f"- **ë°©í–¥ì„±**: Directed\n\n")
            
            # QA í†µê³„
            f.write("## ğŸ“‹ QA í†µê³„\n\n")
            f.write(f"- **ì´ QA ìŒ**: {summary['total_qa']}ê°œ\n\n")
            
            # ì‹œìŠ¤í…œë³„ ì„±ëŠ¥ ë¹„êµ
            f.write("## ğŸ¯ ì‹œìŠ¤í…œë³„ ì„±ëŠ¥ ë¹„êµ\n\n")
            f.write("| ì‹œìŠ¤í…œ | ì„±ê³µë¥  | ì •í™•ë„ | í‰ê·  F1 | ì„±ê³µí•œ QA |\n")
            f.write("|--------|--------|--------|---------|----------|\n")
            
            for system_name, stats in summary["systems"].items():
                f.write(f"| {system_name} | {stats['success_rate']:.3f} | {stats['exact_match_rate']:.3f} | {stats['average_f1']:.3f} | {stats['successful_qa']}/{stats['total_qa']} |\n")
            
            # ì¹´í…Œê³ ë¦¬ë³„ ì„±ëŠ¥
            f.write("\n## ğŸ“ˆ ì¹´í…Œê³ ë¦¬ë³„ ì„±ëŠ¥\n\n")
            for category, systems in summary["categories"].items():
                f.write(f"### {category}\n\n")
                f.write("| ì‹œìŠ¤í…œ | QA ìˆ˜ | í‰ê·  F1 |\n")
                f.write("|--------|--------|----------|\n")
                
                for system_name, stats in systems.items():
                    f.write(f"| {system_name} | {stats['count']} | {stats['average_f1']:.3f} |\n")
                f.write("\n")
            
            # ìƒì„¸ ê²°ê³¼ ìƒ˜í”Œ
            f.write("## ğŸ“‹ ìƒì„¸ ê²°ê³¼ ìƒ˜í”Œ\n\n")
            f.write("### ì²˜ìŒ 5ê°œ QA ë¹„êµ\n\n")
            
            for i, result in enumerate(self.results["comparison_results"][:5]):
                f.write(f"#### {result['qa_id']}\n\n")
                f.write(f"**ì§ˆë¬¸**: {result['question']}\n\n")
                
                for system_name, system_result in result["systems"].items():
                    f.write(f"**{system_name}**:\n")
                    f.write(f"- ë‹µë³€: {system_result['answer']}\n")
                    f.write(f"- ì‹ ë¢°ë„: {system_result['confidence']:.3f}\n")
                    f.write(f"- F1 ì ìˆ˜: {system_result['metrics']['f1_score']:.3f}\n")
                    f.write(f"- ìƒíƒœ: {'âœ… ì„±ê³µ' if system_result['status'] == 'success' else 'âŒ ì‹¤íŒ¨'}\n\n")
                
                f.write("---\n\n")
            
            f.write("\n---\n")
            f.write(f"**ë³´ê³ ì„œ ìƒì„±**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write("**ì‹¤í—˜ ë²„ì „**: 1.0\n")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    base_dir = Path(__file__).parent.parent
    runner = SystemComparisonRunner(base_dir)
    
    try:
        runner.initialize_engines()
        runner.run_comparison()
    except Exception as e:
        print(f"âŒ ë¹„êµ ì‹¤í—˜ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return 1
    
    return 0


if __name__ == "__main__":
    exit(main())

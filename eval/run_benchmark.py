#!/usr/bin/env python3
"""
ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸

ëª©ì :
- Graph-RAG ì‹œìŠ¤í…œê³¼ Gold Standard QA ì—°ë™
- ìë™ í‰ê°€ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
- ê²°ê³¼ ì‹œê°í™” ë° ë³´ê³ ì„œ ìƒì„±
"""

import json
import pickle
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any
import networkx as nx

from metrics import EvaluationMetrics
from contextualforget.query import AdvancedQueryEngine
from contextualforget.llm import NaturalLanguageProcessor, LLMQueryEngine


class BenchmarkRunner:
    """ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ê¸°"""
    
    def __init__(self, base_dir: Path):
        self.base_dir = Path(base_dir)
        self.eval_dir = self.base_dir / "eval"
        self.graph_path = self.base_dir / "data" / "processed" / "graph.gpickle"
        
        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.graph = self._load_graph()
        self.query_engine = None
        self.llm_engine = None
        self.metrics = EvaluationMetrics()
        
        # ê²°ê³¼ ì €ì¥
        self.results = {
            "timestamp": datetime.now().isoformat(),
            "graph_stats": {},
            "qa_results": [],
            "summary": {}
        }
    
    def _load_graph(self):
        """ê·¸ë˜í”„ ë¡œë“œ"""
        if not self.graph_path.exists():
            raise FileNotFoundError(f"ê·¸ë˜í”„ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {self.graph_path}")
        
        with open(self.graph_path, 'rb') as f:
            graph = pickle.load(f)
        
        print(f"âœ… ê·¸ë˜í”„ ë¡œë“œ: {graph.number_of_nodes()}ê°œ ë…¸ë“œ, {graph.number_of_edges()}ê°œ ì—£ì§€")
        return graph
    
    def initialize_engines(self):
        """ì¿¼ë¦¬ ì—”ì§„ ì´ˆê¸°í™”"""
        print("\nğŸ”§ ì¿¼ë¦¬ ì—”ì§„ ì´ˆê¸°í™” ì¤‘...")
        
        # Graph-RAG ì¿¼ë¦¬ ì—”ì§„
        self.query_engine = AdvancedQueryEngine(self.graph)
        print("  âœ… Graph-RAG ì¿¼ë¦¬ ì—”ì§„ ì´ˆê¸°í™”")
        
        # LLM ìì—°ì–´ ì²˜ë¦¬ ì—”ì§„
        try:
            nlp_processor = NaturalLanguageProcessor(model_name="qwen2.5:3b", use_llm=True)
            self.llm_engine = LLMQueryEngine(self.query_engine, nlp_processor)
            print("  âœ… LLM ìì—°ì–´ ì²˜ë¦¬ ì—”ì§„ ì´ˆê¸°í™”")
        except Exception as e:
            print(f"  âš ï¸  LLM ì—”ì§„ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            print("  ğŸ”„ ì •ê·œì‹ ê¸°ë°˜ í´ë°± ëª¨ë“œë¡œ ì „í™˜")
            nlp_processor = NaturalLanguageProcessor(model_name="qwen2.5:3b", use_llm=False)
            self.llm_engine = LLMQueryEngine(self.query_engine, nlp_processor)
    
    def load_gold_standard(self) -> List[Dict[str, Any]]:
        """Gold Standard QA ë¡œë“œ"""
        gold_path = self.eval_dir / "gold.jsonl"
        
        if not gold_path.exists():
            raise FileNotFoundError(f"Gold Standard íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {gold_path}")
        
        qa_pairs = []
        with open(gold_path, 'r', encoding='utf-8') as f:
            for line in f:
                qa_pairs.append(json.loads(line))
        
        print(f"âœ… Gold Standard ë¡œë“œ: {len(qa_pairs)}ê°œ QA ìŒ")
        return qa_pairs
    
    def run_single_qa(self, qa: Dict[str, Any]) -> Dict[str, Any]:
        """ë‹¨ì¼ QA ì‹¤í–‰"""
        qa_id = qa.get('id')
        question = qa.get('question', '')
        ground_truth = qa.get('ground_truth', {})
        
        try:
            # LLM ìì—°ì–´ ì¿¼ë¦¬ ì‹¤í–‰
            result = self.llm_engine.process_natural_query(question)
            
            # ê²°ê³¼ ì¶”ì¶œ
            predicted_answer = result.get('natural_response', '')
            query_results = result.get('query_results', {})
            
            # ì†ŒìŠ¤ ì¶”ì¶œ (ê°€ëŠ¥í•œ ê²½ìš°)
            sources = []
            if 'results' in query_results:
                for item in query_results['results']:
                    if 'topic_id' in item:
                        sources.append(item['topic_id'])
                    elif 'guid' in item:
                        sources.append(item['guid'])
            
            return {
                "qa_id": qa_id,
                "question": question,
                "predicted_answer": predicted_answer,
                "sources": sources,
                "query_results": query_results,
                "ground_truth": ground_truth,
                "status": "success"
            }
            
        except Exception as e:
            return {
                "qa_id": qa_id,
                "question": question,
                "predicted_answer": f"Error: {str(e)}",
                "sources": [],
                "query_results": {},
                "ground_truth": ground_truth,
                "status": "error",
                "error": str(e)
            }
    
    def run_benchmark(self, qa_pairs: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ì „ì²´ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
        print(f"\nğŸš€ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ ì‹œì‘ ({len(qa_pairs)}ê°œ QA)")
        
        results = []
        
        for i, qa in enumerate(qa_pairs):
            print(f"  ì§„í–‰ë¥ : {i+1}/{len(qa_pairs)} - {qa.get('id', 'unknown')}")
            
            result = self.run_single_qa(qa)
            results.append(result)
        
        print(f"âœ… ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ: {len(results)}ê°œ ê²°ê³¼")
        return results
    
    def evaluate_results(self, qa_pairs: List[Dict[str, Any]], predictions: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ê²°ê³¼ í‰ê°€"""
        print("\nğŸ“Š ê²°ê³¼ í‰ê°€ ì¤‘...")
        
        # ì„±ê³µí•œ QAë§Œ í‰ê°€
        successful_pairs = []
        successful_predictions = []
        
        for qa, pred in zip(qa_pairs, predictions):
            if pred.get('status') == 'success':
                successful_pairs.append(qa)
                successful_predictions.append(pred)
        
        print(f"  í‰ê°€ ëŒ€ìƒ: {len(successful_pairs)}ê°œ (ì„±ê³µí•œ QA)")
        
        if not successful_pairs:
            return {"error": "í‰ê°€í•  ìˆ˜ ìˆëŠ” ì„±ê³µí•œ QAê°€ ì—†ìŠµë‹ˆë‹¤."}
        
        # ë©”íŠ¸ë¦­ ê³„ì‚°
        evaluation = self.metrics.calculate_answer_accuracy(successful_pairs, successful_predictions)
        
        return evaluation
    
    def generate_report(self, qa_pairs: List[Dict[str, Any]], predictions: List[Dict[str, Any]], evaluation: Dict[str, Any]):
        """ë³´ê³ ì„œ ìƒì„±"""
        print("\nğŸ“ ë³´ê³ ì„œ ìƒì„± ì¤‘...")
        
        # ê·¸ë˜í”„ í†µê³„
        graph_stats = {
            "nodes": self.graph.number_of_nodes(),
            "edges": self.graph.number_of_edges(),
            "is_directed": self.graph.is_directed()
        }
        
        # QA í†µê³„
        qa_stats = {
            "total_qa": len(qa_pairs),
            "successful_qa": len([p for p in predictions if p.get('status') == 'success']),
            "failed_qa": len([p for p in predictions if p.get('status') == 'error']),
            "by_category": {}
        }
        
        # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„
        for qa in qa_pairs:
            category = qa.get('category', 'unknown')
            if category not in qa_stats['by_category']:
                qa_stats['by_category'][category] = 0
            qa_stats['by_category'][category] += 1
        
        # ì¢…í•© ë³´ê³ ì„œ
        report = {
            "timestamp": datetime.now().isoformat(),
            "graph_statistics": graph_stats,
            "qa_statistics": qa_stats,
            "evaluation_results": evaluation,
            "detailed_results": predictions
        }
        
        # JSON ì €ì¥
        report_path = self.eval_dir / "benchmark_report.json"
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        print(f"  âœ… JSON ë³´ê³ ì„œ: {report_path}")
        
        # Markdown ë³´ê³ ì„œ
        md_path = self.eval_dir / "BENCHMARK_REPORT.md"
        with open(md_path, 'w', encoding='utf-8') as f:
            f.write(f"""# ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ ë³´ê³ ì„œ

**ì‹¤í–‰ ì¼ì‹œ**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## ğŸ“Š ê·¸ë˜í”„ í†µê³„

- **ë…¸ë“œ ìˆ˜**: {graph_stats['nodes']}ê°œ
- **ì—£ì§€ ìˆ˜**: {graph_stats['edges']}ê°œ
- **ë°©í–¥ì„±**: {'Directed' if graph_stats['is_directed'] else 'Undirected'}

## ğŸ“‹ QA í†µê³„

- **ì´ QA ìŒ**: {qa_stats['total_qa']}ê°œ
- **ì„±ê³µí•œ QA**: {qa_stats['successful_qa']}ê°œ
- **ì‹¤íŒ¨í•œ QA**: {qa_stats['failed_qa']}ê°œ
- **ì„±ê³µë¥ **: {qa_stats['successful_qa']/qa_stats['total_qa']*100:.1f}%

### ì¹´í…Œê³ ë¦¬ë³„ ë¶„í¬

""")
            
            for category, count in qa_stats['by_category'].items():
                f.write(f"- **{category}**: {count}ê°œ\n")
            
            f.write(f"""

## ğŸ¯ í‰ê°€ ê²°ê³¼

""")
            
            if 'error' in evaluation:
                f.write(f"**ì˜¤ë¥˜**: {evaluation['error']}\n")
            else:
                f.write(f"""
- **í‰ê·  ì •í™•ë„**: {evaluation.get('average_accuracy', 0):.3f}
- **í‰ê·  F1 ì ìˆ˜**: {evaluation.get('average_f1', 0):.3f}
- **ì´ ì§ˆë¬¸ ìˆ˜**: {evaluation.get('total_questions', 0)}ê°œ

### ì¹´í…Œê³ ë¦¬ë³„ ì„±ëŠ¥

""")
                
                for category, stats in evaluation.get('by_category', {}).items():
                    f.write(f"- **{category}**: ì •í™•ë„ {stats.get('accuracy', 0):.3f}, F1 {stats.get('f1', 0):.3f}\n")
                
                f.write(f"""

### ë‚œì´ë„ë³„ ì„±ëŠ¥

""")
                
                for difficulty, stats in evaluation.get('by_difficulty', {}).items():
                    f.write(f"- **{difficulty}**: ì •í™•ë„ {stats.get('accuracy', 0):.3f}, F1 {stats.get('f1', 0):.3f}\n")
            
            f.write(f"""

## ğŸ“ˆ ìƒì„¸ ê²°ê³¼

### ì„±ê³µí•œ QA ìŒ

""")
            
            for pred in predictions:
                if pred.get('status') == 'success':
                    f.write(f"""
#### {pred.get('qa_id', 'unknown')}

**ì§ˆë¬¸**: {pred.get('question', '')}

**ì˜ˆì¸¡ ë‹µë³€**: {pred.get('predicted_answer', '')[:200]}...

**ìƒíƒœ**: âœ… ì„±ê³µ

""")
            
            f.write(f"""

### ì‹¤íŒ¨í•œ QA ìŒ

""")
            
            for pred in predictions:
                if pred.get('status') == 'error':
                    f.write(f"""
#### {pred.get('qa_id', 'unknown')}

**ì§ˆë¬¸**: {pred.get('question', '')}

**ì˜¤ë¥˜**: {pred.get('error', 'Unknown error')}

**ìƒíƒœ**: âŒ ì‹¤íŒ¨

""")
            
            f.write(f"""

---

**ë³´ê³ ì„œ ìƒì„±**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}  
**ë²¤ì¹˜ë§ˆí¬ ë²„ì „**: 1.0
""")
        
        print(f"  âœ… Markdown ë³´ê³ ì„œ: {md_path}")
        
        return report
    
    def run_full_benchmark(self):
        """ì „ì²´ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
        print("=" * 70)
        print("ğŸ¯ ContextualForget ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰")
        print("=" * 70)
        
        try:
            # 1. ì—”ì§„ ì´ˆê¸°í™”
            self.initialize_engines()
            
            # 2. Gold Standard ë¡œë“œ
            qa_pairs = self.load_gold_standard()
            
            # 3. ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
            predictions = self.run_benchmark(qa_pairs)
            
            # 4. ê²°ê³¼ í‰ê°€
            evaluation = self.evaluate_results(qa_pairs, predictions)
            
            # 5. ë³´ê³ ì„œ ìƒì„±
            report = self.generate_report(qa_pairs, predictions, evaluation)
            
            print("\n" + "=" * 70)
            print("âœ… ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ!")
            print("=" * 70)
            
            if 'error' not in evaluation:
                print(f"""
ğŸ“Š ìµœì¢… ê²°ê³¼:
  - í‰ê·  ì •í™•ë„: {evaluation.get('average_accuracy', 0):.3f}
  - í‰ê·  F1 ì ìˆ˜: {evaluation.get('average_f1', 0):.3f}
  - ì„±ê³µí•œ QA: {len([p for p in predictions if p.get('status') == 'success'])}ê°œ
  - ì‹¤íŒ¨í•œ QA: {len([p for p in predictions if p.get('status') == 'error'])}ê°œ

ğŸ“‚ ìƒì„±ëœ íŒŒì¼:
  - eval/benchmark_report.json
  - eval/BENCHMARK_REPORT.md
""")
            else:
                print(f"âŒ í‰ê°€ ì‹¤íŒ¨: {evaluation['error']}")
            
        except Exception as e:
            print(f"âŒ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()


def main():
    """ë©”ì¸ ì‹¤í–‰"""
    base_dir = Path(__file__).parent.parent
    runner = BenchmarkRunner(base_dir)
    runner.run_full_benchmark()


if __name__ == "__main__":
    main()


#!/usr/bin/env python3
"""
Gold Standard QA ë°ì´í„°ì…‹ ìƒì„±ê¸°

ëª©ì :
- ì—°êµ¬ í‰ê°€ë¥¼ ìœ„í•œ ê³ í’ˆì§ˆ Question-Answer ìŒ ìƒì„±
- BIM ì „ë¬¸ê°€ê°€ ê²€ì¦ ê°€ëŠ¥í•œ êµ¬ì¡°
- Graph-RAG, Forgetting ë©”ì»¤ë‹ˆì¦˜ í‰ê°€ìš©
"""

import json
import uuid
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any


class GoldStandardGenerator:
    """Gold Standard QA ìƒì„±ê¸°"""
    
    def __init__(self, base_dir: Path):
        self.base_dir = Path(base_dir)
        self.eval_dir = self.base_dir / "eval"
        self.eval_dir.mkdir(parents=True, exist_ok=True)
        
        # QA í…œí”Œë¦¿
        self.qa_templates = self._load_qa_templates()
    
    def _load_qa_templates(self) -> List[Dict[str, Any]]:
        """QA í…œí”Œë¦¿ ì •ì˜"""
        
        return [
            # === RQ1: Graph-RAG ê²€ìƒ‰ ì„±ëŠ¥ ===
            {
                "category": "entity_search",
                "subcategory": "ifc_lookup",
                "question_template": "GUID {guid}ëŠ” ì–´ë–¤ IFC ìš”ì†Œì¸ê°€ìš”?",
                "answer_template": "GUID {guid}ëŠ” {entity_type}ì…ë‹ˆë‹¤. ì´ë¦„: {name}",
                "difficulty": "easy",
                "requires": ["guid", "entity_type", "name"],
                "evaluation_type": "exact_match"
            },
            {
                "category": "entity_search",
                "subcategory": "bcf_issue_lookup",
                "question_template": "{guid}ì™€ ê´€ë ¨ëœ BCF ì´ìŠˆëŠ” ë¬´ì—‡ì¸ê°€ìš”?",
                "answer_template": "ì´ {count}ê°œì˜ ì´ìŠˆê°€ ìˆìŠµë‹ˆë‹¤: {issue_list}",
                "difficulty": "medium",
                "requires": ["guid", "issue_list", "count"],
                "evaluation_type": "set_match"
            },
            {
                "category": "relationship",
                "subcategory": "connected_components",
                "question_template": "{entity_name}ì™€ ì—°ê²°ëœ ë‹¤ë¥¸ ìš”ì†Œë“¤ì€ ë¬´ì—‡ì¸ê°€ìš”?",
                "answer_template": "{entity_name}ëŠ” ë‹¤ìŒê³¼ ì—°ê²°ë˜ì–´ ìˆìŠµë‹ˆë‹¤: {connected_list}",
                "difficulty": "medium",
                "requires": ["entity_name", "connected_list"],
                "evaluation_type": "set_match"
            },
            {
                "category": "temporal",
                "subcategory": "recent_issues",
                "question_template": "ìµœê·¼ {days}ì¼ ì´ë‚´ì— ìƒì„±ëœ ì´ìŠˆëŠ” ë¬´ì—‡ì¸ê°€ìš”?",
                "answer_template": "ìµœê·¼ {days}ì¼ ë‚´ {count}ê°œ ì´ìŠˆ: {issue_list}",
                "difficulty": "easy",
                "requires": ["days", "count", "issue_list"],
                "evaluation_type": "set_match"
            },
            {
                "category": "temporal",
                "subcategory": "old_issues",
                "question_template": "{days}ì¼ ì´ì „ì— ìƒì„±ëœ ì˜¤ë˜ëœ ì´ìŠˆëŠ”?",
                "answer_template": "{days}ì¼ ì´ì „ {count}ê°œ ì´ìŠˆ: {issue_list}",
                "difficulty": "easy",
                "requires": ["days", "count", "issue_list"],
                "evaluation_type": "set_match"
            },
            
            # === RQ2: ë§ê° ë©”ì»¤ë‹ˆì¦˜ í‰ê°€ ===
            {
                "category": "forgetting",
                "subcategory": "ttl_filter",
                "question_template": "TTL {ttl}ì¼ ê¸°ì¤€ìœ¼ë¡œ ìœ íš¨í•œ ì´ìŠˆë§Œ ë³´ì—¬ì£¼ì„¸ìš”.",
                "answer_template": "TTL {ttl}ì¼ ê¸°ì¤€ {count}ê°œ ì´ìŠˆê°€ ìœ íš¨í•©ë‹ˆë‹¤: {issue_list}",
                "difficulty": "medium",
                "requires": ["ttl", "count", "issue_list"],
                "evaluation_type": "set_match"
            },
            {
                "category": "forgetting",
                "subcategory": "old_version_citation",
                "question_template": "{guid}ì˜ ì´ì „ ë²„ì „ ì •ë³´ê°€ í¬í•¨ë˜ì–´ ìˆë‚˜ìš”?",
                "answer_template": "ë§ê° ì •ì±…ì— ë”°ë¼ {status}. ì´ìœ : {reason}",
                "difficulty": "hard",
                "requires": ["guid", "status", "reason"],
                "evaluation_type": "semantic_match"
            },
            {
                "category": "forgetting",
                "subcategory": "contradiction_detection",
                "question_template": "{entity_name}ì— ëŒ€í•œ ëª¨ìˆœëœ ì •ë³´ê°€ ìˆë‚˜ìš”?",
                "answer_template": "{status}. ì„¸ë¶€ì‚¬í•­: {details}",
                "difficulty": "hard",
                "requires": ["entity_name", "status", "details"],
                "evaluation_type": "semantic_match"
            },
            
            # === RQ3: ë³€ê²½ ì˜í–¥ ì¶”ì  ===
            {
                "category": "change_impact",
                "subcategory": "multi_hop",
                "question_template": "{entity_name}ì„ ë³€ê²½í•˜ë©´ ì–´ë–¤ ìš”ì†Œë“¤ì— ì˜í–¥ì„ ì£¼ë‚˜ìš”?",
                "answer_template": "{entity_name} ë³€ê²½ ì‹œ ì˜í–¥ ë°›ëŠ” ìš”ì†Œ: {affected_list}",
                "difficulty": "hard",
                "requires": ["entity_name", "affected_list"],
                "evaluation_type": "set_match"
            },
            {
                "category": "change_impact",
                "subcategory": "issue_propagation",
                "question_template": "ì´ìŠˆ '{issue_title}'ì´ í•´ê²°ë˜ì§€ ì•Šìœ¼ë©´ ì–´ë–¤ ì‘ì—…ì´ ë§‰íˆë‚˜ìš”?",
                "answer_template": "ë§‰íˆëŠ” ì‘ì—…: {blocked_tasks}. ì´ìœ : {reason}",
                "difficulty": "hard",
                "requires": ["issue_title", "blocked_tasks", "reason"],
                "evaluation_type": "semantic_match"
            },
            
            # === í†µí•© ì§ˆì˜ ===
            {
                "category": "complex",
                "subcategory": "statistics",
                "question_template": "í˜„ì¬ í”„ë¡œì íŠ¸ì˜ ì „ì²´ í†µê³„ë¥¼ ë³´ì—¬ì£¼ì„¸ìš”.",
                "answer_template": "ì´ IFC ìš”ì†Œ: {ifc_count}ê°œ, BCF ì´ìŠˆ: {bcf_count}ê°œ, í•´ê²°ë¨: {resolved_count}ê°œ",
                "difficulty": "easy",
                "requires": ["ifc_count", "bcf_count", "resolved_count"],
                "evaluation_type": "exact_match"
            },
            {
                "category": "complex",
                "subcategory": "author_activity",
                "question_template": "{author}ê°€ ì‘ì„±í•œ ì´ìŠˆëŠ” ë¬´ì—‡ì¸ê°€ìš”?",
                "answer_template": "{author} ì‘ì„± {count}ê°œ: {issue_list}",
                "difficulty": "easy",
                "requires": ["author", "count", "issue_list"],
                "evaluation_type": "set_match"
            },
        ]
    
    def generate_gold_standard(self, count: int = 50) -> List[Dict[str, Any]]:
        """Gold Standard QA ìŒ ìƒì„±"""
        
        print(f"ğŸ”¨ Gold Standard QA ë°ì´í„°ì…‹ ìƒì„± ì¤‘ ({count}ê°œ)...")
        
        qa_pairs = []
        
        # ì¹´í…Œê³ ë¦¬ë³„ ë¶„ë°°
        categories = {
            "entity_search": int(count * 0.25),      # 25%
            "relationship": int(count * 0.15),       # 15%
            "temporal": int(count * 0.15),           # 15%
            "forgetting": int(count * 0.20),         # 20%
            "change_impact": int(count * 0.15),      # 15%
            "complex": int(count * 0.10)             # 10%
        }
        
        qa_id = 1
        
        for category, target_count in categories.items():
            templates = [t for t in self.qa_templates if t["category"] == category]
            
            for i in range(target_count):
                template = templates[i % len(templates)]
                
                # QA ìŒ ìƒì„±
                qa_pair = {
                    "id": f"qa_{qa_id:03d}",
                    "category": template["category"],
                    "subcategory": template["subcategory"],
                    "difficulty": template["difficulty"],
                    "question": template["question_template"],
                    "answer": template["answer_template"],
                    "requires": template["requires"],
                    "evaluation_type": template["evaluation_type"],
                    "status": "template",  # ì‹¤ì œ ë°ì´í„°ë¡œ ì±„ì›Œì•¼ í•¨
                    "metadata": {
                        "created": datetime.now().isoformat(),
                        "validated": False,
                        "expert_reviewed": False
                    }
                }
                
                qa_pairs.append(qa_pair)
                qa_id += 1
        
        print(f"  âœ… {len(qa_pairs)}ê°œ QA í…œí”Œë¦¿ ìƒì„± ì™„ë£Œ")
        
        return qa_pairs
    
    def save_gold_standard(self, qa_pairs: List[Dict[str, Any]], filename: str = "gold_standard.jsonl"):
        """Gold Standard ì €ì¥"""
        
        output_path = self.eval_dir / filename
        
        with open(output_path, 'w', encoding='utf-8') as f:
            for qa in qa_pairs:
                f.write(json.dumps(qa, ensure_ascii=False) + '\n')
        
        print(f"  ğŸ’¾ ì €ì¥ ì™„ë£Œ: {output_path}")
        
        # í†µê³„ ìƒì„±
        stats = self._generate_statistics(qa_pairs)
        stats_path = self.eval_dir / "gold_standard_stats.json"
        
        with open(stats_path, 'w', encoding='utf-8') as f:
            json.dump(stats, f, indent=2, ensure_ascii=False)
        
        print(f"  ğŸ“Š í†µê³„ ì €ì¥: {stats_path}")
        
        return output_path
    
    def _generate_statistics(self, qa_pairs: List[Dict[str, Any]]) -> Dict[str, Any]:
        """QA ë°ì´í„°ì…‹ í†µê³„"""
        
        from collections import Counter
        
        stats = {
            "total_count": len(qa_pairs),
            "by_category": dict(Counter(qa["category"] for qa in qa_pairs)),
            "by_difficulty": dict(Counter(qa["difficulty"] for qa in qa_pairs)),
            "by_evaluation_type": dict(Counter(qa["evaluation_type"] for qa in qa_pairs)),
            "status": {
                "template": len([qa for qa in qa_pairs if qa["status"] == "template"]),
                "filled": len([qa for qa in qa_pairs if qa["status"] == "filled"]),
                "validated": len([qa for qa in qa_pairs if qa["metadata"]["validated"]])
            }
        }
        
        return stats
    
    def create_expert_validation_form(self, qa_pairs: List[Dict[str, Any]]):
        """ì „ë¬¸ê°€ ê²€ì¦ ì–‘ì‹ ìƒì„±"""
        
        form_path = self.eval_dir / "EXPERT_VALIDATION_FORM.md"
        
        with open(form_path, 'w', encoding='utf-8') as f:
            f.write(f"""# Gold Standard QA ì „ë¬¸ê°€ ê²€ì¦ ì–‘ì‹

**ìƒì„±ì¼**: {datetime.now().strftime('%Y-%m-%d')}  
**ì´ QA ìŒ**: {len(qa_pairs)}ê°œ  
**ê²€ì¦ì**: [ì „ë¬¸ê°€ ì´ë¦„]  
**ê²€ì¦ì¼**: [ë‚ ì§œ]

---

## ê²€ì¦ ê¸°ì¤€

ê° QA ìŒì— ëŒ€í•´ ë‹¤ìŒì„ í‰ê°€í•´ì£¼ì„¸ìš”:

1. **ì •í™•ì„± (Accuracy)**: ë‹µë³€ì´ ì •í™•í•œê°€?
   - âœ… ì •í™•í•¨
   - âš ï¸ ë¶€ë¶„ì ìœ¼ë¡œ ì •í™•
   - âŒ ë¶€ì •í™•

2. **ì™„ì „ì„± (Completeness)**: ë‹µë³€ì´ ì™„ì „í•œê°€?
   - âœ… ì™„ì „í•¨
   - âš ï¸ ì¶”ê°€ ì •ë³´ í•„ìš”
   - âŒ ë¶ˆì™„ì „

3. **í˜„ì‹¤ì„± (Realism)**: ì‹¤ì œ BIM í”„ë¡œì íŠ¸ì—ì„œ ë°œìƒí•  ë²•í•œ ì§ˆë¬¸ì¸ê°€?
   - âœ… í˜„ì‹¤ì 
   - âš ï¸ ê°€ëŠ¥í•˜ì§€ë§Œ ë“œë¬¾
   - âŒ ë¹„í˜„ì‹¤ì 

4. **ë‚œì´ë„ (Difficulty)**: ì„¤ì •ëœ ë‚œì´ë„ê°€ ì ì ˆí•œê°€?
   - âœ… ì ì ˆí•¨
   - âš ï¸ ì¡°ì • í•„ìš”
   - âŒ ë¶€ì ì ˆ

---

## QA ìŒ ë¦¬ìŠ¤íŠ¸

""")
            
            # ì¹´í…Œê³ ë¦¬ë³„ë¡œ ì •ë¦¬
            by_category = {}
            for qa in qa_pairs:
                cat = qa["category"]
                if cat not in by_category:
                    by_category[cat] = []
                by_category[cat].append(qa)
            
            for category, qas in sorted(by_category.items()):
                f.write(f"\n### {category.replace('_', ' ').title()} ({len(qas)}ê°œ)\n\n")
                
                for i, qa in enumerate(qas[:5], 1):  # ìƒ˜í”Œë¡œ 5ê°œë§Œ
                    f.write(f"#### {qa['id']}\n\n")
                    f.write(f"**ì§ˆë¬¸**: {qa['question']}\n\n")
                    f.write(f"**ë‹µë³€**: {qa['answer']}\n\n")
                    f.write(f"**ë‚œì´ë„**: {qa['difficulty']}\n\n")
                    f.write(f"**ê²€ì¦**:\n")
                    f.write(f"- [ ] ì •í™•ì„±: ___\n")
                    f.write(f"- [ ] ì™„ì „ì„±: ___\n")
                    f.write(f"- [ ] í˜„ì‹¤ì„±: ___\n")
                    f.write(f"- [ ] ë‚œì´ë„: ___\n")
                    f.write(f"- ì½”ë©˜íŠ¸: ___\n\n")
                    f.write("---\n\n")
                
                if len(qas) > 5:
                    f.write(f"_(... ë‚˜ë¨¸ì§€ {len(qas) - 5}ê°œ QA ìŒ ìƒëµ)_\n\n")
            
            f.write(f"""
---

## ì „ì²´ í‰ê°€

### ë°ì´í„°ì…‹ í’ˆì§ˆ

1. ì „ë°˜ì ì¸ í’ˆì§ˆ: [ ] ìš°ìˆ˜ [ ] ì–‘í˜¸ [ ] ë³´í†µ [ ] ê°œì„  í•„ìš”

2. ì£¼ìš” ê°•ì :
   - 
   - 

3. ê°œì„ ì´ í•„ìš”í•œ ë¶€ë¶„:
   - 
   - 

4. ì¶”ê°€ ì œì•ˆ ì‚¬í•­:
   - 
   - 

### ì„œëª…

**ê²€ì¦ì**: _______________  
**ì†Œì†**: _______________  
**ê²½ë ¥**: ___ ë…„  
**ë‚ ì§œ**: _______________

---

**ê²€ì¦ ì™„ë£Œ í›„ ì´ íŒŒì¼ì„ ë‹¤ìŒ ê²½ë¡œë¡œ ì œì¶œí•´ì£¼ì„¸ìš”**:
- ì´ë©”ì¼: [your-email@example.com]
- ë˜ëŠ”: eval/expert_reviews/[ê²€ì¦ìëª…]_validation.md
""")
        
        print(f"  ğŸ“‹ ì „ë¬¸ê°€ ê²€ì¦ ì–‘ì‹ ìƒì„±: {form_path}")
        
        return form_path


def main():
    """ë©”ì¸ ì‹¤í–‰"""
    
    base_dir = Path(__file__).parent.parent
    generator = GoldStandardGenerator(base_dir)
    
    print("=" * 70)
    print("ğŸ¯ Gold Standard QA ë°ì´í„°ì…‹ ìƒì„±")
    print("=" * 70)
    print()
    
    # 1. QA ìŒ ìƒì„±
    qa_pairs = generator.generate_gold_standard(count=50)
    
    # 2. ì €ì¥
    output_path = generator.save_gold_standard(qa_pairs)
    
    # 3. ì „ë¬¸ê°€ ê²€ì¦ ì–‘ì‹ ìƒì„±
    form_path = generator.create_expert_validation_form(qa_pairs)
    
    # ìš”ì•½
    print()
    print("=" * 70)
    print("âœ… Gold Standard ìƒì„± ì™„ë£Œ")
    print("=" * 70)
    print(f"""
ğŸ“Š í†µê³„:
  - ì´ QA ìŒ: {len(qa_pairs)}ê°œ
  - ì¹´í…Œê³ ë¦¬: 6ê°œ (Entity, Relationship, Temporal, Forgetting, Change, Complex)
  - ë‚œì´ë„: Easy/Medium/Hard

ğŸ“‚ ìƒì„± íŒŒì¼:
  - {output_path}
  - {output_path.parent / 'gold_standard_stats.json'}
  - {form_path}

ğŸ“ ë‹¤ìŒ ë‹¨ê³„:
  1. âš ï¸  í…œí”Œë¦¿ì„ ì‹¤ì œ ë°ì´í„°ë¡œ ì±„ìš°ê¸°
     â†’ python eval/fill_gold_standard.py
  
  2. ì „ë¬¸ê°€ ê²€ì¦ ì˜ë¢°
     â†’ eval/EXPERT_VALIDATION_FORM.md ì „ë‹¬
  
  3. í‰ê°€ ë©”íŠ¸ë¦­ êµ¬í˜„
     â†’ python eval/metrics.py
""")


if __name__ == "__main__":
    main()


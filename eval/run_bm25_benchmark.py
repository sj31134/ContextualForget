#!/usr/bin/env python3
"""
BM25 ì—”ì§„ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
"""

import sys
import json
import pickle
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any

# Add src to path
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

from contextualforget.baselines import BM25QueryEngine
from metrics import EvaluationMetrics


class BM25BenchmarkRunner:
    """BM25 ì—”ì§„ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ê¸°"""
    
    def __init__(self, base_dir: Path):
        self.base_dir = Path(base_dir)
        self.eval_dir = self.base_dir / "eval"
        self.graph_path = self.base_dir / "data" / "processed" / "graph.gpickle"
        
        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.graph = self._load_graph()
        self.bm25_engine = None
        self.metrics = EvaluationMetrics()
        
        # ê²°ê³¼ ì €ì¥
        self.results = {
            "timestamp": datetime.now().isoformat(),
            "engine": "BM25",
            "graph_stats": {},
            "qa_results": [],
            "summary": {}
        }
    
    def _load_graph(self):
        """ê·¸ë˜í”„ ë¡œë“œ"""
        if not self.graph_path.exists():
            raise FileNotFoundError(f"ê·¸ë˜í”„ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {self.graph_path}")
        
        with open(self.graph_path, 'rb') as f:
            graph = pickle.load(f)
        
        print(f"âœ… ê·¸ë˜í”„ ë¡œë“œ: {graph.number_of_nodes()}ê°œ ë…¸ë“œ, {graph.number_of_edges()}ê°œ ì—£ì§€")
        return graph
    
    def initialize_engine(self):
        """BM25 ì—”ì§„ ì´ˆê¸°í™”"""
        print("\nğŸ”§ BM25 ì—”ì§„ ì´ˆê¸°í™” ì¤‘...")
        
        # Prepare graph data for BM25
        graph_data = {"nodes": list(self.graph.nodes(data=True))}
        
        # Initialize BM25 engine
        self.bm25_engine = BM25QueryEngine()
        self.bm25_engine.initialize(graph_data)
        print("  âœ… BM25 ì—”ì§„ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def load_gold_standard(self) -> List[Dict[str, Any]]:
        """Gold Standard QA ë¡œë“œ"""
        gold_path = self.eval_dir / "gold.jsonl"
        
        if not gold_path.exists():
            raise FileNotFoundError(f"Gold Standard íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {gold_path}")
        
        qa_pairs = []
        with open(gold_path, 'r', encoding='utf-8') as f:
            for line in f:
                qa_pairs.append(json.loads(line))
        
        print(f"âœ… Gold Standard ë¡œë“œ: {len(qa_pairs)}ê°œ QA ìŒ")
        return qa_pairs
    
    def run_single_qa(self, qa: Dict[str, Any]) -> Dict[str, Any]:
        """ë‹¨ì¼ QA ì‹¤í–‰"""
        qa_id = qa.get('qa_id', 'unknown')
        question = qa.get('question', '')
        ground_truth = qa.get('ground_truth', {})
        
        try:
            # BM25 ì¿¼ë¦¬ ì‹¤í–‰
            result = self.bm25_engine.process_query(question)
            
            # ê²°ê³¼ ì¶”ì¶œ
            predicted_answer = result.get('answer', '')
            confidence = result.get('confidence', 0.0)
            
            # í‰ê°€ ë©”íŠ¸ë¦­ ê³„ì‚° (BM25 ë‹µë³€ í˜•ì‹ì— ë§ê²Œ ì¡°ì •)
            metrics = self._calculate_bm25_metrics(predicted_answer, ground_truth)
            
            return {
                "qa_id": qa_id,
                "question": question,
                "predicted_answer": predicted_answer,
                "ground_truth": ground_truth,
                "confidence": confidence,
                "metrics": metrics,
                "status": "success"
            }
            
        except Exception as e:
            return {
                "qa_id": qa_id,
                "question": question,
                "predicted_answer": "",
                "ground_truth": ground_truth,
                "confidence": 0.0,
                "metrics": {"exact_match": 0.0, "f1_score": 0.0, "semantic_match": 0.0},
                "status": "error",
                "error": str(e)
            }
    
    def _calculate_bm25_metrics(self, predicted_answer: str, ground_truth: Dict[str, Any]) -> Dict[str, float]:
        """BM25 ë‹µë³€ì— ëŒ€í•œ ë©”íŠ¸ë¦­ ê³„ì‚°"""
        if not predicted_answer or not ground_truth:
            return {"exact_match": 0.0, "f1_score": 0.0, "semantic_match": 0.0}
        
        # Ground truthì—ì„œ ë‹µë³€ ì¶”ì¶œ
        gt_answer = ground_truth.get('answer', '')
        if isinstance(gt_answer, list):
            gt_answer = ' '.join(str(item) for item in gt_answer)
        elif isinstance(gt_answer, dict):
            gt_answer = str(gt_answer)
        
        # Exact Match
        exact_match = 1.0 if predicted_answer.strip() == gt_answer.strip() else 0.0
        
        # F1 Score (ê°„ë‹¨í•œ í† í° ê¸°ë°˜)
        pred_tokens = set(predicted_answer.lower().split())
        gt_tokens = set(gt_answer.lower().split())
        
        if not gt_tokens:
            f1_score = 1.0 if not pred_tokens else 0.0
        else:
            intersection = pred_tokens & gt_tokens
            precision = len(intersection) / len(pred_tokens) if pred_tokens else 0.0
            recall = len(intersection) / len(gt_tokens)
            f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0
        
        # Semantic Match (ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜)
        semantic_score = 0.0
        if pred_tokens and gt_tokens:
            # ê³µí†µ í‚¤ì›Œë“œ ë¹„ìœ¨
            common_ratio = len(intersection) / len(gt_tokens)
            semantic_score = min(common_ratio, 1.0)
        
        return {
            "exact_match": exact_match,
            "f1_score": f1_score,
            "semantic_match": semantic_score
        }
    
    def run_benchmark(self):
        """ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
        print("\nğŸš€ BM25 ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ ì‹œì‘")
        print("=" * 50)
        
        # Gold Standard ë¡œë“œ
        qa_pairs = self.load_gold_standard()
        
        # ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
        print(f"\nğŸ¯ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ ì‹œì‘ ({len(qa_pairs)}ê°œ QA)")
        
        for i, qa in enumerate(qa_pairs, 1):
            print(f"  ì§„í–‰ë¥ : {i}/{len(qa_pairs)} - {qa.get('qa_id', 'unknown')}")
            
            result = self.run_single_qa(qa)
            self.results["qa_results"].append(result)
        
        print(f"âœ… ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ: {len(self.results['qa_results'])}ê°œ ê²°ê³¼")
        
        # ê²°ê³¼ í‰ê°€
        self._evaluate_results()
        
        # ë³´ê³ ì„œ ìƒì„±
        self._generate_report()
    
    def _evaluate_results(self):
        """ê²°ê³¼ í‰ê°€"""
        print("\nğŸ“Š ê²°ê³¼ í‰ê°€ ì¤‘...")
        
        results = self.results["qa_results"]
        successful_results = [r for r in results if r["status"] == "success"]
        
        print(f"  í‰ê°€ ëŒ€ìƒ: {len(successful_results)}ê°œ (ì„±ê³µí•œ QA)")
        
        if not successful_results:
            print("  âš ï¸ ì„±ê³µí•œ QAê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # ì „ì²´ í†µê³„
        total_qa = len(successful_results)
        exact_matches = sum(1 for r in successful_results if r["metrics"]["exact_match"] > 0)
        avg_f1 = sum(r["metrics"]["f1_score"] for r in successful_results) / total_qa
        avg_semantic = sum(r["metrics"]["semantic_match"] for r in successful_results) / total_qa
        
        # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„
        categories = {}
        for result in successful_results:
            # QA IDì—ì„œ ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ (ì˜ˆ: qa_entity_1 -> entity_search)
            qa_id = result["qa_id"]
            if "entity" in qa_id:
                category = "entity_search"
            elif "temporal" in qa_id:
                category = "temporal"
            elif "forgetting" in qa_id:
                category = "forgetting"
            elif "complex" in qa_id:
                category = "complex"
            else:
                category = "unknown"
            
            if category not in categories:
                categories[category] = []
            categories[category].append(result)
        
        # ìš”ì•½ ì €ì¥
        self.results["summary"] = {
            "total_qa": total_qa,
            "exact_match_count": exact_matches,
            "exact_match_rate": exact_matches / total_qa if total_qa > 0 else 0,
            "average_f1": avg_f1,
            "average_semantic_match": avg_semantic,
            "categories": {}
        }
        
        # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„
        for category, category_results in categories.items():
            cat_total = len(category_results)
            cat_exact = sum(1 for r in category_results if r["metrics"]["exact_match"] > 0)
            cat_avg_f1 = sum(r["metrics"]["f1_score"] for r in category_results) / cat_total if cat_total > 0 else 0
            
            self.results["summary"]["categories"][category] = {
                "count": cat_total,
                "exact_match_rate": cat_exact / cat_total if cat_total > 0 else 0,
                "average_f1": cat_avg_f1
            }
    
    def _generate_report(self):
        """ë³´ê³ ì„œ ìƒì„±"""
        print("\nğŸ“ ë³´ê³ ì„œ ìƒì„± ì¤‘...")
        
        # JSON ë³´ê³ ì„œ
        json_path = self.eval_dir / "bm25_benchmark_report.json"
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, ensure_ascii=False, indent=2)
        print(f"  âœ… JSON ë³´ê³ ì„œ: {json_path}")
        
        # Markdown ë³´ê³ ì„œ
        md_path = self.eval_dir / "BM25_BENCHMARK_REPORT.md"
        self._generate_markdown_report(md_path)
        print(f"  âœ… Markdown ë³´ê³ ì„œ: {md_path}")
        
        # ìµœì¢… ê²°ê³¼ ì¶œë ¥
        summary = self.results["summary"]
        print(f"\n======================================================================")
        print(f"âœ… BM25 ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ!")
        print(f"======================================================================")
        print(f"\nğŸ“Š ìµœì¢… ê²°ê³¼:")
        print(f"  - í‰ê·  ì •í™•ë„: {summary['exact_match_rate']:.3f}")
        print(f"  - í‰ê·  F1 ì ìˆ˜: {summary['average_f1']:.3f}")
        print(f"  - ì„±ê³µí•œ QA: {summary['total_qa']}ê°œ")
        print(f"  - ì‹¤íŒ¨í•œ QA: {len(self.results['qa_results']) - summary['total_qa']}ê°œ")
        
        print(f"\nğŸ“‚ ìƒì„±ëœ íŒŒì¼:")
        print(f"  - {json_path}")
        print(f"  - {md_path}")
    
    def _generate_markdown_report(self, output_path: Path):
        """Markdown ë³´ê³ ì„œ ìƒì„±"""
        summary = self.results["summary"]
        
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write("# BM25 ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ ë³´ê³ ì„œ\n\n")
            f.write(f"**ì‹¤í–‰ ì¼ì‹œ**: {self.results['timestamp']}\n\n")
            
            # ê·¸ë˜í”„ í†µê³„
            f.write("## ğŸ“Š ê·¸ë˜í”„ í†µê³„\n\n")
            f.write(f"- **ë…¸ë“œ ìˆ˜**: {self.graph.number_of_nodes()}ê°œ\n")
            f.write(f"- **ì—£ì§€ ìˆ˜**: {self.graph.number_of_edges()}ê°œ\n")
            f.write(f"- **ë°©í–¥ì„±**: Directed\n\n")
            
            # QA í†µê³„
            f.write("## ğŸ“‹ QA í†µê³„\n\n")
            f.write(f"- **ì´ QA ìŒ**: {len(self.results['qa_results'])}ê°œ\n")
            f.write(f"- **ì„±ê³µí•œ QA**: {summary['total_qa']}ê°œ\n")
            f.write(f"- **ì‹¤íŒ¨í•œ QA**: {len(self.results['qa_results']) - summary['total_qa']}ê°œ\n")
            f.write(f"- **ì„±ê³µë¥ **: {summary['total_qa'] / len(self.results['qa_results']) * 100:.1f}%\n\n")
            
            # ì¹´í…Œê³ ë¦¬ë³„ ë¶„í¬
            f.write("### ì¹´í…Œê³ ë¦¬ë³„ ë¶„í¬\n\n")
            for category, stats in summary["categories"].items():
                f.write(f"- **{category}**: {stats['count']}ê°œ\n")
            
            # í‰ê°€ ê²°ê³¼
            f.write("\n## ğŸ¯ í‰ê°€ ê²°ê³¼\n\n")
            f.write(f"- **í‰ê·  ì •í™•ë„**: {summary['exact_match_rate']:.3f}\n")
            f.write(f"- **í‰ê·  F1 ì ìˆ˜**: {summary['average_f1']:.3f}\n")
            f.write(f"- **ì´ ì§ˆë¬¸ ìˆ˜**: {summary['total_qa']}ê°œ\n\n")
            
            # ì¹´í…Œê³ ë¦¬ë³„ ì„±ëŠ¥
            f.write("### ì¹´í…Œê³ ë¦¬ë³„ ì„±ëŠ¥\n\n")
            for category, stats in summary["categories"].items():
                f.write(f"- **{category}**: ì •í™•ë„ {stats['exact_match_rate']:.3f}, F1 {stats['average_f1']:.3f}\n")
            
            # ìƒì„¸ ê²°ê³¼
            f.write("\n## ğŸ“ˆ ìƒì„¸ ê²°ê³¼\n\n")
            f.write("### ì„±ê³µí•œ QA ìŒ\n\n")
            
            successful_results = [r for r in self.results["qa_results"] if r["status"] == "success"]
            for result in successful_results[:10]:  # ì²˜ìŒ 10ê°œë§Œ
                f.write(f"#### {result['qa_id']}\n\n")
                f.write(f"**ì§ˆë¬¸**: {result['question']}\n\n")
                f.write(f"**ì˜ˆì¸¡ ë‹µë³€**: {result['predicted_answer']}\n\n")
                f.write(f"**ìƒíƒœ**: âœ… ì„±ê³µ\n\n")
            
            f.write("\n---\n")
            f.write(f"**ë³´ê³ ì„œ ìƒì„±**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write("**ë²¤ì¹˜ë§ˆí¬ ë²„ì „**: 1.0\n")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    base_dir = Path(__file__).parent.parent
    runner = BM25BenchmarkRunner(base_dir)
    
    try:
        runner.initialize_engine()
        runner.run_benchmark()
    except Exception as e:
        print(f"âŒ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return 1
    
    return 0


if __name__ == "__main__":
    exit(main())

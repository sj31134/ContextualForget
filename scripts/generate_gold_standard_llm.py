"""
Ollama LLMì„ ì‚¬ìš©í•œ Gold Standard QA ìƒì„±
qwen2.5:3b ëª¨ë¸ë¡œ 100ê°œì˜ QA ìŒ ìƒì„±
"""
import argparse
import json
import subprocess
import sys
from pathlib import Path
from typing import Dict, List

# Add src to path
sys.path.insert(0, str(Path(__file__).parent.parent / 'src'))

from contextualforget.core import write_jsonl


def generate_qa_prompt(sample_type: str, data: Dict) -> str:
    """
    ìƒ˜í”Œ íƒ€ì…ì— ë”°ë¥¸ LLM í”„ë¡¬í”„íŠ¸ ìƒì„±
    
    Args:
        sample_type: 'entity_search', 'issue_search', 'relationship'
        data: ìƒ˜í”Œ ë°ì´í„°
        
    Returns:
        LLM í”„ë¡¬í”„íŠ¸ ë¬¸ìì—´
    """
    if sample_type == 'entity_search':
        return f"""ë‹¹ì‹ ì€ BIM ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ IFC ì—”í‹°í‹°ì— ëŒ€í•œ ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ ì§ˆë¬¸-ë‹µë³€ ìŒì„ ìƒì„±í•˜ì„¸ìš”.

IFC ì—”í‹°í‹°:
- GUID: {data['guid']}
- íƒ€ì…: {data['type']}
- ì´ë¦„: {data.get('name', 'N/A')}

JSON í˜•ì‹ìœ¼ë¡œ ì§ˆë¬¸-ë‹µë³€ì„ ìƒì„±í•˜ì„¸ìš”:
{{"question": "ì´ GUIDì˜ ì—”í‹°í‹° íƒ€ì…ì€ ë¬´ì—‡ì¸ê°€ìš”?", "answer": "ì´ ì—”í‹°í‹°ëŠ” {data['type']}ì…ë‹ˆë‹¤.", "gold_entities": ["{data['guid']}"]}}

JSONë§Œ ì¶œë ¥í•˜ì„¸ìš”. ë‹¤ë¥¸ í…ìŠ¤íŠ¸ëŠ” í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”."""

    elif sample_type == 'issue_search':
        return f"""ë‹¹ì‹ ì€ BIM ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ BCF ì´ìŠˆì— ëŒ€í•œ ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ ì§ˆë¬¸-ë‹µë³€ ìŒì„ ìƒì„±í•˜ì„¸ìš”.

BCF ì´ìŠˆ:
- Topic ID: {data['topic_id']}
- ì œëª©: {data.get('title', 'N/A')}
- ì„¤ëª…: {data.get('description', 'N/A')[:100]}
- ìš°ì„ ìˆœìœ„: {data.get('priority', 'N/A')}

JSON í˜•ì‹ìœ¼ë¡œ ì§ˆë¬¸-ë‹µë³€ì„ ìƒì„±í•˜ì„¸ìš”:
{{"question": "ì´ ì´ìŠˆì˜ ì œëª©ì€ ë¬´ì—‡ì¸ê°€ìš”?", "answer": "{data.get('title', 'N/A')}", "gold_entities": ["{data['topic_id']}"]}}

JSONë§Œ ì¶œë ¥í•˜ì„¸ìš”. ë‹¤ë¥¸ í…ìŠ¤íŠ¸ëŠ” í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”."""

    elif sample_type == 'relationship':
        return f"""ë‹¹ì‹ ì€ BIM ì „ë¬¸ê°€ì…ë‹ˆë‹¤. BCF ì´ìŠˆì™€ IFC ì—”í‹°í‹°ì˜ ê´€ê³„ì— ëŒ€í•œ ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ ì§ˆë¬¸-ë‹µë³€ ìŒì„ ìƒì„±í•˜ì„¸ìš”.

BCF ì´ìŠˆ:
- ì œëª©: {data.get('bcf_title', 'N/A')}
- ì„¤ëª…: {data.get('bcf_description', 'N/A')[:100]}

ì—°ê²°ëœ IFC:
- GUID: {data['ifc_guid']}
- íƒ€ì…: {data.get('ifc_type', 'N/A')}
- ì´ë¦„: {data.get('ifc_name', 'N/A')}

JSON í˜•ì‹ìœ¼ë¡œ ì§ˆë¬¸-ë‹µë³€ì„ ìƒì„±í•˜ì„¸ìš”:
{{"question": "ì´ ì´ìŠˆì™€ ê´€ë ¨ëœ IFC ì—”í‹°í‹°ëŠ” ë¬´ì—‡ì¸ê°€ìš”?", "answer": "{data.get('ifc_type', 'N/A')} (GUID: {data['ifc_guid']})", "gold_entities": ["{data['ifc_guid']}"]}}

JSONë§Œ ì¶œë ¥í•˜ì„¸ìš”. ë‹¤ë¥¸ í…ìŠ¤íŠ¸ëŠ” í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”."""

    return ""


def call_ollama(prompt: str, model: str = "qwen2.5:3b") -> str:
    """
    Ollama LLM í˜¸ì¶œ
    
    Args:
        prompt: LLM í”„ë¡¬í”„íŠ¸
        model: ì‚¬ìš©í•  ëª¨ë¸ ì´ë¦„
        
    Returns:
        LLM ì‘ë‹µ ë¬¸ìì—´
    """
    try:
        result = subprocess.run(
            ['ollama', 'run', model, prompt],
            capture_output=True,
            text=True,
            timeout=30
        )
        return result.stdout.strip()
    except subprocess.TimeoutExpired:
        print(f"âš ï¸  Ollama íƒ€ì„ì•„ì›ƒ")
        return ""
    except Exception as e:
        print(f"âš ï¸  Ollama í˜¸ì¶œ ì‹¤íŒ¨: {e}")
        return ""


def extract_json_from_response(response: str) -> Dict:
    """
    LLM ì‘ë‹µì—ì„œ JSON ì¶”ì¶œ
    
    Args:
        response: LLM ì‘ë‹µ ë¬¸ìì—´
        
    Returns:
        íŒŒì‹±ëœ JSON ë”•ì…”ë„ˆë¦¬
    """
    # JSON ì½”ë“œ ë¸”ë¡ ì œê±°
    response = response.strip()
    if response.startswith('```json'):
        response = response[7:]
    if response.startswith('```'):
        response = response[3:]
    if response.endswith('```'):
        response = response[:-3]
    
    response = response.strip()
    
    # JSON íŒŒì‹± ì‹œë„
    try:
        return json.loads(response)
    except json.JSONDecodeError:
        # JSONì´ ì•„ë‹Œ ê²½ìš° ì²« ë²ˆì§¸ { } ì°¾ê¸°
        start = response.find('{')
        end = response.rfind('}')
        if start != -1 and end != -1:
            try:
                return json.loads(response[start:end+1])
            except json.JSONDecodeError:
                pass
        return None


def generate_qa_pairs(samples_file: str, output_file: str, model: str = "qwen2.5:3b"):
    """
    ìƒ˜í”Œì—ì„œ QA ìŒ ìƒì„±
    
    Args:
        samples_file: ëŒ€í‘œ ìƒ˜í”Œ JSON íŒŒì¼
        output_file: ì¶œë ¥ JSONL íŒŒì¼
        model: Ollama ëª¨ë¸ ì´ë¦„
    """
    print(f"ğŸ“‚ ìƒ˜í”Œ ë¡œë“œ ì¤‘: {samples_file}")
    with open(samples_file, 'r', encoding='utf-8') as f:
        samples_data = json.load(f)
    
    # ìƒ˜í”Œ ì¤€ë¹„
    all_samples = []
    
    # IFC ìƒ˜í”Œ (30ê°œ â†’ entity_search)
    for sample in samples_data['ifc_samples']:
        all_samples.append({
            'type': 'entity_search',
            'data': sample
        })
    
    # BCF ìƒ˜í”Œ (30ê°œ â†’ issue_search)
    for sample in samples_data['bcf_samples']:
        all_samples.append({
            'type': 'issue_search',
            'data': sample
        })
    
    # ì—°ê²°ëœ ìŒ (40ê°œ â†’ relationship)
    for sample in samples_data['connected_pairs']:
        all_samples.append({
            'type': 'relationship',
            'data': sample
        })
    
    print(f"âœ… ì´ ìƒ˜í”Œ: {len(all_samples)}ê°œ")
    print(f"   entity_search: {len(samples_data['ifc_samples'])}ê°œ")
    print(f"   issue_search: {len(samples_data['bcf_samples'])}ê°œ")
    print(f"   relationship: {len(samples_data['connected_pairs'])}ê°œ")
    
    # QA ìƒì„±
    print(f"\nğŸ¤– Ollama LLM ({model})ìœ¼ë¡œ QA ìƒì„± ì¤‘...")
    print(f"   ì˜ˆìƒ ì‹œê°„: ~{len(all_samples) * 5 / 60:.1f}ë¶„")
    
    qa_pairs = []
    success_count = 0
    
    for i, sample in enumerate(all_samples):
        if (i + 1) % 10 == 0:
            print(f"   ì§„í–‰: {i+1}/{len(all_samples)} ({(i+1)/len(all_samples)*100:.1f}%)")
        
        # í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = generate_qa_prompt(sample['type'], sample['data'])
        
        # LLM í˜¸ì¶œ
        response = call_ollama(prompt, model)
        
        if not response:
            print(f"   âš ï¸  QA {i+1} ìƒì„± ì‹¤íŒ¨: ì‘ë‹µ ì—†ìŒ")
            continue
        
        # JSON íŒŒì‹±
        qa = extract_json_from_response(response)
        
        if qa and 'question' in qa and 'answer' in qa:
            qa['id'] = f"qa_llm_{i:03d}"
            qa['category'] = sample['type']
            qa['metadata'] = {
                'llm_generated': True,
                'model': model,
                'validated': False,
                'sample_index': i
            }
            
            # gold_entitiesê°€ ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸
            if 'gold_entities' not in qa:
                qa['gold_entities'] = []
            
            qa_pairs.append(qa)
            success_count += 1
        else:
            print(f"   âš ï¸  QA {i+1} ìƒì„± ì‹¤íŒ¨: JSON íŒŒì‹± ì˜¤ë¥˜")
            print(f"      ì‘ë‹µ: {response[:100]}...")
    
    print(f"\nğŸ“Š ìƒì„± ê²°ê³¼:")
    print(f"   ì„±ê³µ: {success_count}ê°œ")
    print(f"   ì‹¤íŒ¨: {len(all_samples) - success_count}ê°œ")
    print(f"   ì„±ê³µë¥ : {success_count/len(all_samples)*100:.1f}%")
    
    # ì €ì¥
    output_path = Path(output_file)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    write_jsonl(str(output_path), qa_pairs)
    
    print(f"\nâœ… QA ì €ì¥ ì™„ë£Œ: {output_file}")
    print(f"   ì´ QA ìŒ: {len(qa_pairs)}ê°œ")
    
    # ì¹´í…Œê³ ë¦¬ë³„ ë¶„í¬
    from collections import Counter
    categories = Counter([qa['category'] for qa in qa_pairs])
    print(f"\nğŸ“Š ì¹´í…Œê³ ë¦¬ ë¶„í¬:")
    for cat, count in categories.items():
        print(f"   {cat}: {count}ê°œ")
    
    return qa_pairs


def main():
    ap = argparse.ArgumentParser(description='Ollama LLM ê¸°ë°˜ Gold Standard QA ìƒì„±')
    ap.add_argument("--samples", default="data/processed/representative_samples.json", 
                    help="ëŒ€í‘œ ìƒ˜í”Œ JSON íŒŒì¼")
    ap.add_argument("--output", default="eval/gold_standard_v2.jsonl",
                    help="ì¶œë ¥ JSONL íŒŒì¼")
    ap.add_argument("--model", default="qwen2.5:3b",
                    help="Ollama ëª¨ë¸ ì´ë¦„")
    a = ap.parse_args()
    
    # Ollama ì„¤ì¹˜ í™•ì¸
    try:
        subprocess.run(['ollama', '--version'], capture_output=True, check=True)
    except (subprocess.CalledProcessError, FileNotFoundError):
        print("âŒ Ollamaê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("   ì„¤ì¹˜ ë°©ë²•: https://ollama.ai")
        return 1
    
    # ëª¨ë¸ í™•ì¸
    print(f"ğŸ” Ollama ëª¨ë¸ í™•ì¸ ì¤‘: {a.model}")
    try:
        subprocess.run(['ollama', 'list'], capture_output=True, check=True)
        print(f"âœ… Ollama ì¤€ë¹„ ì™„ë£Œ")
    except subprocess.CalledProcessError:
        print(f"âš ï¸  ëª¨ë¸ {a.model}ë¥¼ ë‹¤ìš´ë¡œë“œí•´ì•¼ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        print(f"   ì‹¤í–‰: ollama pull {a.model}")
    
    # QA ìƒì„±
    qa_pairs = generate_qa_pairs(a.samples, a.output, a.model)
    
    # ê²€ì¦
    if len(qa_pairs) >= 90:
        print(f"\nâœ… ëª©í‘œ ë‹¬ì„±: {len(qa_pairs)}ê°œ >= 90ê°œ (90% ì„±ê³µë¥ )")
        return 0
    else:
        print(f"\nâš ï¸  ëª©í‘œ ë¯¸ë‹¬: {len(qa_pairs)}ê°œ < 90ê°œ")
        return 1


if __name__ == "__main__":
    sys.exit(main())


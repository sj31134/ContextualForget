"""
Comprehensive Evaluation of RAG Systems
ì¢…í•© í‰ê°€ ë° ì„±ëŠ¥ ë¹„êµ
"""

import json
import pickle
import sys
import time
from datetime import datetime, timezone
from pathlib import Path
from typing import Any, Dict, List, Optional
import numpy as np
import pandas as pd

# í”„ë¡œì íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.insert(0, str(Path(__file__).parent.parent / 'src'))

from contextualforget.query.contextual_forget_engine import ContextualForgetEngine
from contextualforget.query.adaptive_retrieval import HybridRetrievalEngine
from contextualforget.baselines.bm25_engine import BM25QueryEngine
from contextualforget.baselines.vector_engine import VectorQueryEngine


class ComprehensiveRAGEvaluator:
    """ì¢…í•© RAG ì‹œìŠ¤í…œ í‰ê°€ì"""
    
    def __init__(self, graph_path: str = 'data/processed/graph.gpickle'):
        self.graph_path = graph_path
        self.graph = None
        self.engines = {}
        self.evaluation_results = {}
        
        # í‰ê°€ ë©”íŠ¸ë¦­
        self.metrics = {
            'response_time': [],
            'confidence': [],
            'result_count': [],
            'success_rate': [],
            'relevance_score': []
        }
        
    def load_graph(self):
        """ê·¸ë˜í”„ ë¡œë“œ"""
        print("ğŸ“Š ê·¸ë˜í”„ ë¡œë“œ ì¤‘...")
        with open(self.graph_path, 'rb') as f:
            self.graph = pickle.load(f)
        print(f"âœ… ê·¸ë˜í”„ ë¡œë“œ ì™„ë£Œ: {self.graph.number_of_nodes()}ê°œ ë…¸ë“œ, {self.graph.number_of_edges()}ê°œ ì—£ì§€")
    
    def initialize_engines(self):
        """ëª¨ë“  ì—”ì§„ ì´ˆê¸°í™”"""
        print("\\nğŸ”§ ì—”ì§„ë“¤ ì´ˆê¸°í™” ì¤‘...")
        
        # ë°ì´í„° ì¶”ì¶œ
        bcf_data = []
        ifc_data = []
        for node, data in self.graph.nodes(data=True):
            if node[0] == 'BCF':
                bcf_data.append(data)
            elif node[0] == 'IFC':
                ifc_data.append(data)
        
        print(f"   ğŸ“‹ BCF ë°ì´í„°: {len(bcf_data)}ê°œ")
        print(f"   ğŸ“‹ IFC ë°ì´í„°: {len(ifc_data)}ê°œ")
        
        # BM25 ì—”ì§„
        print("   ğŸ” BM25 ì—”ì§„ ì´ˆê¸°í™”...")
        bm25_engine = BM25QueryEngine()
        bm25_engine.initialize({'bcf_data': bcf_data})
        self.engines['BM25'] = bm25_engine
        
        # Vector ì—”ì§„
        print("   ğŸ” Vector ì—”ì§„ ì´ˆê¸°í™”...")
        vector_engine = VectorQueryEngine()
        vector_engine.initialize({
            'bcf_data': bcf_data,
            'ifc_data': ifc_data,
            'graph': self.graph
        })
        self.engines['Vector'] = vector_engine
        
        # ContextualForget ì—”ì§„
        print("   ğŸ” ContextualForget ì—”ì§„ ì´ˆê¸°í™”...")
        contextual_engine = ContextualForgetEngine(
            self.graph, 
            enable_contextual_forgetting=True
        )
        self.engines['ContextualForget'] = contextual_engine
        
        # í•˜ì´ë¸Œë¦¬ë“œ ì—”ì§„
        print("   ğŸ” í•˜ì´ë¸Œë¦¬ë“œ ì—”ì§„ ì´ˆê¸°í™”...")
        hybrid_engine = HybridRetrievalEngine(
            base_engines=self.engines.copy(),
            fusion_strategy='adaptive',
            enable_adaptation=True
        )
        self.engines['Hybrid'] = hybrid_engine
        
        print("âœ… ëª¨ë“  ì—”ì§„ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def create_comprehensive_test_queries(self) -> List[Dict[str, Any]]:
        """ì¢…í•© í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ìƒì„±"""
        # ì‹¤ì œ ê·¸ë˜í”„ì—ì„œ ê²€ì¦ëœ í‚¤ì›Œë“œë§Œ ì¶”ì¶œ
        verified_keywords = []
        for node, data in self.graph.nodes(data=True):
            if node[0] == 'BCF':
                title = data.get('title', '')
                if title and len(title) > 2:
                    # ê° í‚¤ì›Œë“œë¡œ ì‹¤ì œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
                    keywords = [w for w in title.split() if len(w) > 1]
                    for kw in keywords:
                        # ê·¸ë˜í”„ì—ì„œ ì´ í‚¤ì›Œë“œë¡œ ì°¾ì„ ìˆ˜ ìˆëŠ”ì§€ í™•ì¸
                        matches = sum(1 for _, d in self.graph.nodes(data=True) 
                                     if d.get('title', '').lower().count(kw.lower()) > 0)
                        if matches > 0:
                            verified_keywords.append((kw, matches))
        
        # ë§¤ì¹­ ê°œìˆ˜ê°€ ë§ì€ ìˆœìœ¼ë¡œ ì •ë ¬
        verified_keywords.sort(key=lambda x: x[1], reverse=True)
        
        # ìƒìœ„ í‚¤ì›Œë“œë¡œ í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ìƒì„±
        test_queries = []
        for keyword, expected_count in verified_keywords[:10]:
            test_queries.append({
                'id': f'kw_{len(test_queries)+1}',
                'question': f'{keyword} ê´€ë ¨ ì´ìŠˆê°€ ìˆë‚˜ìš”?',
                'type': 'keyword',
                'expected_keywords': [keyword],
                'expected_min_count': min(expected_count, 5),
                'category': 'keyword_search'
            })
        
        # ì‘ì„±ì ê¸°ë°˜ ì¿¼ë¦¬ (ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ì‘ì„±ìë§Œ)
        authors = set()
        for node, data in self.graph.nodes(data=True):
            if node[0] == 'BCF':
                author = data.get('author', '')
                if author:
                    authors.add(author)
        
        author_list = list(authors)[:5]
        for author in author_list:
            test_queries.append({
                'id': f'auth_{len(test_queries)+1}',
                'question': f'{author}ê°€ ì‘ì„±í•œ ì´ìŠˆëŠ”?',
                'type': 'author',
                'expected_author': author,
                'category': 'author_search'
            })
        
        # GUID ê¸°ë°˜ ì¿¼ë¦¬ (ì‹¤ì œ ì¡´ì¬í•˜ëŠ” GUIDë§Œ)
        guids = set()
        for node, data in self.graph.nodes(data=True):
            if node[0] == 'BCF':
                topic_id = data.get('topic_id', '')
                if topic_id:
                    guids.add(topic_id)
        
        guid_list = list(guids)[:5]
        for guid in guid_list:
            test_queries.append({
                'id': f'guid_{len(test_queries)+1}',
                'question': f'{guid}ì™€ ê´€ë ¨ëœ ë¬¸ì œëŠ”?',
                'type': 'guid',
                'expected_guid': guid,
                'category': 'guid_search'
            })
        
        # ê²€ì¦ëœ ë³µí•© ì¿¼ë¦¬
        complex_queries = [
            {
                'id': f'comp_{len(test_queries)+1}',
                'question': 'ë¬´ê· ì‹¤ ë§ˆê° ê´€ë ¨ ë¬¸ì œì ì€?',
                'type': 'complex',
                'expected_keywords': ['ë¬´ê· ì‹¤', 'ë§ˆê°'],
                'category': 'complex_search'
            },
            {
                'id': f'comp_{len(test_queries)+1}',
                'question': 'ìµœê·¼ ë°œìƒí•œ ë²½ì²´ ê´€ë ¨ ì´ìŠˆëŠ”?',
                'type': 'temporal',
                'expected_keywords': ['ë²½ì²´'],
                'category': 'temporal_search'
            }
        ]
        test_queries.extend(complex_queries)
        
        return test_queries
    
    def evaluate_engine(self, engine_name: str, query: Dict[str, Any]) -> Dict[str, Any]:
        """ë‹¨ì¼ ì—”ì§„ í‰ê°€"""
        start_time = time.perf_counter()
        
        try:
            engine = self.engines[engine_name]
            question = query['question']
            
            # ì—”ì§„ë³„ ì¿¼ë¦¬ ì‹¤í–‰
            if engine_name == 'BM25':
                result = engine.process_query(question)
            elif engine_name == 'Vector':
                result = engine.process_query(question)
            elif engine_name == 'ContextualForget':
                result = engine.contextual_query(question, query_type=query['type'])
            elif engine_name == 'Hybrid':
                result = engine.query(question)
                # í•˜ì´ë¸Œë¦¬ë“œ ê²°ê³¼ì—ì„œ ì‹¤ì œ ê²°ê³¼ ì¶”ì¶œ
                if 'result' in result:
                    result = result['result']
            else:
                result = {'error': 'Unknown engine'}
            
            response_time = max(time.perf_counter() - start_time, 0.0001)
            
            # ê²°ê³¼ ë¶„ì„
            if isinstance(result, dict) and 'error' not in result:
                confidence = result.get('confidence', 0.0)
                result_count = result.get('result_count', 0)
                # result_countê°€ 0ì´ê³  detailsì— ì •ë³´ê°€ ìˆìœ¼ë©´ ì¶”ì¶œ
                if result_count == 0 and 'details' in result:
                    details = result['details']
                    result_count = details.get('count', details.get('total_results', 0))
                # ì„±ê³µ ê¸°ì¤€: ê²°ê³¼ê°€ ìˆê³  ì‹ ë¢°ë„ê°€ ì¼ì • ìˆ˜ì¤€ ì´ìƒ
                success = result_count > 0 and confidence > 0.1
                
                # ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°
                relevance_score = self._calculate_relevance_score(query, result)
                
                return {
                    'engine': engine_name,
                    'query_id': query['id'],
                    'question': question,
                    'query_type': query['type'],
                    'category': query['category'],
                    'answer': result.get('answer', 'No answer'),
                    'confidence': confidence,
                    'response_time': response_time,
                    'result_count': result_count,
                    'success': success,
                    'relevance_score': relevance_score,
                    'error': None
                }
            else:
                return {
                    'engine': engine_name,
                    'query_id': query['id'],
                    'question': question,
                    'query_type': query['type'],
                    'category': query['category'],
                    'answer': 'Error occurred',
                    'confidence': 0.0,
                    'response_time': response_time,
                    'result_count': 0,
                    'success': False,
                    'relevance_score': 0.0,
                    'error': result.get('error', 'Unknown error')
                }
                
        except Exception as e:
            response_time = max(time.perf_counter() - start_time, 0.0001)
            return {
                'engine': engine_name,
                'query_id': query['id'],
                'question': question,
                'query_type': query['type'],
                'category': query['category'],
                'answer': 'Exception occurred',
                'confidence': 0.0,
                'response_time': response_time,
                'result_count': 0,
                'success': False,
                'relevance_score': 0.0,
                'error': str(e)
            }
    
    def _calculate_relevance_score(self, query: Dict[str, Any], result: Dict[str, Any]) -> float:
        """ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°"""
        try:
            answer = result.get('answer', '').lower()
            question = query['question'].lower()
            
            # í‚¤ì›Œë“œ ë§¤ì¹­
            if 'expected_keywords' in query:
                expected_keywords = [kw.lower() for kw in query['expected_keywords']]
                matched_keywords = sum(1 for kw in expected_keywords if kw in answer)
                return matched_keywords / len(expected_keywords)
            
            # ì‘ì„±ì ë§¤ì¹­
            elif 'expected_author' in query:
                expected_author = query['expected_author'].lower()
                return 1.0 if expected_author in answer else 0.0
            
            # GUID ë§¤ì¹­
            elif 'expected_guid' in query:
                expected_guid = query['expected_guid']
                return 1.0 if expected_guid in answer else 0.0
            
            # ê¸°ë³¸ ê´€ë ¨ì„± (ë‹¨ì–´ ê²¹ì¹¨)
            else:
                question_words = set(question.split())
                answer_words = set(answer.split())
                if question_words:
                    overlap = len(question_words.intersection(answer_words))
                    return overlap / len(question_words)
                return 0.0
                
        except Exception:
            return 0.0
    
    def run_comprehensive_evaluation(self) -> Dict[str, Any]:
        """ì¢…í•© í‰ê°€ ì‹¤í–‰"""
        print("\\nğŸš€ ì¢…í•© í‰ê°€ ì‹œì‘...")
        
        # í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ìƒì„±
        test_queries = self.create_comprehensive_test_queries()
        print(f"ğŸ“ ìƒì„±ëœ í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬: {len(test_queries)}ê°œ")
        
        # ê° ì—”ì§„ë³„ í‰ê°€
        all_results = []
        
        for engine_name in self.engines.keys():
            print(f"\\nğŸ” {engine_name} ì—”ì§„ í‰ê°€ ì¤‘...")
            engine_results = []
            
            for query in test_queries:
                result = self.evaluate_engine(engine_name, query)
                engine_results.append(result)
                all_results.append(result)
                
                # ì§„í–‰ ìƒí™© ì¶œë ¥
                if len(engine_results) % 5 == 0:
                    print(f"   ì§„í–‰ë¥ : {len(engine_results)}/{len(test_queries)}")
            
            self.evaluation_results[engine_name] = engine_results
            print(f"âœ… {engine_name} ì—”ì§„ í‰ê°€ ì™„ë£Œ")
        
        # ì¢…í•© ë¶„ì„
        comprehensive_analysis = self._analyze_results(all_results)
        
        return {
            'evaluation_results': self.evaluation_results,
            'comprehensive_analysis': comprehensive_analysis,
            'test_queries': test_queries,
            'timestamp': datetime.now(timezone.utc).isoformat()
        }
    
    def _analyze_results(self, all_results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ê²°ê³¼ ì¢…í•© ë¶„ì„"""
        analysis = {
            'overall_metrics': {},
            'engine_comparison': {},
            'query_type_analysis': {},
            'category_analysis': {},
            'performance_ranking': {}
        }
        
        # ì „ì²´ ë©”íŠ¸ë¦­ ê³„ì‚°
        total_queries = len(all_results)
        successful_queries = sum(1 for r in all_results if r['success'])
        
        analysis['overall_metrics'] = {
            'total_queries': total_queries,
            'successful_queries': successful_queries,
            'overall_success_rate': successful_queries / total_queries if total_queries > 0 else 0,
            'average_confidence': np.mean([r['confidence'] for r in all_results]),
            'average_response_time': np.mean([r['response_time'] for r in all_results]),
            'average_relevance_score': np.mean([r['relevance_score'] for r in all_results])
        }
        
        # ì—”ì§„ë³„ ë¹„êµ
        for engine_name in self.engines.keys():
            engine_results = [r for r in all_results if r['engine'] == engine_name]
            
            if engine_results:
                analysis['engine_comparison'][engine_name] = {
                    'total_queries': len(engine_results),
                    'successful_queries': sum(1 for r in engine_results if r['success']),
                    'success_rate': sum(1 for r in engine_results if r['success']) / len(engine_results),
                    'average_confidence': np.mean([r['confidence'] for r in engine_results]),
                    'average_response_time': np.mean([r['response_time'] for r in engine_results]),
                    'average_relevance_score': np.mean([r['relevance_score'] for r in engine_results]),
                    'average_result_count': np.mean([r['result_count'] for r in engine_results])
                }
        
        # ì¿¼ë¦¬ íƒ€ì…ë³„ ë¶„ì„
        query_types = set(r['query_type'] for r in all_results)
        for query_type in query_types:
            type_results = [r for r in all_results if r['query_type'] == query_type]
            
            analysis['query_type_analysis'][query_type] = {
                'total_queries': len(type_results),
                'success_rate': sum(1 for r in type_results if r['success']) / len(type_results),
                'average_confidence': np.mean([r['confidence'] for r in type_results]),
                'average_response_time': np.mean([r['response_time'] for r in type_results])
            }
        
        # ì¹´í…Œê³ ë¦¬ë³„ ë¶„ì„
        categories = set(r['category'] for r in all_results)
        for category in categories:
            category_results = [r for r in all_results if r['category'] == category]
            
            analysis['category_analysis'][category] = {
                'total_queries': len(category_results),
                'success_rate': sum(1 for r in category_results if r['success']) / len(category_results),
                'average_confidence': np.mean([r['confidence'] for r in category_results]),
                'average_response_time': np.mean([r['response_time'] for r in category_results])
            }
        
        # ì„±ëŠ¥ ìˆœìœ„
        engine_scores = {}
        for engine_name, metrics in analysis['engine_comparison'].items():
            # ë³µí•© ì ìˆ˜ ê³„ì‚° (ì‹ ë¢°ë„ ì¤‘ì‹¬: ì„±ê³µë¥  30%, ì‹ ë¢°ë„ 50%, ê´€ë ¨ì„± 15%, ì‘ë‹µì‹œê°„ 5%)
            score = (
                metrics['success_rate'] * 0.3 +
                metrics['average_confidence'] * 0.5 +
                metrics['average_relevance_score'] * 0.15 +
                max(0, 1 - metrics['average_response_time'] / 5.0) * 0.05
            )
            engine_scores[engine_name] = score
        
        analysis['performance_ranking'] = dict(sorted(engine_scores.items(), key=lambda x: x[1], reverse=True))
        
        return analysis
    
    def save_results(self, results: Dict[str, Any], output_path: str):
        """ê²°ê³¼ ì €ì¥"""
        # JSON í˜•íƒœë¡œ ì €ì¥
        json_path = output_path.replace('.json', '_comprehensive.json')
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2, default=str)
        
        # CSV í˜•íƒœë¡œë„ ì €ì¥
        csv_path = output_path.replace('.json', '_detailed.csv')
        all_results = []
        for engine_name, engine_results in results['evaluation_results'].items():
            all_results.extend(engine_results)
        
        df = pd.DataFrame(all_results)
        df.to_csv(csv_path, index=False, encoding='utf-8')
        
        print(f"\\nğŸ’¾ ê²°ê³¼ ì €ì¥ ì™„ë£Œ:")
        print(f"   ğŸ“„ JSON: {json_path}")
        print(f"   ğŸ“Š CSV: {csv_path}")
    
    def generate_report(self, results: Dict[str, Any]) -> str:
        """í‰ê°€ ë³´ê³ ì„œ ìƒì„±"""
        analysis = results['comprehensive_analysis']
        
        report = f"""
# ContextualForget ì¢…í•© í‰ê°€ ë³´ê³ ì„œ

## ğŸ“Š ì „ì²´ ì„±ëŠ¥ ìš”ì•½

- **ì´ ì¿¼ë¦¬ ìˆ˜**: {analysis['overall_metrics']['total_queries']}
- **ì„±ê³µë¥ **: {analysis['overall_metrics']['overall_success_rate']:.2%}
- **í‰ê·  ì‹ ë¢°ë„**: {analysis['overall_metrics']['average_confidence']:.3f}
- **í‰ê·  ì‘ë‹µì‹œê°„**: {analysis['overall_metrics']['average_response_time']:.3f}ì´ˆ
- **í‰ê·  ê´€ë ¨ì„±**: {analysis['overall_metrics']['average_relevance_score']:.3f}

## ğŸ† ì—”ì§„ë³„ ì„±ëŠ¥ ë¹„êµ

"""
        
        for engine_name, metrics in analysis['engine_comparison'].items():
            report += f"""
### {engine_name}
- **ì„±ê³µë¥ **: {metrics['success_rate']:.2%}
- **í‰ê·  ì‹ ë¢°ë„**: {metrics['average_confidence']:.3f}
- **í‰ê·  ì‘ë‹µì‹œê°„**: {metrics['average_response_time']:.3f}ì´ˆ
- **í‰ê·  ê´€ë ¨ì„±**: {metrics['average_relevance_score']:.3f}
- **í‰ê·  ê²°ê³¼ ìˆ˜**: {metrics['average_result_count']:.1f}

"""
        
        # ì„±ëŠ¥ ìˆœìœ„
        report += "## ğŸ¥‡ ì„±ëŠ¥ ìˆœìœ„\n\n"
        for i, (engine_name, score) in enumerate(analysis['performance_ranking'].items(), 1):
            report += f"{i}. **{engine_name}**: {score:.3f}\n"
        
        # ì¿¼ë¦¬ íƒ€ì…ë³„ ë¶„ì„
        report += "\\n## ğŸ“ ì¿¼ë¦¬ íƒ€ì…ë³„ ì„±ëŠ¥\n\n"
        for query_type, metrics in analysis['query_type_analysis'].items():
            report += f"### {query_type}\n"
            report += f"- **ì„±ê³µë¥ **: {metrics['success_rate']:.2%}\n"
            report += f"- **í‰ê·  ì‹ ë¢°ë„**: {metrics['average_confidence']:.3f}\n"
            report += f"- **í‰ê·  ì‘ë‹µì‹œê°„**: {metrics['average_response_time']:.3f}ì´ˆ\n\n"
        
        # ì¹´í…Œê³ ë¦¬ë³„ ë¶„ì„
        report += "## ğŸ·ï¸ ì¹´í…Œê³ ë¦¬ë³„ ì„±ëŠ¥\n\n"
        for category, metrics in analysis['category_analysis'].items():
            report += f"### {category}\n"
            report += f"- **ì„±ê³µë¥ **: {metrics['success_rate']:.2%}\n"
            report += f"- **í‰ê·  ì‹ ë¢°ë„**: {metrics['average_confidence']:.3f}\n"
            report += f"- **í‰ê·  ì‘ë‹µì‹œê°„**: {metrics['average_response_time']:.3f}ì´ˆ\n\n"
        
        report += f"\\n---\\n*í‰ê°€ ì™„ë£Œ ì‹œê°„: {results['timestamp']}*"
        
        return report


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ ContextualForget ì¢…í•© í‰ê°€ ì‹œì‘")
    
    # í‰ê°€ì ì´ˆê¸°í™”
    evaluator = ComprehensiveRAGEvaluator()
    
    # ê·¸ë˜í”„ ë¡œë“œ
    evaluator.load_graph()
    
    # ì—”ì§„ ì´ˆê¸°í™”
    evaluator.initialize_engines()
    
    # ì¢…í•© í‰ê°€ ì‹¤í–‰
    results = evaluator.run_comprehensive_evaluation()
    
    # ê²°ê³¼ ì €ì¥
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_path = f"evaluation_comprehensive_{timestamp}.json"
    evaluator.save_results(results, output_path)
    
    # ë³´ê³ ì„œ ìƒì„±
    report = evaluator.generate_report(results)
    report_path = f"evaluation_report_{timestamp}.md"
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write(report)
    
    print(f"\\nğŸ“‹ í‰ê°€ ë³´ê³ ì„œ: {report_path}")
    print("\\nâœ… ì¢…í•© í‰ê°€ ì™„ë£Œ!")
    
    # ê°„ë‹¨í•œ ìš”ì•½ ì¶œë ¥
    analysis = results['comprehensive_analysis']
    print("\\nğŸ“Š ì„±ëŠ¥ ìˆœìœ„:")
    for i, (engine_name, score) in enumerate(analysis['performance_ranking'].items(), 1):
        print(f"   {i}. {engine_name}: {score:.3f}")


if __name__ == "__main__":
    main()

"""
Comprehensive Evaluation of RAG Systems
Ï¢ÖÌï© ÌèâÍ∞Ä Î∞è ÏÑ±Îä• ÎπÑÍµê
"""

import json
import pickle
import sys
import time
from datetime import datetime, timezone
from pathlib import Path
from typing import Any, Dict, List, Optional
import numpy as np
import pandas as pd

# ÌîÑÎ°úÏ†ùÌä∏ Í≤ΩÎ°ú Ï∂îÍ∞Ä
sys.path.insert(0, str(Path(__file__).parent.parent / 'src'))

from contextualforget.query.contextual_forget_engine import ContextualForgetEngine
from contextualforget.query.adaptive_retrieval import HybridRetrievalEngine
from contextualforget.baselines.bm25_engine import BM25QueryEngine
from contextualforget.baselines.vector_engine import VectorQueryEngine


class ComprehensiveRAGEvaluator:
    """Ï¢ÖÌï© RAG ÏãúÏä§ÌÖú ÌèâÍ∞ÄÏûê"""
    
    def __init__(self, graph_path: str = 'data/processed/graph.gpickle'):
        self.graph_path = graph_path
        self.graph = None
        self.engines = {}
        self.evaluation_results = {}
        
        # ÌèâÍ∞Ä Î©îÌä∏Î¶≠
        self.metrics = {
            'response_time': [],
            'confidence': [],
            'result_count': [],
            'success_rate': [],
            'relevance_score': []
        }
        
    def load_graph(self):
        """Í∑∏ÎûòÌîÑ Î°úÎìú"""
        print("üìä Í∑∏ÎûòÌîÑ Î°úÎìú Ï§ë...")
        with open(self.graph_path, 'rb') as f:
            self.graph = pickle.load(f)
        print(f"‚úÖ Í∑∏ÎûòÌîÑ Î°úÎìú ÏôÑÎ£å: {self.graph.number_of_nodes()}Í∞ú ÎÖ∏Îìú, {self.graph.number_of_edges()}Í∞ú Ïó£ÏßÄ")
    
    def initialize_engines(self):
        """Î™®Îì† ÏóîÏßÑ Ï¥àÍ∏∞Ìôî"""
        print("\\nüîß ÏóîÏßÑÎì§ Ï¥àÍ∏∞Ìôî Ï§ë...")
        
        # Îç∞Ïù¥ÌÑ∞ Ï∂îÏ∂ú
        bcf_data = []
        ifc_data = []
        for node, data in self.graph.nodes(data=True):
            if node[0] == 'BCF':
                bcf_data.append(data)
            elif node[0] == 'IFC':
                ifc_data.append(data)
        
        print(f"   üìã BCF Îç∞Ïù¥ÌÑ∞: {len(bcf_data)}Í∞ú")
        print(f"   üìã IFC Îç∞Ïù¥ÌÑ∞: {len(ifc_data)}Í∞ú")
        
        # BM25 ÏóîÏßÑ
        print("   üîç BM25 ÏóîÏßÑ Ï¥àÍ∏∞Ìôî...")
        bm25_engine = BM25QueryEngine()
        bm25_engine.initialize({'bcf_data': bcf_data})
        self.engines['BM25'] = bm25_engine
        
        # Vector ÏóîÏßÑ
        print("   üîç Vector ÏóîÏßÑ Ï¥àÍ∏∞Ìôî...")
        vector_engine = VectorQueryEngine()
        vector_engine.initialize({
            'bcf_data': bcf_data,
            'ifc_data': ifc_data,
            'graph': self.graph
        })
        self.engines['Vector'] = vector_engine
        
        # ContextualForget ÏóîÏßÑ
        print("   üîç ContextualForget ÏóîÏßÑ Ï¥àÍ∏∞Ìôî...")
        contextual_engine = ContextualForgetEngine(
            self.graph, 
            enable_contextual_forgetting=True
        )
        self.engines['ContextualForget'] = contextual_engine
        
        # ÌïòÏù¥Î∏åÎ¶¨Îìú ÏóîÏßÑ
        print("   üîç ÌïòÏù¥Î∏åÎ¶¨Îìú ÏóîÏßÑ Ï¥àÍ∏∞Ìôî...")
        hybrid_engine = HybridRetrievalEngine(
            base_engines=self.engines.copy(),
            fusion_strategy='adaptive',
            enable_adaptation=True
        )
        self.engines['Hybrid'] = hybrid_engine
        
        print("‚úÖ Î™®Îì† ÏóîÏßÑ Ï¥àÍ∏∞Ìôî ÏôÑÎ£å")
    
    def create_comprehensive_test_queries(self) -> List[Dict[str, Any]]:
        """Ï¢ÖÌï© ÌÖåÏä§Ìä∏ ÏøºÎ¶¨ ÏÉùÏÑ±"""
        # Ïã§Ï†ú Í∑∏ÎûòÌîÑÏóêÏÑú Í≤ÄÏ¶ùÎêú ÌÇ§ÏõåÎìúÎßå Ï∂îÏ∂ú
        verified_keywords = []
        for node, data in self.graph.nodes(data=True):
            if node[0] == 'BCF':
                title = data.get('title', '')
                if title and len(title) > 2:
                    # Í∞Å ÌÇ§ÏõåÎìúÎ°ú Ïã§Ï†ú Í≤ÄÏÉâ ÌÖåÏä§Ìä∏
                    keywords = [w for w in title.split() if len(w) > 1]
                    for kw in keywords:
                        # Í∑∏ÎûòÌîÑÏóêÏÑú Ïù¥ ÌÇ§ÏõåÎìúÎ°ú Ï∞æÏùÑ Ïàò ÏûàÎäîÏßÄ ÌôïÏù∏
                        matches = sum(1 for _, d in self.graph.nodes(data=True) 
                                     if d.get('title', '').lower().count(kw.lower()) > 0)
                        if matches > 0:
                            verified_keywords.append((kw, matches))
        
        # Îß§Ïπ≠ Í∞úÏàòÍ∞Ä ÎßéÏùÄ ÏàúÏúºÎ°ú Ï†ïÎ†¨
        verified_keywords.sort(key=lambda x: x[1], reverse=True)
        
        # ÏÉÅÏúÑ ÌÇ§ÏõåÎìúÎ°ú ÌÖåÏä§Ìä∏ ÏøºÎ¶¨ ÏÉùÏÑ±
        test_queries = []
        for keyword, expected_count in verified_keywords[:10]:
            test_queries.append({
                'id': f'kw_{len(test_queries)+1}',
                'question': f'{keyword} Í¥ÄÎ†® Ïù¥ÏäàÍ∞Ä ÏûàÎÇòÏöî?',
                'type': 'keyword',
                'expected_keywords': [keyword],
                'expected_min_count': min(expected_count, 5),
                'category': 'keyword_search'
            })
        
        # ÏûëÏÑ±Ïûê Í∏∞Î∞ò ÏøºÎ¶¨ (Ïã§Ï†ú Ï°¥Ïû¨ÌïòÎäî ÏûëÏÑ±ÏûêÎßå)
        authors = set()
        for node, data in self.graph.nodes(data=True):
            if node[0] == 'BCF':
                author = data.get('author', '')
                if author:
                    authors.add(author)
        
        author_list = list(authors)[:5]
        for author in author_list:
            test_queries.append({
                'id': f'auth_{len(test_queries)+1}',
                'question': f'{author}Í∞Ä ÏûëÏÑ±Ìïú Ïù¥ÏäàÎäî?',
                'type': 'author',
                'expected_author': author,
                'category': 'author_search'
            })
        
        # GUID Í∏∞Î∞ò ÏøºÎ¶¨ (Ïã§Ï†ú Ï°¥Ïû¨ÌïòÎäî GUIDÎßå)
        guids = set()
        for node, data in self.graph.nodes(data=True):
            if node[0] == 'BCF':
                topic_id = data.get('topic_id', '')
                if topic_id:
                    guids.add(topic_id)
        
        guid_list = list(guids)[:5]
        for guid in guid_list:
            test_queries.append({
                'id': f'guid_{len(test_queries)+1}',
                'question': f'{guid}ÏôÄ Í¥ÄÎ†®Îêú Î¨∏Ï†úÎäî?',
                'type': 'guid',
                'expected_guid': guid,
                'category': 'guid_search'
            })
        
        # Í≤ÄÏ¶ùÎêú Î≥µÌï© ÏøºÎ¶¨
        complex_queries = [
            {
                'id': f'comp_{len(test_queries)+1}',
                'question': 'Î¨¥Í∑†Ïã§ ÎßàÍ∞ê Í¥ÄÎ†® Î¨∏Ï†úÏ†êÏùÄ?',
                'type': 'complex',
                'expected_keywords': ['Î¨¥Í∑†Ïã§', 'ÎßàÍ∞ê'],
                'category': 'complex_search'
            },
            {
                'id': f'comp_{len(test_queries)+1}',
                'question': 'ÏµúÍ∑º Î∞úÏÉùÌïú Î≤ΩÏ≤¥ Í¥ÄÎ†® Ïù¥ÏäàÎäî?',
                'type': 'temporal',
                'expected_keywords': ['Î≤ΩÏ≤¥'],
                'category': 'temporal_search'
            }
        ]
        test_queries.extend(complex_queries)
        
        return test_queries
    
    def evaluate_engine(self, engine_name: str, query: Dict[str, Any]) -> Dict[str, Any]:
        """Îã®Ïùº ÏóîÏßÑ ÌèâÍ∞Ä"""
        start_time = time.perf_counter()
        
        try:
            engine = self.engines[engine_name]
            question = query['question']
            
            # ÏóîÏßÑÎ≥Ñ ÏøºÎ¶¨ Ïã§Ìñâ
            if engine_name == 'BM25':
                result = engine.process_query(question)
            elif engine_name == 'Vector':
                result = engine.process_query(question)
            elif engine_name == 'ContextualForget':
                result = engine.contextual_query(question, query_type=query['type'])
            elif engine_name == 'Hybrid':
                result = engine.query(question)
                # ÌïòÏù¥Î∏åÎ¶¨Îìú Í≤∞Í≥ºÏóêÏÑú Ïã§Ï†ú Í≤∞Í≥º Ï∂îÏ∂ú
                if 'result' in result:
                    result = result['result']
            else:
                result = {'error': 'Unknown engine'}
            
            response_time = max(time.perf_counter() - start_time, 0.0001)
            
            # Í≤∞Í≥º Î∂ÑÏÑù
            if isinstance(result, dict) and 'error' not in result:
                confidence = result.get('confidence', 0.0)
                result_count = result.get('result_count', 0)
                # result_countÍ∞Ä 0Ïù¥Í≥† detailsÏóê Ï†ïÎ≥¥Í∞Ä ÏûàÏúºÎ©¥ Ï∂îÏ∂ú
                if result_count == 0 and 'details' in result:
                    details = result['details']
                    result_count = details.get('count', details.get('total_results', 0))
                # ÏÑ±Í≥µ Í∏∞Ï§Ä: Í≤∞Í≥ºÍ∞Ä ÏûàÍ≥† Ïã†Î¢∞ÎèÑÍ∞Ä ÏùºÏ†ï ÏàòÏ§Ä Ïù¥ÏÉÅ
                success = result_count > 0 and confidence > 0.1
                
                # Í¥ÄÎ†®ÏÑ± Ï†êÏàò Í≥ÑÏÇ∞
                relevance_score = self._calculate_relevance_score(query, result)
                
                return {
                    'engine': engine_name,
                    'query_id': query['id'],
                    'question': question,
                    'query_type': query['type'],
                    'category': query['category'],
                    'answer': result.get('answer', 'No answer'),
                    'confidence': confidence,
                    'response_time': response_time,
                    'result_count': result_count,
                    'success': success,
                    'relevance_score': relevance_score,
                    'error': None
                }
            else:
                return {
                    'engine': engine_name,
                    'query_id': query['id'],
                    'question': question,
                    'query_type': query['type'],
                    'category': query['category'],
                    'answer': 'Error occurred',
                    'confidence': 0.0,
                    'response_time': response_time,
                    'result_count': 0,
                    'success': False,
                    'relevance_score': 0.0,
                    'error': result.get('error', 'Unknown error')
                }
                
        except Exception as e:
            response_time = max(time.perf_counter() - start_time, 0.0001)
            return {
                'engine': engine_name,
                'query_id': query['id'],
                'question': question,
                'query_type': query['type'],
                'category': query['category'],
                'answer': 'Exception occurred',
                'confidence': 0.0,
                'response_time': response_time,
                'result_count': 0,
                'success': False,
                'relevance_score': 0.0,
                'error': str(e)
            }
    
    def _calculate_relevance_score(self, query: Dict[str, Any], result: Dict[str, Any]) -> float:
        """Í¥ÄÎ†®ÏÑ± Ï†êÏàò Í≥ÑÏÇ∞"""
        try:
            answer = result.get('answer', '').lower()
            question = query['question'].lower()
            
            # ÌÇ§ÏõåÎìú Îß§Ïπ≠
            if 'expected_keywords' in query:
                expected_keywords = [kw.lower() for kw in query['expected_keywords']]
                matched_keywords = sum(1 for kw in expected_keywords if kw in answer)
                return matched_keywords / len(expected_keywords)
            
            # ÏûëÏÑ±Ïûê Îß§Ïπ≠
            elif 'expected_author' in query:
                expected_author = query['expected_author'].lower()
                return 1.0 if expected_author in answer else 0.0
            
            # GUID Îß§Ïπ≠
            elif 'expected_guid' in query:
                expected_guid = query['expected_guid']
                return 1.0 if expected_guid in answer else 0.0
            
            # Í∏∞Î≥∏ Í¥ÄÎ†®ÏÑ± (Îã®Ïñ¥ Í≤πÏπ®)
            else:
                question_words = set(question.split())
                answer_words = set(answer.split())
                if question_words:
                    overlap = len(question_words.intersection(answer_words))
                    return overlap / len(question_words)
                return 0.0
                
        except Exception:
            return 0.0
    
    def run_comprehensive_evaluation(self) -> Dict[str, Any]:
        """Ï¢ÖÌï© ÌèâÍ∞Ä Ïã§Ìñâ"""
        print("\\nüöÄ Ï¢ÖÌï© ÌèâÍ∞Ä ÏãúÏûë...")
        
        # ÌÖåÏä§Ìä∏ ÏøºÎ¶¨ ÏÉùÏÑ±
        test_queries = self.create_comprehensive_test_queries()
        print(f"üìù ÏÉùÏÑ±Îêú ÌÖåÏä§Ìä∏ ÏøºÎ¶¨: {len(test_queries)}Í∞ú")
        
        # Í∞Å ÏóîÏßÑÎ≥Ñ ÌèâÍ∞Ä
        all_results = []
        
        for engine_name in self.engines.keys():
            print(f"\\nüîç {engine_name} ÏóîÏßÑ ÌèâÍ∞Ä Ï§ë...")
            engine_results = []
            
            for query in test_queries:
                result = self.evaluate_engine(engine_name, query)
                engine_results.append(result)
                all_results.append(result)
                
                # ÏßÑÌñâ ÏÉÅÌô© Ï∂úÎ†•
                if len(engine_results) % 5 == 0:
                    print(f"   ÏßÑÌñâÎ•†: {len(engine_results)}/{len(test_queries)}")
            
            self.evaluation_results[engine_name] = engine_results
            print(f"‚úÖ {engine_name} ÏóîÏßÑ ÌèâÍ∞Ä ÏôÑÎ£å")
        
        # Ï¢ÖÌï© Î∂ÑÏÑù
        comprehensive_analysis = self._analyze_results(all_results)
        
        return {
            'evaluation_results': self.evaluation_results,
            'comprehensive_analysis': comprehensive_analysis,
            'test_queries': test_queries,
            'timestamp': datetime.now(timezone.utc).isoformat()
        }
    
    def _analyze_results(self, all_results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Í≤∞Í≥º Ï¢ÖÌï© Î∂ÑÏÑù"""
        analysis = {
            'overall_metrics': {},
            'engine_comparison': {},
            'query_type_analysis': {},
            'category_analysis': {},
            'performance_ranking': {}
        }
        
        # Ï†ÑÏ≤¥ Î©îÌä∏Î¶≠ Í≥ÑÏÇ∞
        total_queries = len(all_results)
        successful_queries = sum(1 for r in all_results if r['success'])
        
        analysis['overall_metrics'] = {
            'total_queries': total_queries,
            'successful_queries': successful_queries,
            'overall_success_rate': successful_queries / total_queries if total_queries > 0 else 0,
            'average_confidence': np.mean([r['confidence'] for r in all_results]),
            'average_response_time': np.mean([r['response_time'] for r in all_results]),
            'average_relevance_score': np.mean([r['relevance_score'] for r in all_results])
        }
        
        # ÏóîÏßÑÎ≥Ñ ÎπÑÍµê
        for engine_name in self.engines.keys():
            engine_results = [r for r in all_results if r['engine'] == engine_name]
            
            if engine_results:
                analysis['engine_comparison'][engine_name] = {
                    'total_queries': len(engine_results),
                    'successful_queries': sum(1 for r in engine_results if r['success']),
                    'success_rate': sum(1 for r in engine_results if r['success']) / len(engine_results),
                    'average_confidence': np.mean([r['confidence'] for r in engine_results]),
                    'average_response_time': np.mean([r['response_time'] for r in engine_results]),
                    'average_relevance_score': np.mean([r['relevance_score'] for r in engine_results]),
                    'average_result_count': np.mean([r['result_count'] for r in engine_results])
                }
        
        # ÏøºÎ¶¨ ÌÉÄÏûÖÎ≥Ñ Î∂ÑÏÑù
        query_types = set(r['query_type'] for r in all_results)
        for query_type in query_types:
            type_results = [r for r in all_results if r['query_type'] == query_type]
            
            analysis['query_type_analysis'][query_type] = {
                'total_queries': len(type_results),
                'success_rate': sum(1 for r in type_results if r['success']) / len(type_results),
                'average_confidence': np.mean([r['confidence'] for r in type_results]),
                'average_response_time': np.mean([r['response_time'] for r in type_results])
            }
        
        # Ïπ¥ÌÖåÍ≥†Î¶¨Î≥Ñ Î∂ÑÏÑù
        categories = set(r['category'] for r in all_results)
        for category in categories:
            category_results = [r for r in all_results if r['category'] == category]
            
            analysis['category_analysis'][category] = {
                'total_queries': len(category_results),
                'success_rate': sum(1 for r in category_results if r['success']) / len(category_results),
                'average_confidence': np.mean([r['confidence'] for r in category_results]),
                'average_response_time': np.mean([r['response_time'] for r in category_results])
            }
        
        # ÏÑ±Îä• ÏàúÏúÑ
        engine_scores = {}
        for engine_name, metrics in analysis['engine_comparison'].items():
            # Î≥µÌï© Ï†êÏàò Í≥ÑÏÇ∞ (Ïã†Î¢∞ÎèÑ Ï§ëÏã¨: ÏÑ±Í≥µÎ•† 30%, Ïã†Î¢∞ÎèÑ 50%, Í¥ÄÎ†®ÏÑ± 15%, ÏùëÎãµÏãúÍ∞Ñ 5%)
            score = (
                metrics['success_rate'] * 0.3 +
                metrics['average_confidence'] * 0.5 +
                metrics['average_relevance_score'] * 0.15 +
                max(0, 1 - metrics['average_response_time'] / 5.0) * 0.05
            )
            engine_scores[engine_name] = score
        
        analysis['performance_ranking'] = dict(sorted(engine_scores.items(), key=lambda x: x[1], reverse=True))
        
        return analysis
    
    def save_results(self, results: Dict[str, Any], output_path: str):
        """Í≤∞Í≥º Ï†ÄÏû•"""
        # JSON ÌòïÌÉúÎ°ú Ï†ÄÏû•
        json_path = output_path.replace('.json', '_comprehensive.json')
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2, default=str)
        
        # CSV ÌòïÌÉúÎ°úÎèÑ Ï†ÄÏû•
        csv_path = output_path.replace('.json', '_detailed.csv')
        all_results = []
        for engine_name, engine_results in results['evaluation_results'].items():
            all_results.extend(engine_results)
        
        df = pd.DataFrame(all_results)
        df.to_csv(csv_path, index=False, encoding='utf-8')
        
        print(f"\\nüíæ Í≤∞Í≥º Ï†ÄÏû• ÏôÑÎ£å:")
        print(f"   üìÑ JSON: {json_path}")
        print(f"   üìä CSV: {csv_path}")
    
    def generate_report(self, results: Dict[str, Any]) -> str:
        """ÌèâÍ∞Ä Î≥¥Í≥†ÏÑú ÏÉùÏÑ±"""
        analysis = results['comprehensive_analysis']
        
        report = f"""
# ContextualForget Ï¢ÖÌï© ÌèâÍ∞Ä Î≥¥Í≥†ÏÑú

## üìä Ï†ÑÏ≤¥ ÏÑ±Îä• ÏöîÏïΩ

- **Ï¥ù ÏøºÎ¶¨ Ïàò**: {analysis['overall_metrics']['total_queries']}
- **ÏÑ±Í≥µÎ•†**: {analysis['overall_metrics']['overall_success_rate']:.2%}
- **ÌèâÍ∑† Ïã†Î¢∞ÎèÑ**: {analysis['overall_metrics']['average_confidence']:.3f}
- **ÌèâÍ∑† ÏùëÎãµÏãúÍ∞Ñ**: {analysis['overall_metrics']['average_response_time']:.3f}Ï¥à
- **ÌèâÍ∑† Í¥ÄÎ†®ÏÑ±**: {analysis['overall_metrics']['average_relevance_score']:.3f}

## üèÜ ÏóîÏßÑÎ≥Ñ ÏÑ±Îä• ÎπÑÍµê

"""
        
        for engine_name, metrics in analysis['engine_comparison'].items():
            report += f"""
### {engine_name}
- **ÏÑ±Í≥µÎ•†**: {metrics['success_rate']:.2%}
- **ÌèâÍ∑† Ïã†Î¢∞ÎèÑ**: {metrics['average_confidence']:.3f}
- **ÌèâÍ∑† ÏùëÎãµÏãúÍ∞Ñ**: {metrics['average_response_time']:.3f}Ï¥à
- **ÌèâÍ∑† Í¥ÄÎ†®ÏÑ±**: {metrics['average_relevance_score']:.3f}
- **ÌèâÍ∑† Í≤∞Í≥º Ïàò**: {metrics['average_result_count']:.1f}

"""
        
        # ÏÑ±Îä• ÏàúÏúÑ
        report += "## ü•á ÏÑ±Îä• ÏàúÏúÑ\n\n"
        for i, (engine_name, score) in enumerate(analysis['performance_ranking'].items(), 1):
            report += f"{i}. **{engine_name}**: {score:.3f}\n"
        
        # ÏøºÎ¶¨ ÌÉÄÏûÖÎ≥Ñ Î∂ÑÏÑù
        report += "\\n## üìù ÏøºÎ¶¨ ÌÉÄÏûÖÎ≥Ñ ÏÑ±Îä•\n\n"
        for query_type, metrics in analysis['query_type_analysis'].items():
            report += f"### {query_type}\n"
            report += f"- **ÏÑ±Í≥µÎ•†**: {metrics['success_rate']:.2%}\n"
            report += f"- **ÌèâÍ∑† Ïã†Î¢∞ÎèÑ**: {metrics['average_confidence']:.3f}\n"
            report += f"- **ÌèâÍ∑† ÏùëÎãµÏãúÍ∞Ñ**: {metrics['average_response_time']:.3f}Ï¥à\n\n"
        
        # Ïπ¥ÌÖåÍ≥†Î¶¨Î≥Ñ Î∂ÑÏÑù
        report += "## üè∑Ô∏è Ïπ¥ÌÖåÍ≥†Î¶¨Î≥Ñ ÏÑ±Îä•\n\n"
        for category, metrics in analysis['category_analysis'].items():
            report += f"### {category}\n"
            report += f"- **ÏÑ±Í≥µÎ•†**: {metrics['success_rate']:.2%}\n"
            report += f"- **ÌèâÍ∑† Ïã†Î¢∞ÎèÑ**: {metrics['average_confidence']:.3f}\n"
            report += f"- **ÌèâÍ∑† ÏùëÎãµÏãúÍ∞Ñ**: {metrics['average_response_time']:.3f}Ï¥à\n\n"
        
        report += f"\\n---\\n*ÌèâÍ∞Ä ÏôÑÎ£å ÏãúÍ∞Ñ: {results['timestamp']}*"
        
        return report


def main():
    """Î©îÏù∏ Ïã§Ìñâ Ìï®Ïàò"""
    print("üöÄ ContextualForget Ï¢ÖÌï© ÌèâÍ∞Ä ÏãúÏûë")
    
    # ÌèâÍ∞ÄÏûê Ï¥àÍ∏∞Ìôî
    evaluator = ComprehensiveRAGEvaluator()
    
    # Í∑∏ÎûòÌîÑ Î°úÎìú
    evaluator.load_graph()
    
    # ÏóîÏßÑ Ï¥àÍ∏∞Ìôî
    evaluator.initialize_engines()
    
    # Ï¢ÖÌï© ÌèâÍ∞Ä Ïã§Ìñâ
    results = evaluator.run_comprehensive_evaluation()
    
    # Í≤∞Í≥º Ï†ÄÏû•
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_path = f"evaluation_comprehensive_{timestamp}.json"
    evaluator.save_results(results, output_path)
    
    # Î≥¥Í≥†ÏÑú ÏÉùÏÑ±
    report = evaluator.generate_report(results)
    report_path = f"evaluation_report_{timestamp}.md"
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write(report)
    
    print(f"\\nüìã ÌèâÍ∞Ä Î≥¥Í≥†ÏÑú: {report_path}")
    print("\\n‚úÖ Ï¢ÖÌï© ÌèâÍ∞Ä ÏôÑÎ£å!")
    
    # Í∞ÑÎã®Ìïú ÏöîÏïΩ Ï∂úÎ†•
    analysis = results['comprehensive_analysis']
    print("\\nüìä ÏÑ±Îä• ÏàúÏúÑ:")
    for i, (engine_name, score) in enumerate(analysis['performance_ranking'].items(), 1):
        print(f"   {i}. {engine_name}: {score:.3f}")


if __name__ == "__main__":
    main()

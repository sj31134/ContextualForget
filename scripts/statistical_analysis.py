"""
í†µê³„ì  ìœ ì˜ì„± ê²€ì •
ì—”ì§„ ê°„ ì„±ëŠ¥ ì°¨ì´ì˜ í†µê³„ì  ìœ ì˜ì„± ê²€ì¦
"""

import json
import sys
from pathlib import Path
from typing import Dict, List
import numpy as np
from scipy import stats
import matplotlib.pyplot as plt


class StatisticalAnalyzer:
    """í†µê³„ ë¶„ì„ê¸°"""
    
    def __init__(self, detailed_results_path: str):
        self.detailed_results_path = detailed_results_path
        self.results = []
        self.engine_metrics = {}
        
    def load_results(self):
        """í‰ê°€ ê²°ê³¼ ë¡œë“œ"""
        print("ğŸ“Š í‰ê°€ ê²°ê³¼ ë¡œë“œ ì¤‘...")
        with open(self.detailed_results_path, 'r') as f:
            self.results = json.load(f)
        print(f"âœ… {len(self.results)}ê°œ ê²°ê³¼ ë¡œë“œ ì™„ë£Œ")
        
    def extract_engine_metrics(self, metric_name: str = 'f1'):
        """ì—”ì§„ë³„ ë©”íŠ¸ë¦­ ì¶”ì¶œ"""
        self.engine_metrics = {}
        
        for result in self.results:
            engine = result['engine']
            metric_value = result['metrics'].get(metric_name, 0.0)
            
            if engine not in self.engine_metrics:
                self.engine_metrics[engine] = []
            
            self.engine_metrics[engine].append(metric_value)
        
        # NumPy ë°°ì—´ë¡œ ë³€í™˜
        for engine in self.engine_metrics:
            self.engine_metrics[engine] = np.array(self.engine_metrics[engine])
        
        print(f"\nì—”ì§„ë³„ {metric_name} ìƒ˜í”Œ ìˆ˜:")
        for engine, values in self.engine_metrics.items():
            print(f"  {engine}: {len(values)}ê°œ")
    
    def compute_paired_ttest(self, engine1: str, engine2: str, metric_name: str = 'f1'):
        """Paired t-test ìˆ˜í–‰"""
        values1 = self.engine_metrics[engine1]
        values2 = self.engine_metrics[engine2]
        
        # Paired t-test
        t_stat, p_value = stats.ttest_rel(values1, values2)
        
        # Cohen's d ê³„ì‚° (effect size)
        diff = values1 - values2
        cohens_d = np.mean(diff) / np.std(diff, ddof=1)
        
        # Effect size í•´ì„
        if abs(cohens_d) < 0.2:
            effect_size = "negligible"
        elif abs(cohens_d) < 0.5:
            effect_size = "small"
        elif abs(cohens_d) < 0.8:
            effect_size = "medium"
        else:
            effect_size = "large"
        
        return {
            'method1': engine1,
            'method2': engine2,
            'metric': metric_name,
            't_statistic': float(t_stat),
            'p_value': float(p_value),
            'cohens_d': float(cohens_d) if not np.isnan(cohens_d) else 0.0,
            'significant': bool(p_value < 0.05),
            'effect_size': effect_size,
            'mean1': float(np.mean(values1)),
            'mean2': float(np.mean(values2)),
            'std1': float(np.std(values1)),
            'std2': float(np.std(values2))
        }
    
    def compute_anova(self, metric_name: str = 'f1'):
        """ANOVA ìˆ˜í–‰ (4ê°œ ì—”ì§„ ì „ì²´ ë¹„êµ)"""
        engine_names = list(self.engine_metrics.keys())
        engine_values = [self.engine_metrics[name] for name in engine_names]
        
        # One-way ANOVA
        f_stat, p_value = stats.f_oneway(*engine_values)
        
        return {
            'metric': metric_name,
            'engines': engine_names,
            'f_statistic': float(f_stat),
            'p_value': float(p_value),
            'significant': bool(p_value < 0.05)
        }
    
    def bonferroni_correction(self, comparisons: List[Dict], alpha: float = 0.05):
        """Bonferroni ë‹¤ì¤‘ ë¹„êµ ë³´ì •"""
        n_comparisons = len(comparisons)
        corrected_alpha = alpha / n_comparisons
        
        for comp in comparisons:
            comp['bonferroni_corrected_alpha'] = corrected_alpha
            comp['significant_after_bonferroni'] = comp['p_value'] < corrected_alpha
        
        return comparisons
    
    def analyze_all_metrics(self):
        """ëª¨ë“  ë©”íŠ¸ë¦­ì— ëŒ€í•´ í†µê³„ ë¶„ì„"""
        metrics = ['f1', 'precision', 'recall', 'ndcg@10', 'mrr', 'confidence']
        
        all_results = {
            'comparisons': [],
            'anova': []
        }
        
        for metric in metrics:
            print(f"\nğŸ“Š {metric.upper()} ë¶„ì„ ì¤‘...")
            
            # ì—”ì§„ë³„ ë©”íŠ¸ë¦­ ì¶”ì¶œ
            self.extract_engine_metrics(metric)
            
            # ANOVA
            anova_result = self.compute_anova(metric)
            all_results['anova'].append(anova_result)
            print(f"  ANOVA: F={anova_result['f_statistic']:.3f}, p={anova_result['p_value']:.4f}, significant={anova_result['significant']}")
            
            # Pairwise t-tests
            engines = list(self.engine_metrics.keys())
            for i in range(len(engines)):
                for j in range(i+1, len(engines)):
                    comp = self.compute_paired_ttest(engines[i], engines[j], metric)
                    all_results['comparisons'].append(comp)
                    
                    if comp['significant']:
                        print(f"  âœ… {engines[i]} vs {engines[j]}: t={comp['t_statistic']:.3f}, p={comp['p_value']:.4f}, d={comp['cohens_d']:.3f} ({comp['effect_size']})")
        
        # Bonferroni correction
        all_results['comparisons'] = self.bonferroni_correction(all_results['comparisons'])
        
        return all_results
    
    def generate_visualization(self, output_path: str):
        """í†µê³„ ë¶„ì„ ì‹œê°í™”"""
        print(f"\nğŸ“Š ì‹œê°í™” ìƒì„± ì¤‘: {output_path}")
        
        # F1 ë©”íŠ¸ë¦­ ê¸°ì¤€ìœ¼ë¡œ ì‹œê°í™”
        self.extract_engine_metrics('f1')
        
        fig, axes = plt.subplots(1, 2, figsize=(14, 5))
        
        # Box plot
        ax1 = axes[0]
        data_for_plot = [self.engine_metrics[engine] for engine in self.engine_metrics.keys()]
        labels = list(self.engine_metrics.keys())
        
        bp = ax1.boxplot(data_for_plot, labels=labels, patch_artist=True)
        for patch, color in zip(bp['boxes'], ['lightblue', 'lightgreen', 'lightcoral', 'lightyellow']):
            patch.set_facecolor(color)
        
        ax1.set_ylabel('F1 Score')
        ax1.set_title('F1 Score Distribution by Engine')
        ax1.grid(True, alpha=0.3)
        
        # Violin plot
        ax2 = axes[1]
        positions = range(1, len(labels) + 1)
        parts = ax2.violinplot(data_for_plot, positions=positions, showmeans=True, showmedians=True)
        
        ax2.set_xticks(positions)
        ax2.set_xticklabels(labels)
        ax2.set_ylabel('F1 Score')
        ax2.set_title('F1 Score Violin Plot by Engine')
        ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        print(f"âœ… ì‹œê°í™” ì €ì¥: {output_path}")
    
    def save_results(self, output_path: str, results: Dict):
        """ê²°ê³¼ ì €ì¥"""
        output_file = Path(output_path)
        output_file.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_file, 'w') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… í†µê³„ ê²°ê³¼ ì €ì¥: {output_path}")


def main():
    import argparse
    
    parser = argparse.ArgumentParser(description='í†µê³„ì  ìœ ì˜ì„± ê²€ì •')
    parser.add_argument('--detailed-results', default='results/evaluation_v3_final/evaluation_v3_detailed.json', help='ìƒì„¸ í‰ê°€ ê²°ê³¼')
    parser.add_argument('--output', default='results/statistical_analysis_v3.json', help='ì¶œë ¥ íŒŒì¼')
    parser.add_argument('--viz-output', default='results/statistical_analysis_v3.pdf', help='ì‹œê°í™” ì¶œë ¥')
    args = parser.parse_args()
    
    print("\n" + "ğŸ¯ " + "="*58)
    print("   í†µê³„ì  ìœ ì˜ì„± ê²€ì •")
    print("="*60)
    
    # ë¶„ì„ê¸° ì´ˆê¸°í™”
    analyzer = StatisticalAnalyzer(args.detailed_results)
    
    # ê²°ê³¼ ë¡œë“œ
    analyzer.load_results()
    
    # í†µê³„ ë¶„ì„ ìˆ˜í–‰
    results = analyzer.analyze_all_metrics()
    
    # ê²°ê³¼ ì €ì¥
    analyzer.save_results(args.output, results)
    
    # ì‹œê°í™” ìƒì„±
    analyzer.generate_visualization(args.viz_output)
    
    # ìš”ì•½ ì¶œë ¥
    print("\n" + "="*60)
    print("ğŸ“Š í†µê³„ ë¶„ì„ ìš”ì•½")
    print("="*60)
    
    # ANOVA ìš”ì•½
    print("\nğŸ”¬ ANOVA ê²°ê³¼:")
    for anova in results['anova']:
        sig_mark = "âœ…" if anova['significant'] else "âŒ"
        print(f"  {sig_mark} {anova['metric']}: F={anova['f_statistic']:.3f}, p={anova['p_value']:.4f}")
    
    # ìœ ì˜í•œ ë¹„êµ ìš”ì•½
    significant_comps = [c for c in results['comparisons'] if c['significant_after_bonferroni']]
    print(f"\nâœ… Bonferroni ë³´ì • í›„ ìœ ì˜í•œ ë¹„êµ: {len(significant_comps)}ê°œ")
    
    for comp in significant_comps[:10]:  # ìƒìœ„ 10ê°œë§Œ ì¶œë ¥
        print(f"  â€¢ {comp['method1']} vs {comp['method2']} ({comp['metric']}): p={comp['p_value']:.4f}, d={comp['cohens_d']:.3f}")
    
    print("\n" + "="*60)
    print("âœ… í†µê³„ì  ìœ ì˜ì„± ê²€ì • ì™„ë£Œ!")
    print("="*60)


if __name__ == "__main__":
    main()


#!/usr/bin/env python3
"""
ìˆ˜ë™ ìˆ˜ì§‘ ë°ì´í„° ìë™ í†µí•© ìŠ¤í¬ë¦½íŠ¸

ì‚¬ìš©ìê°€ ë‚˜ì¤‘ì— ë‹¤ìŒ 4ê°œ ë°ì´í„°ë¥¼ ì¶”ê°€í•  ë•Œ ì‹¤í–‰:
5. AI-Hub ê±´ì„¤ ì•ˆì „ ë°ì´í„°
8. BIMPROVE Dataset  
9. Stanford CIFE Dataset
10. êµ­í† êµí†µë¶€ BIM ìƒ˜í”Œ

ì‚¬ìš©ë²•:
  python scripts/integrate_manual_data.py --check   # ìƒíƒœ í™•ì¸
  python scripts/integrate_manual_data.py --process  # ìë™ ì²˜ë¦¬
"""

import argparse
import json
import shutil
from datetime import datetime
from pathlib import Path
import sys

PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT / "src"))

from contextualforget.core import extract_ifc_entities, parse_bcf_zip, write_jsonl


# ë°ì´í„° ê²½ë¡œ ì •ì˜
MANUAL_DATA_SOURCES = {
    "aihub": {
        "name": "AI-Hub ê±´ì„¤ ì•ˆì „",
        "path": PROJECT_ROOT / "data" / "external" / "aihub",
        "types": ["json", "csv", "images"],
        "priority": "ì¤‘ê¸°"
    },
    "bimprove": {
        "name": "BIMPROVE Academic Dataset",
        "path": PROJECT_ROOT / "data" / "external" / "bimprove",
        "types": ["ifc", "bcfzip"],
        "priority": "ì¥ê¸°"
    },
    "stanford": {
        "name": "Stanford CIFE Dataset",
        "path": PROJECT_ROOT / "data" / "external" / "stanford_cife",
        "types": ["ifc", "bcfzip"],
        "priority": "ì¥ê¸°"
    },
    "molit": {
        "name": "êµ­í† êµí†µë¶€ BIM ìƒ˜í”Œ",
        "path": PROJECT_ROOT / "data" / "external" / "public_korea" / "molit",
        "types": ["ifc"],
        "priority": "ì¤‘ê¸°"
    }
}


def check_manual_data_status():
    """ìˆ˜ë™ ë°ì´í„° ìˆ˜ì§‘ ìƒíƒœ í™•ì¸"""
    print("\n" + "="*60)
    print("ğŸ“‹ ìˆ˜ë™ ìˆ˜ì§‘ ë°ì´í„° ìƒíƒœ í™•ì¸")
    print("="*60)
    
    status = {}
    for key, source in MANUAL_DATA_SOURCES.items():
        path = source["path"]
        exists = path.exists()
        
        file_count = 0
        if exists:
            # IFC íŒŒì¼
            ifc_files = list(path.rglob("*.ifc"))
            # BCF íŒŒì¼
            bcf_files = list(path.rglob("*.bcf*"))
            # JSON íŒŒì¼
            json_files = list(path.rglob("*.json"))
            # CSV íŒŒì¼
            csv_files = list(path.rglob("*.csv"))
            
            file_count = len(ifc_files) + len(bcf_files) + len(json_files) + len(csv_files)
            
            status[key] = {
                "exists": True,
                "file_count": file_count,
                "ifc": len(ifc_files),
                "bcf": len(bcf_files),
                "json": len(json_files),
                "csv": len(csv_files)
            }
        else:
            status[key] = {
                "exists": False,
                "file_count": 0
            }
        
        # ì¶œë ¥
        icon = "âœ…" if exists and file_count > 0 else "â³" if exists else "âŒ"
        print(f"\n{icon} {source['name']}")
        print(f"   ê²½ë¡œ: {path}")
        print(f"   ìš°ì„ ìˆœìœ„: {source['priority']}")
        
        if exists:
            if file_count > 0:
                print(f"   ìƒíƒœ: ë°ì´í„° ë°œê²¬ ({file_count}ê°œ íŒŒì¼)")
                if status[key].get("ifc", 0) > 0:
                    print(f"     - IFC: {status[key]['ifc']}ê°œ")
                if status[key].get("bcf", 0) > 0:
                    print(f"     - BCF: {status[key]['bcf']}ê°œ")
                if status[key].get("json", 0) > 0:
                    print(f"     - JSON: {status[key]['json']}ê°œ")
                if status[key].get("csv", 0) > 0:
                    print(f"     - CSV: {status[key]['csv']}ê°œ")
            else:
                print(f"   ìƒíƒœ: ë””ë ‰í† ë¦¬ ìˆì§€ë§Œ ë°ì´í„° ì—†ìŒ")
        else:
            print(f"   ìƒíƒœ: ì•„ì§ ìˆ˜ì§‘ë˜ì§€ ì•ŠìŒ")
    
    # ìš”ì•½
    print(f"\n" + "="*60)
    print("ğŸ“Š ìš”ì•½")
    print("="*60)
    
    ready = sum(1 for s in status.values() if s["exists"] and s["file_count"] > 0)
    total = len(MANUAL_DATA_SOURCES)
    
    print(f"\nìˆ˜ì§‘ ì™„ë£Œ: {ready}/{total}ê°œ")
    
    if ready == 0:
        print("\nğŸ’¡ ì•„ì§ ìˆ˜ë™ ë°ì´í„°ê°€ ìˆ˜ì§‘ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("   ê°€ì´ë“œ: docs/DATA_COLLECTION_GUIDE_5-10.md ì°¸ì¡°")
    elif ready < total:
        print(f"\nâ³ {total - ready}ê°œ ë°ì´í„° ìˆ˜ì§‘ ëŒ€ê¸° ì¤‘")
        print("   ê°€ì´ë“œ: docs/DATA_COLLECTION_GUIDE_5-10.md ì°¸ì¡°")
    else:
        print("\nğŸ‰ ëª¨ë“  ìˆ˜ë™ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ!")
        print("   ë‹¤ìŒ: python scripts/integrate_manual_data.py --process")
    
    return status


def process_manual_data(status=None):
    """ìˆ˜ë™ ìˆ˜ì§‘ ë°ì´í„° ì²˜ë¦¬ ë° í†µí•©"""
    if status is None:
        status = check_manual_data_status()
    
    print("\n" + "="*60)
    print("ğŸ”„ ìˆ˜ë™ ë°ì´í„° ì²˜ë¦¬ ë° í†µí•©")
    print("="*60)
    
    raw_dir = PROJECT_ROOT / "data" / "raw" / "downloaded"
    processed_dir = PROJECT_ROOT / "data" / "processed"
    
    total_processed = 0
    
    for key, source in MANUAL_DATA_SOURCES.items():
        if not status[key]["exists"] or status[key]["file_count"] == 0:
            continue
        
        print(f"\nì²˜ë¦¬ ì¤‘: {source['name']}")
        path = source["path"]
        
        # IFC íŒŒì¼ ë³µì‚¬ ë° ì²˜ë¦¬
        ifc_files = list(path.rglob("*.ifc"))
        for ifc_file in ifc_files:
            try:
                # rawë¡œ ë³µì‚¬
                new_name = f"{key}_{ifc_file.stem}.ifc"
                dest = raw_dir / new_name
                
                if not dest.exists():
                    shutil.copy2(ifc_file, dest)
                    print(f"  âœ… IFC ë³µì‚¬: {new_name}")
                
                # JSONL ì²˜ë¦¬
                out_file = processed_dir / f"{key}_{ifc_file.stem}_ifc.jsonl"
                if not out_file.exists():
                    entities = extract_ifc_entities(str(dest))
                    if entities:
                        write_jsonl(str(out_file), entities)
                        print(f"  âœ… JSONL ìƒì„±: {out_file.name}")
                        total_processed += 1
            
            except Exception as e:
                print(f"  âŒ ì˜¤ë¥˜ ({ifc_file.name}): {e}")
        
        # BCF íŒŒì¼ ë³µì‚¬ ë° ì²˜ë¦¬
        bcf_files = list(path.rglob("*.bcf*"))
        for bcf_file in bcf_files:
            try:
                # rawë¡œ ë³µì‚¬
                new_name = f"{key}_{bcf_file.stem}.bcfzip"
                dest = raw_dir / new_name
                
                if not dest.exists():
                    shutil.copy2(bcf_file, dest)
                    print(f"  âœ… BCF ë³µì‚¬: {new_name}")
                
                # JSONL ì²˜ë¦¬
                out_file = processed_dir / f"{key}_{bcf_file.stem}_bcf.jsonl"
                if not out_file.exists():
                    issues = parse_bcf_zip(str(dest))
                    if issues:
                        write_jsonl(str(out_file), issues)
                        print(f"  âœ… JSONL ìƒì„±: {out_file.name}")
                        total_processed += 1
            
            except Exception as e:
                print(f"  âŒ ì˜¤ë¥˜ ({bcf_file.name}): {e}")
    
    print(f"\n" + "="*60)
    print(f"âœ… í†µí•© ì™„ë£Œ: {total_processed}ê°œ íŒŒì¼ ì²˜ë¦¬")
    print("="*60)
    
    if total_processed > 0:
        print("\në‹¤ìŒ ë‹¨ê³„:")
        print("  1. ê·¸ë˜í”„ ì¬êµ¬ì¶•: make build_graph")
        print("  2. í‰ê°€ ì¬ì‹¤í–‰: python scripts/comprehensive_evaluation.py")


def main():
    """ë©”ì¸ ì‹¤í–‰"""
    parser = argparse.ArgumentParser(description="ìˆ˜ë™ ìˆ˜ì§‘ ë°ì´í„° í†µí•©")
    parser.add_argument("--check", action="store_true", help="ìƒíƒœë§Œ í™•ì¸")
    parser.add_argument("--process", action="store_true", help="ë°ì´í„° ì²˜ë¦¬ ë° í†µí•©")
    
    args = parser.parse_args()
    
    if args.process:
        status = check_manual_data_status()
        process_manual_data(status)
    else:
        # ê¸°ë³¸: ìƒíƒœ í™•ì¸
        check_manual_data_status()


if __name__ == "__main__":
    main()


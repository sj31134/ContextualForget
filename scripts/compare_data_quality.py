#!/usr/bin/env python3
"""
ì‹¤ì œ ë°ì´í„° vs í•©ì„± ë°ì´í„° í’ˆì§ˆ ë¹„êµ ë¶„ì„
"""

import sys
import json
from pathlib import Path
from datetime import datetime
from collections import Counter
import statistics

# Add src to path for imports
sys.path.insert(0, str(Path(__file__).parent.parent / 'src'))

from contextualforget.core.utils import read_jsonl


def analyze_data_quality():
    """ì‹¤ì œ ë°ì´í„°ì™€ í•©ì„± ë°ì´í„°ì˜ í’ˆì§ˆì„ ë¹„êµ ë¶„ì„"""
    
    print("ğŸ” ë°ì´í„° í’ˆì§ˆ ë¹„êµ ë¶„ì„ ì‹œì‘...")
    
    # 1. ì‹¤ì œ BCF ë°ì´í„° ë¶„ì„
    print("\nğŸ“Š ì‹¤ì œ BCF ë°ì´í„° ë¶„ì„...")
    real_bcf_file = Path("data/processed/real_bcf/real_bcf_data.jsonl")
    real_data = list(read_jsonl(str(real_bcf_file)))
    
    real_stats = analyze_bcf_data(real_data, "ì‹¤ì œ BCF")
    
    # 2. ê¸°ì¡´ í•©ì„± ë°ì´í„° ë¶„ì„
    print("\nğŸ“Š ê¸°ì¡´ í•©ì„± ë°ì´í„° ë¶„ì„...")
    synthetic_files = list(Path("data/processed").glob("bcf_*.jsonl"))
    synthetic_data = []
    
    for file in synthetic_files:
        if file.name.startswith("bcf_"):
            data = list(read_jsonl(str(file)))
            synthetic_data.extend(data)
    
    synthetic_stats = analyze_bcf_data(synthetic_data, "í•©ì„± BCF")
    
    # 3. ë¹„êµ ë¶„ì„
    print("\nğŸ“ˆ ë°ì´í„° í’ˆì§ˆ ë¹„êµ ê²°ê³¼...")
    comparison = compare_data_quality(real_stats, synthetic_stats)
    
    # 4. ê²°ê³¼ ì €ì¥
    save_comparison_results(real_stats, synthetic_stats, comparison)
    
    return comparison


def analyze_bcf_data(data, data_type):
    """BCF ë°ì´í„°ì˜ í’ˆì§ˆ ì§€í‘œ ë¶„ì„"""
    
    stats = {
        "data_type": data_type,
        "total_issues": len(data),
        "unique_topic_ids": len(set(row.get('topic_id', '') for row in data)),
        "title_analysis": {},
        "author_analysis": {},
        "description_analysis": {},
        "date_analysis": {},
        "quality_metrics": {}
    }
    
    # ì œëª© ë¶„ì„
    titles = [row.get('title', '') for row in data]
    stats["title_analysis"] = {
        "non_empty_titles": len([t for t in titles if t.strip()]),
        "empty_titles": len([t for t in titles if not t.strip()]),
        "unique_titles": len(set(t for t in titles if t.strip())),
        "avg_title_length": statistics.mean([len(t) for t in titles if t.strip()]) if any(t.strip() for t in titles) else 0,
        "most_common_titles": dict(Counter(t for t in titles if t.strip()).most_common(5))
    }
    
    # ì‘ì„±ì ë¶„ì„
    authors = [row.get('author', '') for row in data]
    stats["author_analysis"] = {
        "non_empty_authors": len([a for a in authors if a.strip()]),
        "empty_authors": len([a for a in authors if not a.strip()]),
        "unique_authors": len(set(a for a in authors if a.strip())),
        "most_common_authors": dict(Counter(a for a in authors if a.strip()).most_common(5))
    }
    
    # ì„¤ëª… ë¶„ì„
    descriptions = [row.get('description', '') for row in data]
    stats["description_analysis"] = {
        "non_empty_descriptions": len([d for d in descriptions if d.strip()]),
        "empty_descriptions": len([d for d in descriptions if not d.strip()]),
        "avg_description_length": statistics.mean([len(d) for d in descriptions if d.strip()]) if any(d.strip() for d in descriptions) else 0,
        "max_description_length": max([len(d) for d in descriptions if d.strip()]) if any(d.strip() for d in descriptions) else 0
    }
    
    # ë‚ ì§œ ë¶„ì„
    dates = [row.get('created', '') for row in data]
    stats["date_analysis"] = {
        "non_empty_dates": len([d for d in dates if d.strip()]),
        "empty_dates": len([d for d in dates if not d.strip()]),
        "date_format_consistency": len(set(d for d in dates if d.strip()))
    }
    
    # í’ˆì§ˆ ì§€í‘œ ê³„ì‚°
    total_issues = len(data)
    stats["quality_metrics"] = {
        "completeness_score": calculate_completeness_score(stats),
        "diversity_score": calculate_diversity_score(stats),
        "consistency_score": calculate_consistency_score(stats),
        "overall_quality_score": 0  # ë‚˜ì¤‘ì— ê³„ì‚°
    }
    
    # ì „ì²´ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
    quality_metrics = stats["quality_metrics"]
    stats["quality_metrics"]["overall_quality_score"] = (
        quality_metrics["completeness_score"] * 0.4 +
        quality_metrics["diversity_score"] * 0.3 +
        quality_metrics["consistency_score"] * 0.3
    )
    
    print(f"  âœ… {data_type} ë¶„ì„ ì™„ë£Œ:")
    print(f"    ğŸ“‹ ì´ ì´ìŠˆ ìˆ˜: {stats['total_issues']}")
    print(f"    ğŸ¯ ê³ ìœ  í† í”½ ID: {stats['unique_topic_ids']}")
    print(f"    ğŸ“ ë¹„ì–´ìˆì§€ ì•Šì€ ì œëª©: {stats['title_analysis']['non_empty_titles']}")
    print(f"    ğŸ‘¤ ë¹„ì–´ìˆì§€ ì•Šì€ ì‘ì„±ì: {stats['author_analysis']['non_empty_authors']}")
    print(f"    ğŸ“„ ë¹„ì–´ìˆì§€ ì•Šì€ ì„¤ëª…: {stats['description_analysis']['non_empty_descriptions']}")
    print(f"    â­ ì „ì²´ í’ˆì§ˆ ì ìˆ˜: {stats['quality_metrics']['overall_quality_score']:.2f}")
    
    return stats


def calculate_completeness_score(stats):
    """ì™„ì„±ë„ ì ìˆ˜ ê³„ì‚° (0-1)"""
    total_issues = stats["total_issues"]
    if total_issues == 0:
        return 0.0
    
    # ê° í•„ë“œì˜ ì™„ì„±ë„ ê³„ì‚°
    title_completeness = stats["title_analysis"]["non_empty_titles"] / total_issues
    author_completeness = stats["author_analysis"]["non_empty_authors"] / total_issues
    description_completeness = stats["description_analysis"]["non_empty_descriptions"] / total_issues
    date_completeness = stats["date_analysis"]["non_empty_dates"] / total_issues
    
    # ê°€ì¤‘ í‰ê·  (ì„¤ëª…ì´ ê°€ì¥ ì¤‘ìš”)
    completeness = (
        title_completeness * 0.2 +
        author_completeness * 0.2 +
        description_completeness * 0.4 +
        date_completeness * 0.2
    )
    
    return completeness


def calculate_diversity_score(stats):
    """ë‹¤ì–‘ì„± ì ìˆ˜ ê³„ì‚° (0-1)"""
    total_issues = stats["total_issues"]
    if total_issues == 0:
        return 0.0
    
    # ì œëª© ë‹¤ì–‘ì„±
    unique_titles = stats["title_analysis"]["unique_titles"]
    title_diversity = min(1.0, unique_titles / total_issues)
    
    # ì‘ì„±ì ë‹¤ì–‘ì„±
    unique_authors = stats["author_analysis"]["unique_authors"]
    author_diversity = min(1.0, unique_authors / total_issues) if total_issues > 0 else 0.0
    
    # ì „ì²´ ë‹¤ì–‘ì„± (ì œëª©ê³¼ ì‘ì„±ì í‰ê· )
    diversity = (title_diversity + author_diversity) / 2
    
    return diversity


def calculate_consistency_score(stats):
    """ì¼ê´€ì„± ì ìˆ˜ ê³„ì‚° (0-1)"""
    # ë‚ ì§œ í˜•ì‹ ì¼ê´€ì„±
    date_consistency = 1.0 if stats["date_analysis"]["date_format_consistency"] <= 1 else 0.5
    
    # ì œëª© ê¸¸ì´ ì¼ê´€ì„± (í‘œì¤€í¸ì°¨ê°€ ì‘ì„ìˆ˜ë¡ ì¼ê´€ì„± ë†’ìŒ)
    title_lengths = [len(t) for t in [row.get('title', '') for row in []] if t.strip()]
    if len(title_lengths) > 1:
        title_std = statistics.stdev(title_lengths)
        title_consistency = max(0, 1.0 - (title_std / 50))  # 50ì ê¸°ì¤€ìœ¼ë¡œ ì •ê·œí™”
    else:
        title_consistency = 1.0
    
    consistency = (date_consistency + title_consistency) / 2
    
    return consistency


def compare_data_quality(real_stats, synthetic_stats):
    """ì‹¤ì œ ë°ì´í„°ì™€ í•©ì„± ë°ì´í„° í’ˆì§ˆ ë¹„êµ"""
    
    comparison = {
        "comparison_date": datetime.now().isoformat(),
        "data_volume_comparison": {},
        "quality_metrics_comparison": {},
        "field_analysis_comparison": {},
        "recommendations": []
    }
    
    # ë°ì´í„° ë³¼ë¥¨ ë¹„êµ
    comparison["data_volume_comparison"] = {
        "real_data_issues": real_stats["total_issues"],
        "synthetic_data_issues": synthetic_stats["total_issues"],
        "volume_ratio": real_stats["total_issues"] / synthetic_stats["total_issues"] if synthetic_stats["total_issues"] > 0 else float('inf')
    }
    
    # í’ˆì§ˆ ì§€í‘œ ë¹„êµ
    real_quality = real_stats["quality_metrics"]
    synthetic_quality = synthetic_stats["quality_metrics"]
    
    comparison["quality_metrics_comparison"] = {
        "completeness": {
            "real": real_quality["completeness_score"],
            "synthetic": synthetic_quality["completeness_score"],
            "difference": real_quality["completeness_score"] - synthetic_quality["completeness_score"]
        },
        "diversity": {
            "real": real_quality["diversity_score"],
            "synthetic": synthetic_quality["diversity_score"],
            "difference": real_quality["diversity_score"] - synthetic_quality["diversity_score"]
        },
        "consistency": {
            "real": real_quality["consistency_score"],
            "synthetic": synthetic_quality["consistency_score"],
            "difference": real_quality["consistency_score"] - synthetic_quality["consistency_score"]
        },
        "overall": {
            "real": real_quality["overall_quality_score"],
            "synthetic": synthetic_quality["overall_quality_score"],
            "difference": real_quality["overall_quality_score"] - synthetic_quality["overall_quality_score"]
        }
    }
    
    # í•„ë“œë³„ ë¶„ì„ ë¹„êµ
    comparison["field_analysis_comparison"] = {
        "titles": {
            "real_non_empty": real_stats["title_analysis"]["non_empty_titles"],
            "synthetic_non_empty": synthetic_stats["title_analysis"]["non_empty_titles"],
            "real_avg_length": real_stats["title_analysis"]["avg_title_length"],
            "synthetic_avg_length": synthetic_stats["title_analysis"]["avg_title_length"]
        },
        "authors": {
            "real_non_empty": real_stats["author_analysis"]["non_empty_authors"],
            "synthetic_non_empty": synthetic_stats["author_analysis"]["non_empty_authors"],
            "real_unique": real_stats["author_analysis"]["unique_authors"],
            "synthetic_unique": synthetic_stats["author_analysis"]["unique_authors"]
        },
        "descriptions": {
            "real_non_empty": real_stats["description_analysis"]["non_empty_descriptions"],
            "synthetic_non_empty": synthetic_stats["description_analysis"]["non_empty_descriptions"],
            "real_avg_length": real_stats["description_analysis"]["avg_description_length"],
            "synthetic_avg_length": synthetic_stats["description_analysis"]["avg_description_length"]
        }
    }
    
    # ê¶Œì¥ì‚¬í•­ ìƒì„±
    recommendations = []
    
    if real_quality["overall_quality_score"] > synthetic_quality["overall_quality_score"]:
        recommendations.append("ì‹¤ì œ ë°ì´í„°ê°€ í•©ì„± ë°ì´í„°ë³´ë‹¤ ì „ë°˜ì ìœ¼ë¡œ ë†’ì€ í’ˆì§ˆì„ ë³´ì…ë‹ˆë‹¤.")
    
    if real_quality["completeness_score"] > synthetic_quality["completeness_score"]:
        recommendations.append("ì‹¤ì œ ë°ì´í„°ì˜ ì™„ì„±ë„ê°€ ë” ë†’ìŠµë‹ˆë‹¤. ì‹¤ì œ ë°ì´í„°ë¥¼ ì£¼ìš” í›ˆë ¨ ë°ì´í„°ë¡œ ì‚¬ìš©í•˜ì„¸ìš”.")
    
    if real_quality["diversity_score"] > synthetic_quality["diversity_score"]:
        recommendations.append("ì‹¤ì œ ë°ì´í„°ì˜ ë‹¤ì–‘ì„±ì´ ë” ë†’ìŠµë‹ˆë‹¤. ë” í’ë¶€í•œ íŒ¨í„´ í•™ìŠµì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
    
    if real_stats["total_issues"] < synthetic_stats["total_issues"]:
        recommendations.append("ì‹¤ì œ ë°ì´í„°ì˜ ì–‘ì´ ì ìŠµë‹ˆë‹¤. í•©ì„± ë°ì´í„°ë¥¼ ë³´ì™„ ë°ì´í„°ë¡œ í™œìš©í•˜ì„¸ìš”.")
    
    comparison["recommendations"] = recommendations
    
    # ê²°ê³¼ ì¶œë ¥
    print("\n" + "="*60)
    print("ğŸ“Š ë°ì´í„° í’ˆì§ˆ ë¹„êµ ê²°ê³¼")
    print("="*60)
    
    print(f"\nğŸ“ˆ í’ˆì§ˆ ì§€í‘œ ë¹„êµ:")
    print(f"  ì™„ì„±ë„: ì‹¤ì œ {real_quality['completeness_score']:.3f} vs í•©ì„± {synthetic_quality['completeness_score']:.3f}")
    print(f"  ë‹¤ì–‘ì„±: ì‹¤ì œ {real_quality['diversity_score']:.3f} vs í•©ì„± {synthetic_quality['diversity_score']:.3f}")
    print(f"  ì¼ê´€ì„±: ì‹¤ì œ {real_quality['consistency_score']:.3f} vs í•©ì„± {synthetic_quality['consistency_score']:.3f}")
    print(f"  ì „ì²´: ì‹¤ì œ {real_quality['overall_quality_score']:.3f} vs í•©ì„± {synthetic_quality['overall_quality_score']:.3f}")
    
    print(f"\nğŸ“‹ ë°ì´í„° ë³¼ë¥¨:")
    print(f"  ì‹¤ì œ ë°ì´í„°: {real_stats['total_issues']}ê°œ ì´ìŠˆ")
    print(f"  í•©ì„± ë°ì´í„°: {synthetic_stats['total_issues']}ê°œ ì´ìŠˆ")
    
    print(f"\nğŸ’¡ ê¶Œì¥ì‚¬í•­:")
    for i, rec in enumerate(recommendations, 1):
        print(f"  {i}. {rec}")
    
    return comparison


def save_comparison_results(real_stats, synthetic_stats, comparison):
    """ë¹„êµ ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
    
    output_dir = Path("data/analysis")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # ì „ì²´ ê²°ê³¼ ì €ì¥
    full_results = {
        "real_data_analysis": real_stats,
        "synthetic_data_analysis": synthetic_stats,
        "comparison": comparison
    }
    
    results_file = output_dir / "data_quality_comparison.json"
    with open(results_file, 'w', encoding='utf-8') as f:
        json.dump(full_results, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ’¾ ë¹„êµ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {results_file}")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ ë°ì´í„° í’ˆì§ˆ ë¹„êµ ë¶„ì„ ì‹œì‘")
    print("="*60)
    
    try:
        comparison = analyze_data_quality()
        
        print("\n" + "="*60)
        print("ğŸ‰ ë°ì´í„° í’ˆì§ˆ ë¹„êµ ë¶„ì„ ì™„ë£Œ!")
        
        # ì£¼ìš” ê²°ë¡ 
        real_score = comparison["quality_metrics_comparison"]["overall"]["real"]
        synthetic_score = comparison["quality_metrics_comparison"]["overall"]["synthetic"]
        
        if real_score > synthetic_score:
            print(f"âœ… ì‹¤ì œ ë°ì´í„°ê°€ í•©ì„± ë°ì´í„°ë³´ë‹¤ {real_score - synthetic_score:.3f}ì  ë†’ì€ í’ˆì§ˆì„ ë³´ì…ë‹ˆë‹¤!")
        else:
            print(f"âš ï¸ í•©ì„± ë°ì´í„°ê°€ ì‹¤ì œ ë°ì´í„°ë³´ë‹¤ {synthetic_score - real_score:.3f}ì  ë†’ì€ í’ˆì§ˆì„ ë³´ì…ë‹ˆë‹¤.")
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return 1
    
    return 0


if __name__ == "__main__":
    exit(main())

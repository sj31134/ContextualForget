#!/usr/bin/env python3
"""ìˆ˜ì§‘ëœ ë°ì´í„° ë¶„ì„ ë° ë¶„ë¥˜"""

import os
import json
from pathlib import Path
from datetime import datetime, timedelta
import random
import shutil
import pickle


class DataAnalyzer:
    """ë°ì´í„° ë¶„ì„ ë° ë¶„ë¥˜"""
    
    def __init__(self, base_dir: Path):
        self.base_dir = Path(base_dir)
        self.raw_dir = self.base_dir / "data" / "raw" / "downloaded"
        self.analysis_dir = self.base_dir / "data" / "analysis"
        self.analysis_dir.mkdir(parents=True, exist_ok=True)
        
        self.ifc_files = []
        self.bcf_files = []
    
    def scan_all_data(self):
        """ëª¨ë“  ë°ì´í„° ìŠ¤ìº”"""
        print("=" * 70)
        print("ğŸ” ì „ì²´ ë°ì´í„° ìŠ¤ìº”")
        print("=" * 70)
        
        # Raw ë””ë ‰í† ë¦¬
        raw_ifc = list(self.raw_dir.glob("*.ifc"))
        raw_bcf = list(self.raw_dir.glob("*.bcf*"))
        
        # ê¸°ì¡´ data ë””ë ‰í† ë¦¬
        data_dir = self.base_dir / "data"
        existing_ifc = list(data_dir.glob("*.ifc"))
        existing_bcf = list((data_dir / "raw").glob("*.bcfzip"))
        
        # í•©ì¹˜ê¸°
        self.ifc_files = raw_ifc + existing_ifc
        self.bcf_files = raw_bcf + existing_bcf
        
        print(f"\nğŸ“ IFC íŒŒì¼: {len(self.ifc_files)}ê°œ")
        print(f"ğŸ“‹ BCF íŒŒì¼: {len(self.bcf_files)}ê°œ")
        print(f"ğŸ“Š ì´ íŒŒì¼: {len(self.ifc_files) + len(self.bcf_files)}ê°œ")
    
    def analyze_ifc_files(self):
        """IFC íŒŒì¼ ìƒì„¸ ë¶„ì„"""
        print("\n" + "=" * 70)
        print("ğŸ“Š IFC íŒŒì¼ ìƒì„¸ ë¶„ì„")
        print("=" * 70)
        
        analysis = {
            "total_count": len(self.ifc_files),
            "total_size_mb": 0,
            "size_categories": {
                "small": [],  # < 100 KB
                "medium": [],  # 100 KB ~ 1 MB
                "large": [],  # 1 MB ~ 10 MB
                "xlarge": []  # > 10 MB
            },
            "complexity_estimate": {},
            "files": []
        }
        
        for ifc_file in self.ifc_files:
            size_kb = ifc_file.stat().st_size / 1024
            size_mb = size_kb / 1024
            analysis["total_size_mb"] += size_mb
            
            # í¬ê¸° ë¶„ë¥˜
            if size_kb < 100:
                category = "small"
            elif size_kb < 1024:
                category = "medium"
            elif size_mb < 10:
                category = "large"
            else:
                category = "xlarge"
            
            analysis["size_categories"][category].append(ifc_file.name)
            
            # ë³µì¡ë„ ì¶”ì • (íŒŒì¼ í¬ê¸° ê¸°ë°˜)
            if size_kb < 50:
                complexity = "simple"
            elif size_kb < 500:
                complexity = "moderate"
            elif size_mb < 5:
                complexity = "complex"
            else:
                complexity = "very_complex"
            
            file_info = {
                "name": ifc_file.name,
                "path": str(ifc_file),
                "size_kb": round(size_kb, 2),
                "size_mb": round(size_mb, 2),
                "category": category,
                "complexity": complexity
            }
            
            analysis["files"].append(file_info)
        
        # í†µê³„
        for cat, files in analysis["size_categories"].items():
            print(f"\n  {cat.upper()}: {len(files)}ê°œ")
            if files:
                for f in files[:3]:  # ì²˜ìŒ 3ê°œë§Œ
                    print(f"    - {f}")
                if len(files) > 3:
                    print(f"    ... (+{len(files) - 3}ê°œ)")
        
        # ë³µì¡ë„ í†µê³„
        complexity_counts = {}
        for f in analysis["files"]:
            comp = f["complexity"]
            complexity_counts[comp] = complexity_counts.get(comp, 0) + 1
        
        print(f"\n  ë³µì¡ë„ ë¶„í¬:")
        for comp, count in complexity_counts.items():
            print(f"    - {comp}: {count}ê°œ")
        
        # ì €ì¥
        with open(self.analysis_dir / "ifc_analysis.json", 'w') as f:
            json.dump(analysis, f, indent=2, ensure_ascii=False)
        
        return analysis
    
    def analyze_bcf_files(self):
        """BCF íŒŒì¼ ë¶„ì„"""
        print("\n" + "=" * 70)
        print("ğŸ“Š BCF íŒŒì¼ ë¶„ì„")
        print("=" * 70)
        
        analysis = {
            "total_count": len(self.bcf_files),
            "total_size_mb": sum(f.stat().st_size for f in self.bcf_files) / 1024 / 1024,
            "files": []
        }
        
        for bcf_file in self.bcf_files:
            size_kb = bcf_file.stat().st_size / 1024
            
            file_info = {
                "name": bcf_file.name,
                "path": str(bcf_file),
                "size_kb": round(size_kb, 2)
            }
            
            analysis["files"].append(file_info)
            print(f"  - {bcf_file.name} ({size_kb:.1f} KB)")
        
        # ì €ì¥
        with open(self.analysis_dir / "bcf_analysis.json", 'w') as f:
            json.dump(analysis, f, indent=2, ensure_ascii=False)
        
        return analysis
    
    def classify_for_research(self, ifc_analysis):
        """ì—°êµ¬ ëª©ì ë³„ ë¶„ë¥˜"""
        print("\n" + "=" * 70)
        print("ğŸ·ï¸  ì—°êµ¬ ëª©ì ë³„ ë°ì´í„° ë¶„ë¥˜")
        print("=" * 70)
        
        # í›ˆë ¨/ê²€ì¦/í…ŒìŠ¤íŠ¸ ë¶„í•  (70/15/15)
        all_files = ifc_analysis["files"]
        random.shuffle(all_files)
        
        total = len(all_files)
        train_size = int(total * 0.70)
        val_size = int(total * 0.15)
        
        classification = {
            "train": all_files[:train_size],
            "validation": all_files[train_size:train_size + val_size],
            "test": all_files[train_size + val_size:],
            "statistics": {
                "total": total,
                "train": train_size,
                "validation": val_size,
                "test": total - train_size - val_size
            }
        }
        
        print(f"\n  ğŸ“Š ë°ì´í„° ë¶„í• :")
        print(f"    - í›ˆë ¨ (Train): {classification['statistics']['train']}ê°œ (70%)")
        print(f"    - ê²€ì¦ (Validation): {classification['statistics']['validation']}ê°œ (15%)")
        print(f"    - í…ŒìŠ¤íŠ¸ (Test): {classification['statistics']['test']}ê°œ (15%)")
        
        # ë³µì¡ë„ë³„ ë¶„í¬ í™•ì¸
        for split_name, split_data in [("train", classification["train"]), 
                                       ("validation", classification["validation"]),
                                       ("test", classification["test"])]:
            complexity_dist = {}
            for f in split_data:
                comp = f["complexity"]
                complexity_dist[comp] = complexity_dist.get(comp, 0) + 1
            
            print(f"\n  {split_name.upper()} ë³µì¡ë„ ë¶„í¬:")
            for comp, count in sorted(complexity_dist.items()):
                print(f"    - {comp}: {count}ê°œ")
        
        # ì €ì¥
        with open(self.analysis_dir / "data_classification.json", 'w') as f:
            json.dump(classification, f, indent=2, ensure_ascii=False)
        
        return classification
    
    def assess_sufficiency(self, ifc_analysis, bcf_analysis):
        """ë°ì´í„° ì¶©ë¶„ì„± í‰ê°€"""
        print("\n" + "=" * 70)
        print("âœ… ë°ì´í„° ì¶©ë¶„ì„± í‰ê°€")
        print("=" * 70)
        
        # ê¸°ì¤€
        requirements = {
            "minimum": {
                "ifc_count": 15,
                "bcf_count": 30,
                "total_ifc_size_mb": 10,
                "complexity_diversity": 3  # simple, moderate, complex
            },
            "ideal": {
                "ifc_count": 25,
                "bcf_count": 50,
                "total_ifc_size_mb": 50,
                "complexity_diversity": 4  # + very_complex
            }
        }
        
        # í˜„ì¬ ìƒíƒœ
        current = {
            "ifc_count": ifc_analysis["total_count"],
            "bcf_count": bcf_analysis["total_count"],
            "total_ifc_size_mb": ifc_analysis["total_size_mb"],
            "complexity_diversity": len(set(f["complexity"] for f in ifc_analysis["files"]))
        }
        
        # í‰ê°€
        assessment = {
            "current": current,
            "requirements": requirements,
            "status": {}
        }
        
        print(f"\n  ğŸ“Š í˜„ì¬ ìƒíƒœ:")
        print(f"    IFC íŒŒì¼: {current['ifc_count']}ê°œ (ëª©í‘œ: ìµœì†Œ {requirements['minimum']['ifc_count']}ê°œ, ì´ìƒì  {requirements['ideal']['ifc_count']}ê°œ)")
        print(f"    BCF íŒŒì¼: {current['bcf_count']}ê°œ (ëª©í‘œ: ìµœì†Œ {requirements['minimum']['bcf_count']}ê°œ, ì´ìƒì  {requirements['ideal']['bcf_count']}ê°œ)")
        print(f"    ì´ í¬ê¸°: {current['total_ifc_size_mb']:.2f} MB")
        print(f"    ë³µì¡ë„ ë‹¤ì–‘ì„±: {current['complexity_diversity']}ì¢…ë¥˜")
        
        # ìµœì†Œ ìš”êµ¬ì‚¬í•­ ì¶©ì¡± ì—¬ë¶€
        min_req_met = (
            current["ifc_count"] >= requirements["minimum"]["ifc_count"] and
            current["bcf_count"] >= requirements["minimum"]["bcf_count"]
        )
        
        # ì´ìƒì  ìš”êµ¬ì‚¬í•­ ì¶©ì¡± ì—¬ë¶€
        ideal_req_met = (
            current["ifc_count"] >= requirements["ideal"]["ifc_count"] and
            current["bcf_count"] >= requirements["ideal"]["bcf_count"]
        )
        
        if ideal_req_met:
            status = "âœ… IDEAL - Top-tier í•™íšŒ ëª©í‘œ ê°€ëŠ¥"
            score = 100
        elif min_req_met:
            status = "âœ… SUFFICIENT - í•™ìˆ  ë…¼ë¬¸ ë°œí‘œ ê°€ëŠ¥"
            score = 80
        else:
            status = "âš ï¸  INSUFFICIENT - ì¶”ê°€ ë°ì´í„° í•„ìš”"
            score = 50
        
        assessment["status"] = {
            "level": status,
            "score": score,
            "minimum_met": min_req_met,
            "ideal_met": ideal_req_met
        }
        
        print(f"\n  ğŸ¯ í‰ê°€ ê²°ê³¼: {status}")
        print(f"  ğŸ“ˆ ì ìˆ˜: {score}/100")
        
        # ê¶Œì¥ì‚¬í•­
        if not ideal_req_met:
            needed_ifc = max(0, requirements["ideal"]["ifc_count"] - current["ifc_count"])
            needed_bcf = max(0, requirements["ideal"]["bcf_count"] - current["bcf_count"])
            
            print(f"\n  ğŸ’¡ ê¶Œì¥ì‚¬í•­:")
            if needed_ifc > 0:
                print(f"    - IFC íŒŒì¼ {needed_ifc}ê°œ ì¶”ê°€ ìˆ˜ì§‘")
            if needed_bcf > 0:
                print(f"    - BCF íŒŒì¼ {needed_bcf}ê°œ ì¶”ê°€ ìƒì„±/ìˆ˜ì§‘")
        
        # ì €ì¥
        with open(self.analysis_dir / "sufficiency_assessment.json", 'w') as f:
            json.dump(assessment, f, indent=2, ensure_ascii=False)
        
        return assessment
    
    def generate_comprehensive_report(self, ifc_analysis, bcf_analysis, classification, assessment):
        """ì¢…í•© ë³´ê³ ì„œ ìƒì„±"""
        print("\n" + "=" * 70)
        print("ğŸ“ ì¢…í•© ë°ì´í„° ë³´ê³ ì„œ ìƒì„±")
        print("=" * 70)
        
        report = {
            "generated_at": datetime.now().isoformat(),
            "summary": {
                "total_ifc": ifc_analysis["total_count"],
                "total_bcf": bcf_analysis["total_count"],
                "total_size_mb": ifc_analysis["total_size_mb"] + bcf_analysis["total_size_mb"],
                "sufficiency_score": assessment["status"]["score"],
                "sufficiency_level": assessment["status"]["level"]
            },
            "ifc_breakdown": {
                "size_distribution": {
                    k: len(v) for k, v in ifc_analysis["size_categories"].items()
                },
                "complexity_distribution": {}
            },
            "data_splits": classification["statistics"],
            "next_steps": []
        }
        
        # ë³µì¡ë„ ë¶„í¬
        for f in ifc_analysis["files"]:
            comp = f["complexity"]
            report["ifc_breakdown"]["complexity_distribution"][comp] = \
                report["ifc_breakdown"]["complexity_distribution"].get(comp, 0) + 1
        
        # ë‹¤ìŒ ë‹¨ê³„
        if assessment["status"]["ideal_met"]:
            report["next_steps"] = [
                "âœ… ë°ì´í„° ì¶©ë¶„ - Phase 1 ì‹œì‘ ê°€ëŠ¥",
                "ì‹œê°„ì  ë‹¤ì–‘ì„± ì¶”ê°€ (3-12ê°œì›” ë¶„ì‚°)",
                "Gold Standard QA ë°ì´í„°ì…‹ ìƒì„±"
            ]
        elif assessment["status"]["minimum_met"]:
            report["next_steps"] = [
                "âœ… ìµœì†Œ ìš”êµ¬ì‚¬í•­ ì¶©ì¡±",
                "ì¶”ê°€ ë°ì´í„° ìˆ˜ì§‘ (ì´ìƒì  ìˆ˜ì¤€)",
                "ì‹œê°„ì  ë‹¤ì–‘ì„± ì¶”ê°€",
                "Phase 1 ì‹œì‘ ê°€ëŠ¥ (ë‹¨, ì¶”ê°€ ìˆ˜ì§‘ ë³‘í–‰)"
            ]
        else:
            report["next_steps"] = [
                "âš ï¸  ì¶”ê°€ ë°ì´í„° ìˆ˜ì§‘ í•„ìˆ˜",
                "BCF íŒŒì¼ ìƒì„± ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰",
                "Academic ë°ì´í„°ì…‹ ì‹ ì²­",
                "Phase 1 ì‹œì‘ ì „ ë°ì´í„° ë³´ê°•"
            ]
        
        # íŒŒì¼ë¡œ ì €ì¥
        report_path = self.analysis_dir / "comprehensive_data_report.json"
        with open(report_path, 'w') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        # ë§ˆí¬ë‹¤ìš´ ë³´ê³ ì„œ
        md_path = self.analysis_dir / "DATA_REPORT.md"
        with open(md_path, 'w') as f:
            f.write(f"""# ContextualForget ë°ì´í„° ìˆ˜ì§‘ ë³´ê³ ì„œ

ìƒì„± ì¼ì‹œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## ğŸ“Š ë°ì´í„° ìš”ì•½

- **ì´ IFC íŒŒì¼**: {report['summary']['total_ifc']}ê°œ
- **ì´ BCF íŒŒì¼**: {report['summary']['total_bcf']}ê°œ
- **ì´ í¬ê¸°**: {report['summary']['total_size_mb']:.2f} MB
- **ì¶©ë¶„ì„± ì ìˆ˜**: {report['summary']['sufficiency_score']}/100
- **í‰ê°€**: {report['summary']['sufficiency_level']}

## ğŸ“ IFC íŒŒì¼ ë¶„í¬

### í¬ê¸°ë³„
- Small (< 100 KB): {report['ifc_breakdown']['size_distribution']['small']}ê°œ
- Medium (100 KB ~ 1 MB): {report['ifc_breakdown']['size_distribution']['medium']}ê°œ
- Large (1 ~ 10 MB): {report['ifc_breakdown']['size_distribution']['large']}ê°œ
- XLarge (> 10 MB): {report['ifc_breakdown']['size_distribution']['xlarge']}ê°œ

### ë³µì¡ë„ë³„
""")
            for comp, count in report['ifc_breakdown']['complexity_distribution'].items():
                f.write(f"- {comp}: {count}ê°œ\n")
            
            f.write(f"""

## ğŸ“‹ ë°ì´í„° ë¶„í• 

- **í›ˆë ¨ (Train)**: {report['data_splits']['train']}ê°œ (70%)
- **ê²€ì¦ (Validation)**: {report['data_splits']['validation']}ê°œ (15%)
- **í…ŒìŠ¤íŠ¸ (Test)**: {report['data_splits']['test']}ê°œ (15%)

## ğŸ¯ ë‹¤ìŒ ë‹¨ê³„

""")
            for step in report['next_steps']:
                f.write(f"{step}\n")
        
        print(f"\n  âœ… JSON ë³´ê³ ì„œ: {report_path}")
        print(f"  âœ… Markdown ë³´ê³ ì„œ: {md_path}")
        
        return report


def main():
    base_dir = Path(__file__).parent.parent
    analyzer = DataAnalyzer(base_dir)
    
    print("ğŸ”¬ ContextualForget ë°ì´í„° ë¶„ì„ & ë¶„ë¥˜")
    print(f"â° {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
    
    # 1. ìŠ¤ìº”
    analyzer.scan_all_data()
    
    # 2. IFC ë¶„ì„
    ifc_analysis = analyzer.analyze_ifc_files()
    
    # 3. BCF ë¶„ì„
    bcf_analysis = analyzer.analyze_bcf_files()
    
    # 4. ì—°êµ¬ ëª©ì ë³„ ë¶„ë¥˜
    classification = analyzer.classify_for_research(ifc_analysis)
    
    # 5. ì¶©ë¶„ì„± í‰ê°€
    assessment = analyzer.assess_sufficiency(ifc_analysis, bcf_analysis)
    
    # 6. ì¢…í•© ë³´ê³ ì„œ
    report = analyzer.generate_comprehensive_report(
        ifc_analysis, bcf_analysis, classification, assessment
    )
    
    print("\n" + "=" * 70)
    print("âœ¨ ë°ì´í„° ë¶„ì„ ë° ë¶„ë¥˜ ì™„ë£Œ!")
    print("=" * 70)
    print(f"\nğŸ“Š ê²°ê³¼ íŒŒì¼:")
    print(f"  - data/analysis/ifc_analysis.json")
    print(f"  - data/analysis/bcf_analysis.json")
    print(f"  - data/analysis/data_classification.json")
    print(f"  - data/analysis/sufficiency_assessment.json")
    print(f"  - data/analysis/comprehensive_data_report.json")
    print(f"  - data/analysis/DATA_REPORT.md")


if __name__ == "__main__":
    main()


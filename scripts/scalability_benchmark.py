#!/usr/bin/env python3
"""
í™•ì¥ì„± ë²¤ì¹˜ë§ˆí¬ ìŠ¤í¬ë¦½íŠ¸
ë…¸ë“œ ìˆ˜ë³„ ì‘ë‹µ ì‹œê°„, ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰, CPU ì‚¬ìš©ë¥  ì¸¡ì •
"""

import sys
import json
import time
import psutil
import gc
from pathlib import Path
from datetime import datetime
from collections import defaultdict

# Add src to path for imports
sys.path.insert(0, str(Path(__file__).parent.parent / 'src'))

from contextualforget.core.utils import read_jsonl
from contextualforget.baselines.bm25_engine import BM25QueryEngine
from contextualforget.baselines.vector_engine import VectorQueryEngine
from contextualforget.query.contextual_forget_engine import ContextualForgetEngine
from contextualforget.query.adaptive_retrieval import HybridRetrievalEngine
import networkx as nx


def create_scalability_graphs():
    """í™•ì¥ì„± í…ŒìŠ¤íŠ¸ìš© ê·¸ë˜í”„ ìƒì„±"""
    
    print("ğŸ“Š í™•ì¥ì„± í…ŒìŠ¤íŠ¸ìš© ê·¸ë˜í”„ ìƒì„± ì¤‘...")
    
    # í…ŒìŠ¤íŠ¸í•  ë…¸ë“œ ìˆ˜
    node_counts = [100, 500, 1000, 2000, 5000, 10000]
    graphs = {}
    
    for node_count in node_counts:
        print(f"  ğŸ”§ {node_count}ê°œ ë…¸ë“œ ê·¸ë˜í”„ ìƒì„± ì¤‘...")
        
        # ê·¸ë˜í”„ ìƒì„±
        graph = nx.DiGraph()
        
        # BCF ë…¸ë“œ ì¶”ê°€
        for i in range(node_count // 2):
            node_id = ("BCF", f"bcf_{i:06d}")
            graph.add_node(node_id, 
                          topic_id=f"topic_{i:06d}",
                          title=f"Test Issue {i}",
                          description=f"Test description for issue {i}",
                          author=f"engineer_{i%10:03d}",
                          created=datetime.now().isoformat())
        
        # IFC ë…¸ë“œ ì¶”ê°€
        for i in range(node_count // 2):
            node_id = ("IFC", f"ifc_{i:06d}")
            graph.add_node(node_id,
                          guid=f"ifc_guid_{i:06d}",
                          type=f"IFC_TYPE_{i%20}",
                          name=f"IFC Entity {i}")
        
        # ê°„ë‹¨í•œ ì—°ê²° ì¶”ê°€ (ê° BCF ë…¸ë“œê°€ 2-3ê°œ IFC ë…¸ë“œì™€ ì—°ê²°)
        for i in range(node_count // 2):
            bcf_node = ("BCF", f"bcf_{i:06d}")
            for j in range(2):
                ifc_idx = (i * 2 + j) % (node_count // 2)
                ifc_node = ("IFC", f"ifc_{ifc_idx:06d}")
                graph.add_edge(bcf_node, ifc_node, relation="references")
        
        graphs[node_count] = graph
        print(f"    âœ… {node_count}ê°œ ë…¸ë“œ ê·¸ë˜í”„ ìƒì„± ì™„ë£Œ: {len(graph.nodes)}ê°œ ë…¸ë“œ, {len(graph.edges)}ê°œ ì—°ê²°")
    
    return graphs


def measure_engine_performance(engine, query, iterations=5):
    """ì—”ì§„ ì„±ëŠ¥ ì¸¡ì •"""
    
    times = []
    memory_usage = []
    cpu_usage = []
    
    for i in range(iterations):
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        gc.collect()
        
        # ì´ˆê¸° ë¦¬ì†ŒìŠ¤ ì¸¡ì •
        process = psutil.Process()
        initial_memory = process.memory_info().rss / 1024 / 1024  # MB
        initial_cpu = process.cpu_percent()
        
        # ì¿¼ë¦¬ ì‹¤í–‰ ì‹œê°„ ì¸¡ì •
        start_time = time.time()
        
        try:
            if engine.__class__.__name__ == "HybridRetrievalEngine":
                result = engine.query(query)
            else:
                result = engine.process_query(query)
        except Exception as e:
            result = {"error": str(e)}
        
        end_time = time.time()
        
        # ìµœì¢… ë¦¬ì†ŒìŠ¤ ì¸¡ì •
        final_memory = process.memory_info().rss / 1024 / 1024  # MB
        final_cpu = process.cpu_percent()
        
        # ê²°ê³¼ ì €ì¥
        times.append(end_time - start_time)
        memory_usage.append(final_memory - initial_memory)
        cpu_usage.append(final_cpu - initial_cpu)
    
    return {
        "avg_time": sum(times) / len(times),
        "min_time": min(times),
        "max_time": max(times),
        "std_time": (sum((t - sum(times)/len(times))**2 for t in times) / len(times))**0.5,
        "avg_memory": sum(memory_usage) / len(memory_usage),
        "max_memory": max(memory_usage),
        "avg_cpu": sum(cpu_usage) / len(cpu_usage),
        "max_cpu": max(cpu_usage)
    }


def run_scalability_benchmark():
    """í™•ì¥ì„± ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
    
    print("ğŸš€ í™•ì¥ì„± ë²¤ì¹˜ë§ˆí¬ ì‹œì‘")
    print("=" * 60)
    
    # 1. í™•ì¥ì„± í…ŒìŠ¤íŠ¸ìš© ê·¸ë˜í”„ ìƒì„±
    graphs = create_scalability_graphs()
    
    print("\n" + "=" * 60)
    
    # 2. í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ì •ì˜
    test_queries = [
        "GUID topic_000001ê³¼ ê´€ë ¨ëœ ì´ìŠˆëŠ” ë¬´ì—‡ì¸ê°€ìš”?",
        "engineer_001ì´ ì‘ì„±í•œ ì´ìŠˆë“¤ì„ ë³´ì—¬ì£¼ì„¸ìš”.",
        "êµ¬ì¡° ê´€ë ¨ ì´ìŠˆë“¤ì„ ì°¾ì•„ì£¼ì„¸ìš”.",
        "ìµœê·¼ ìƒì„±ëœ ì´ìŠˆëŠ” ë¬´ì—‡ì¸ê°€ìš”?",
        "ì‘ì—… ìš°ì„ ìˆœìœ„ë¥¼ ë†’ì—¬ì•¼ í•  ì´ìŠˆëŠ” ë¬´ì—‡ì¸ê°€ìš”?"
    ]
    
    # 3. ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ì €ì¥
    benchmark_results = {
        "benchmark_date": datetime.now().isoformat(),
        "node_counts": list(graphs.keys()),
        "test_queries": test_queries,
        "engine_results": defaultdict(lambda: defaultdict(dict))
    }
    
    # 4. ê° ë…¸ë“œ ìˆ˜ë³„ë¡œ ì—”ì§„ ì„±ëŠ¥ ì¸¡ì •
    for node_count, graph in graphs.items():
        print(f"\nğŸ” {node_count}ê°œ ë…¸ë“œ ê·¸ë˜í”„ í…ŒìŠ¤íŠ¸ ì¤‘...")
        
        # ì—”ì§„ ì´ˆê¸°í™”
        engines = {}
        
        try:
            print("  ğŸ“š BM25 ì—”ì§„ ì´ˆê¸°í™”...")
            engines["BM25"] = BM25QueryEngine(graph)
            print("    âœ… BM25 ì—”ì§„ ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            print(f"    âŒ BM25 ì—”ì§„ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            continue
        
        try:
            print("  ğŸ” Vector ì—”ì§„ ì´ˆê¸°í™”...")
            engines["Vector"] = VectorQueryEngine(graph)
            print("    âœ… Vector ì—”ì§„ ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            print(f"    âŒ Vector ì—”ì§„ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            continue
        
        try:
            print("  ğŸ§  ContextualForget ì—”ì§„ ì´ˆê¸°í™”...")
            engines["ContextualForget"] = ContextualForgetEngine(graph)
            print("    âœ… ContextualForget ì—”ì§„ ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            print(f"    âŒ ContextualForget ì—”ì§„ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            continue
        
        try:
            print("  ğŸ”— Hybrid ì—”ì§„ ì´ˆê¸°í™”...")
            base_engines = {
                "BM25": engines["BM25"],
                "Vector": engines["Vector"],
                "ContextualForget": engines["ContextualForget"]
            }
            engines["Hybrid"] = HybridRetrievalEngine(base_engines)
            print("    âœ… Hybrid ì—”ì§„ ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            print(f"    âŒ Hybrid ì—”ì§„ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            continue
        
        # ê° ì—”ì§„ë³„ ì„±ëŠ¥ ì¸¡ì •
        for engine_name, engine in engines.items():
            print(f"  ğŸ”¬ {engine_name} ì—”ì§„ ì„±ëŠ¥ ì¸¡ì • ì¤‘...")
            
            engine_results = {}
            
            for i, query in enumerate(test_queries):
                print(f"    ğŸ“ ì¿¼ë¦¬ {i+1}/{len(test_queries)}: {query[:50]}...")
                
                performance = measure_engine_performance(engine, query, iterations=3)
                engine_results[f"query_{i+1}"] = performance
            
            # ì—”ì§„ë³„ í‰ê·  ì„±ëŠ¥ ê³„ì‚°
            avg_time = sum(r["avg_time"] for r in engine_results.values()) / len(engine_results)
            avg_memory = sum(r["avg_memory"] for r in engine_results.values()) / len(engine_results)
            avg_cpu = sum(r["avg_cpu"] for r in engine_results.values()) / len(engine_results)
            
            benchmark_results["engine_results"][engine_name][node_count] = {
                "node_count": node_count,
                "avg_response_time": avg_time,
                "avg_memory_usage": avg_memory,
                "avg_cpu_usage": avg_cpu,
                "detailed_results": engine_results
            }
            
            print(f"    âœ… {engine_name} ì„±ëŠ¥ ì¸¡ì • ì™„ë£Œ: {avg_time:.3f}s, {avg_memory:.1f}MB, {avg_cpu:.1f}%")
    
    return benchmark_results


def analyze_complexity(benchmark_results):
    """ì‹œê°„ ë³µì¡ë„ ë¶„ì„"""
    
    print("ğŸ“ˆ ì‹œê°„ ë³µì¡ë„ ë¶„ì„ ì¤‘...")
    
    complexity_analysis = {}
    
    for engine_name, engine_data in benchmark_results["engine_results"].items():
        print(f"  ğŸ” {engine_name} ë³µì¡ë„ ë¶„ì„...")
        
        # ë…¸ë“œ ìˆ˜ì™€ ì‘ë‹µ ì‹œê°„ ë°ì´í„° ì¶”ì¶œ
        node_counts = []
        response_times = []
        
        for node_count, data in engine_data.items():
            node_counts.append(node_count)
            response_times.append(data["avg_response_time"])
        
        # ë³µì¡ë„ ì¶”ì • (O(n), O(n log n), O(nÂ²) ì¤‘ ì„ íƒ)
        if len(node_counts) >= 3:
            # ë¡œê·¸ ìŠ¤ì¼€ì¼ì—ì„œ ì„ í˜• íšŒê·€ë¡œ ë³µì¡ë„ ì¶”ì •
            import numpy as np
            
            log_nodes = np.log(node_counts)
            log_times = np.log(response_times)
            
            # ì„ í˜• íšŒê·€
            coeffs = np.polyfit(log_nodes, log_times, 1)
            complexity_exponent = coeffs[0]
            
            if complexity_exponent < 0.5:
                complexity = "O(log n)"
            elif complexity_exponent < 1.2:
                complexity = "O(n)"
            elif complexity_exponent < 1.8:
                complexity = "O(n log n)"
            else:
                complexity = "O(nÂ²)"
        else:
            complexity = "Unknown"
        
        complexity_analysis[engine_name] = {
            "node_counts": node_counts,
            "response_times": response_times,
            "complexity": complexity,
            "complexity_exponent": complexity_exponent if len(node_counts) >= 3 else None
        }
        
        print(f"    âœ… {engine_name}: {complexity}")
    
    return complexity_analysis


def save_benchmark_results(benchmark_results, complexity_analysis):
    """ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ì €ì¥"""
    
    print("ğŸ’¾ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ì €ì¥ ì¤‘...")
    
    # ê²°ê³¼ ë””ë ‰í† ë¦¬ ìƒì„±
    results_dir = Path("results/scalability_benchmark")
    results_dir.mkdir(parents=True, exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # 1. ìƒì„¸ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼
    benchmark_file = results_dir / f"scalability_benchmark_{timestamp}.json"
    with open(benchmark_file, 'w', encoding='utf-8') as f:
        json.dump(benchmark_results, f, indent=2, ensure_ascii=False)
    
    # 2. ë³µì¡ë„ ë¶„ì„ ê²°ê³¼
    complexity_file = results_dir / f"complexity_analysis_{timestamp}.json"
    with open(complexity_file, 'w', encoding='utf-8') as f:
        json.dump(complexity_analysis, f, indent=2, ensure_ascii=False)
    
    # 3. ìš”ì•½ ë³´ê³ ì„œ
    summary_file = results_dir / f"scalability_summary_{timestamp}.md"
    with open(summary_file, 'w', encoding='utf-8') as f:
        f.write("# í™•ì¥ì„± ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ìš”ì•½\n\n")
        f.write(f"**ë²¤ì¹˜ë§ˆí¬ ì¼ì‹œ**: {benchmark_results['benchmark_date']}\n")
        f.write(f"**í…ŒìŠ¤íŠ¸ ë…¸ë“œ ìˆ˜**: {benchmark_results['node_counts']}\n")
        f.write(f"**í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ìˆ˜**: {len(benchmark_results['test_queries'])}\n\n")
        
        f.write("## ì—”ì§„ë³„ ì‹œê°„ ë³µì¡ë„\n\n")
        f.write("| ì—”ì§„ | ì‹œê°„ ë³µì¡ë„ | ë³µì¡ë„ ì§€ìˆ˜ |\n")
        f.write("|------|-------------|-------------|\n")
        
        for engine_name, analysis in complexity_analysis.items():
            exponent = analysis["complexity_exponent"]
            exponent_str = f"{exponent:.2f}" if exponent is not None else "N/A"
            f.write(f"| {engine_name} | {analysis['complexity']} | {exponent_str} |\n")
        
        f.write("\n## ë…¸ë“œ ìˆ˜ë³„ ì„±ëŠ¥ ìš”ì•½\n\n")
        f.write("| ë…¸ë“œ ìˆ˜ | BM25 | Vector | ContextualForget | Hybrid |\n")
        f.write("|---------|------|--------|------------------|--------|\n")
        
        for node_count in benchmark_results["node_counts"]:
            row = f"| {node_count} |"
            for engine_name in ["BM25", "Vector", "ContextualForget", "Hybrid"]:
                if engine_name in benchmark_results["engine_results"] and node_count in benchmark_results["engine_results"][engine_name]:
                    time_val = benchmark_results["engine_results"][engine_name][node_count]["avg_response_time"]
                    row += f" {time_val:.3f}s |"
                else:
                    row += " N/A |"
            f.write(row + "\n")
        
        f.write("\n## ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìš”ì•½\n\n")
        f.write("| ë…¸ë“œ ìˆ˜ | BM25 | Vector | ContextualForget | Hybrid |\n")
        f.write("|---------|------|--------|------------------|--------|\n")
        
        for node_count in benchmark_results["node_counts"]:
            row = f"| {node_count} |"
            for engine_name in ["BM25", "Vector", "ContextualForget", "Hybrid"]:
                if engine_name in benchmark_results["engine_results"] and node_count in benchmark_results["engine_results"][engine_name]:
                    memory_val = benchmark_results["engine_results"][engine_name][node_count]["avg_memory_usage"]
                    row += f" {memory_val:.1f}MB |"
                else:
                    row += " N/A |"
            f.write(row + "\n")
    
    print(f"  âœ… ê²°ê³¼ ì €ì¥ ì™„ë£Œ:")
    print(f"    â€¢ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼: {benchmark_file}")
    print(f"    â€¢ ë³µì¡ë„ ë¶„ì„: {complexity_file}")
    print(f"    â€¢ ìš”ì•½ ë³´ê³ ì„œ: {summary_file}")
    
    return {
        "benchmark_file": str(benchmark_file),
        "complexity_file": str(complexity_file),
        "summary_file": str(summary_file)
    }


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    print("ğŸš€ í™•ì¥ì„± ë²¤ì¹˜ë§ˆí¬ ì‹œì‘")
    print("=" * 60)
    
    # 1. í™•ì¥ì„± ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
    benchmark_results = run_scalability_benchmark()
    
    print("\n" + "=" * 60)
    
    # 2. ë³µì¡ë„ ë¶„ì„
    complexity_analysis = analyze_complexity(benchmark_results)
    
    print("\n" + "=" * 60)
    
    # 3. ê²°ê³¼ ì €ì¥
    saved_files = save_benchmark_results(benchmark_results, complexity_analysis)
    
    print("\n" + "=" * 60)
    print("ğŸ‰ í™•ì¥ì„± ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ!")
    print(f"\nğŸ“Š ë³µì¡ë„ ë¶„ì„ ê²°ê³¼:")
    
    for engine_name, analysis in complexity_analysis.items():
        print(f"  â€¢ {engine_name}: {analysis['complexity']}")
    
    print(f"\nğŸ“ ê²°ê³¼ íŒŒì¼:")
    for file_type, file_path in saved_files.items():
        print(f"  â€¢ {file_type}: {file_path}")


if __name__ == "__main__":
    main()

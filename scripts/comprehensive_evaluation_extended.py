#!/usr/bin/env python3
"""
í™•ì¥ ì¢…í•© í‰ê°€ ìŠ¤í¬ë¦½íŠ¸
600ê°œ ì§ˆì˜ë¡œ 4ê°œ ì—”ì§„ì˜ ì„±ëŠ¥ì„ ì—”ì§„ë³„/ì¿¼ë¦¬íƒ€ì…ë³„/í”„ë¡œì íŠ¸íƒ€ì…ë³„ë¡œ í‰ê°€
"""

import sys
import json
import time
import psutil
from pathlib import Path
from datetime import datetime
from collections import defaultdict

# Add src to path for imports
sys.path.insert(0, str(Path(__file__).parent.parent / 'src'))

from contextualforget.core.utils import read_jsonl
from contextualforget.baselines.bm25_engine import BM25QueryEngine
from contextualforget.baselines.vector_engine import VectorQueryEngine
from contextualforget.query.contextual_forget_engine import ContextualForgetEngine
from contextualforget.query.adaptive_retrieval import HybridRetrievalEngine
from contextualforget.data.build_graph import build_graph_from_files
sys.path.insert(0, str(Path(__file__).parent.parent))
from eval.metrics_v2 import EvaluationMetricsV2


def load_integrated_graph():
    """í†µí•© ê·¸ë˜í”„ ë¡œë“œ"""
    
    print("ğŸ“‚ í†µí•© ê·¸ë˜í”„ ë¡œë“œ ì¤‘...")
    
    # ê¸°ì¡´ ê·¸ë˜í”„ íŒŒì¼ í™•ì¸
    graph_file = Path("data/processed/graph_with_connections.pkl")
    if graph_file.exists():
        print("  ğŸ“ ê¸°ì¡´ ê·¸ë˜í”„ íŒŒì¼ ë¡œë“œ ì¤‘...")
        import pickle
        with open(graph_file, 'rb') as f:
            graph = pickle.load(f)
        print(f"  âœ… ê¸°ì¡´ ê·¸ë˜í”„ ë¡œë“œ ì™„ë£Œ: {len(graph.nodes)}ê°œ ë…¸ë“œ, {len(graph.edges)}ê°œ ì—°ê²°")
        return graph
    
    # í†µí•© ë°ì´í„°ì…‹ ê²½ë¡œ
    bcf_file = Path("data/processed/integrated_dataset/integrated_bcf_data.jsonl")
    
    if not bcf_file.exists():
        print("âŒ í†µí•© BCF ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None
    
    # ê°„ë‹¨í•œ ê·¸ë˜í”„ êµ¬ì¶• (BCF ë°ì´í„°ë§Œ ì‚¬ìš©)
    import networkx as nx
    graph = nx.DiGraph()
    
    # BCF ë°ì´í„° ë¡œë“œ
    bcf_data = list(read_jsonl(str(bcf_file)))
    
    # BCF ë…¸ë“œ ì¶”ê°€
    for issue in bcf_data:
        node_id = ("BCF", issue.get("topic_id", f"bcf_{len(graph.nodes)}"))
        graph.add_node(node_id, **issue)
    
    print(f"  âœ… ê·¸ë˜í”„ êµ¬ì¶• ì™„ë£Œ: {len(graph.nodes)}ê°œ ë…¸ë“œ, {len(graph.edges)}ê°œ ì—°ê²°")
    
    return graph


def load_gold_standard():
    """í™•ì¥ Gold Standard ë¡œë“œ"""
    
    print("ğŸ“‹ í™•ì¥ Gold Standard ë¡œë“œ ì¤‘...")
    
    gold_standard_file = Path("eval/gold_standard_comprehensive.jsonl")
    if not gold_standard_file.exists():
        print("âŒ í™•ì¥ Gold Standardë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None
    
    queries = list(read_jsonl(str(gold_standard_file)))
    
    # ì¿¼ë¦¬ íƒ€ì…ë³„ ë¶„ë¥˜
    query_types = defaultdict(list)
    for query in queries:
        query_types[query["query_type"]].append(query)
    
    print(f"  âœ… Gold Standard ë¡œë“œ ì™„ë£Œ: {len(queries)}ê°œ ì§ˆì˜")
    print(f"  ğŸ“Š ì¿¼ë¦¬ íƒ€ì…ë³„ ë¶„í¬:")
    for qtype, qlist in query_types.items():
        print(f"    â€¢ {qtype}: {len(qlist)}ê°œ")
    
    return queries, query_types


def initialize_engines(graph):
    """4ê°œ ì—”ì§„ ì´ˆê¸°í™”"""
    
    print("ğŸš€ ì—”ì§„ ì´ˆê¸°í™” ì¤‘...")
    
    engines = {}
    
    try:
        # BM25 ì—”ì§„
        print("  ğŸ“š BM25 ì—”ì§„ ì´ˆê¸°í™”...")
        engines["BM25"] = BM25QueryEngine(graph)
        print("    âœ… BM25 ì—”ì§„ ì´ˆê¸°í™” ì™„ë£Œ")
        
        # Vector ì—”ì§„
        print("  ğŸ” Vector ì—”ì§„ ì´ˆê¸°í™”...")
        engines["Vector"] = VectorQueryEngine(graph)
        print("    âœ… Vector ì—”ì§„ ì´ˆê¸°í™” ì™„ë£Œ")
        
        # ContextualForget ì—”ì§„
        print("  ğŸ§  ContextualForget ì—”ì§„ ì´ˆê¸°í™”...")
        engines["ContextualForget"] = ContextualForgetEngine(graph)
        print("    âœ… ContextualForget ì—”ì§„ ì´ˆê¸°í™” ì™„ë£Œ")
        
        # Hybrid ì—”ì§„
        print("  ğŸ”— Hybrid ì—”ì§„ ì´ˆê¸°í™”...")
        base_engines = {
            "BM25": engines["BM25"],
            "Vector": engines["Vector"],
            "ContextualForget": engines["ContextualForget"]
        }
        engines["Hybrid"] = HybridRetrievalEngine(base_engines)
        print("    âœ… Hybrid ì—”ì§„ ì´ˆê¸°í™” ì™„ë£Œ")
        
    except Exception as e:
        print(f"    âŒ ì—”ì§„ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return None
    
    print(f"  ğŸ‰ ëª¨ë“  ì—”ì§„ ì´ˆê¸°í™” ì™„ë£Œ: {list(engines.keys())}")
    return engines


def measure_detailed_performance(engine, query, query_type):
    """ìƒì„¸ ì„±ëŠ¥ ì¸¡ì •"""
    
    start_time = time.time()
    start_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB
    start_cpu = psutil.Process().cpu_percent()
    
    # ì¿¼ë¦¬ ì‹¤í–‰
    try:
        if engine.__class__.__name__ == "HybridRetrievalEngine":
            result = engine.query(query["question"])
        else:
            result = engine.process_query(query["question"])
    except Exception as e:
        result = {"error": str(e)}
    
    end_time = time.time()
    end_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB
    end_cpu = psutil.Process().cpu_percent()
    
    # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚°
    response_time = end_time - start_time
    memory_delta = end_memory - start_memory
    cpu_delta = end_cpu - start_cpu
    
    # ìƒì„¸ ì‘ë‹µ ì‹œê°„ ë¶„í•´
    detailed_timing = {
        "query_parsing": response_time * 0.1,  # ì¶”ì •ê°’
        "index_search": response_time * 0.5,   # ì¶”ì •ê°’
        "forgetting_computation": response_time * 0.2 if "ContextualForget" in engine.__class__.__name__ else 0,
        "result_ranking": response_time * 0.2,  # ì¶”ì •ê°’
        "total": response_time
    }
    
    return {
        "result": result,
        "response_time": response_time,
        "memory_delta_mb": memory_delta,
        "cpu_delta_percent": cpu_delta,
        "detailed_timing": detailed_timing
    }


def run_comprehensive_evaluation(engines, queries, query_types):
    """ì¢…í•© í‰ê°€ ì‹¤í–‰"""
    
    print("ğŸ”¬ ì¢…í•© í‰ê°€ ì‹¤í–‰ ì¤‘...")
    print(f"  ğŸ“Š í‰ê°€ ëŒ€ìƒ: {len(queries)}ê°œ ì§ˆì˜, {len(engines)}ê°œ ì—”ì§„")
    
    results = {
        "evaluation_date": datetime.now().isoformat(),
        "total_queries": len(queries),
        "engines": list(engines.keys()),
        "query_types": list(query_types.keys()),
        "engine_results": defaultdict(lambda: defaultdict(list)),
        "query_type_results": defaultdict(lambda: defaultdict(list)),
        "detailed_metrics": defaultdict(dict)
    }
    
    # ê° ì—”ì§„ë³„ í‰ê°€
    for engine_name, engine in engines.items():
        print(f"\n  ğŸ” {engine_name} ì—”ì§„ í‰ê°€ ì¤‘...")
        
        engine_results = []
        for i, query in enumerate(queries):
            if i % 100 == 0:
                print(f"    ğŸ“ ì§„í–‰ë¥ : {i}/{len(queries)} ({i/len(queries)*100:.1f}%)")
            
            # ìƒì„¸ ì„±ëŠ¥ ì¸¡ì •
            performance = measure_detailed_performance(engine, query, query["query_type"])
            
            # ë©”íŠ¸ë¦­ ê³„ì‚°
            metrics = EvaluationMetricsV2()
            success = metrics.is_success(performance["result"], {})
            confidence = performance["result"].get("confidence", 0.0)
            
            result_entry = {
                "query_id": query["id"],
                "query_type": query["query_type"],
                "question": query["question"],
                "success": success,
                "confidence": confidence,
                "response_time": performance["response_time"],
                "memory_delta_mb": performance["memory_delta_mb"],
                "cpu_delta_percent": performance["cpu_delta_percent"],
                "detailed_timing": performance["detailed_timing"],
                "result": performance["result"]
            }
            
            engine_results.append(result_entry)
            results["engine_results"][engine_name][query["query_type"]].append(result_entry)
            results["query_type_results"][query["query_type"]][engine_name].append(result_entry)
        
        # ì—”ì§„ë³„ í†µê³„
        total_success = sum(1 for r in engine_results if r["success"])
        avg_confidence = sum(r["confidence"] for r in engine_results) / len(engine_results)
        avg_response_time = sum(r["response_time"] for r in engine_results) / len(engine_results)
        avg_memory = sum(r["memory_delta_mb"] for r in engine_results) / len(engine_results)
        
        results["detailed_metrics"][engine_name] = {
            "total_queries": len(engine_results),
            "success_count": total_success,
            "success_rate": total_success / len(engine_results),
            "avg_confidence": avg_confidence,
            "avg_response_time": avg_response_time,
            "avg_memory_delta_mb": avg_memory,
            "query_type_breakdown": {}
        }
        
        # ì¿¼ë¦¬ íƒ€ì…ë³„ í†µê³„
        for qtype in query_types.keys():
            type_results = [r for r in engine_results if r["query_type"] == qtype]
            if type_results:
                type_success = sum(1 for r in type_results if r["success"])
                type_confidence = sum(r["confidence"] for r in type_results) / len(type_results)
                type_time = sum(r["response_time"] for r in type_results) / len(type_results)
                
                results["detailed_metrics"][engine_name]["query_type_breakdown"][qtype] = {
                    "count": len(type_results),
                    "success_rate": type_success / len(type_results),
                    "avg_confidence": type_confidence,
                    "avg_response_time": type_time
                }
        
        print(f"    âœ… {engine_name} í‰ê°€ ì™„ë£Œ: {total_success}/{len(engine_results)} ì„±ê³µ ({total_success/len(engine_results)*100:.1f}%)")
    
    return results


def save_evaluation_results(results):
    """í‰ê°€ ê²°ê³¼ ì €ì¥"""
    
    print("ğŸ’¾ í‰ê°€ ê²°ê³¼ ì €ì¥ ì¤‘...")
    
    # ê²°ê³¼ ë””ë ‰í† ë¦¬ ìƒì„±
    results_dir = Path("results/evaluation_extended")
    results_dir.mkdir(parents=True, exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # 1. ì¢…í•© ê²°ê³¼ JSON
    comprehensive_file = results_dir / f"evaluation_extended_{timestamp}_comprehensive.json"
    with open(comprehensive_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    # 2. ìƒì„¸ ê²°ê³¼ CSV
    detailed_file = results_dir / f"evaluation_extended_{timestamp}_detailed.csv"
    with open(detailed_file, 'w', encoding='utf-8') as f:
        f.write("engine,query_id,query_type,success,confidence,response_time,memory_delta_mb,cpu_delta_percent\n")
        
        for engine_name, engine_data in results["engine_results"].items():
            for qtype, query_results in engine_data.items():
                for result in query_results:
                    f.write(f"{engine_name},{result['query_id']},{result['query_type']},{result['success']},{result['confidence']},{result['response_time']},{result['memory_delta_mb']},{result['cpu_delta_percent']}\n")
    
    # 3. ìš”ì•½ ë³´ê³ ì„œ
    summary_file = results_dir / f"evaluation_extended_{timestamp}_summary.md"
    with open(summary_file, 'w', encoding='utf-8') as f:
        f.write("# í™•ì¥ ì¢…í•© í‰ê°€ ê²°ê³¼ ìš”ì•½\n\n")
        f.write(f"**í‰ê°€ ì¼ì‹œ**: {results['evaluation_date']}\n")
        f.write(f"**ì´ ì§ˆì˜ ìˆ˜**: {results['total_queries']}\n")
        f.write(f"**í‰ê°€ ì—”ì§„**: {', '.join(results['engines'])}\n")
        f.write(f"**ì¿¼ë¦¬ íƒ€ì…**: {', '.join(results['query_types'])}\n\n")
        
        f.write("## ì—”ì§„ë³„ ì„±ëŠ¥ ìš”ì•½\n\n")
        f.write("| ì—”ì§„ | ì„±ê³µë¥  | í‰ê·  ì‹ ë¢°ë„ | í‰ê·  ì‘ë‹µì‹œê°„ | í‰ê·  ë©”ëª¨ë¦¬ |\n")
        f.write("|------|--------|-------------|---------------|-------------|\n")
        
        for engine_name, metrics in results["detailed_metrics"].items():
            f.write(f"| {engine_name} | {metrics['success_rate']:.1%} | {metrics['avg_confidence']:.3f} | {metrics['avg_response_time']:.3f}s | {metrics['avg_memory_delta_mb']:.1f}MB |\n")
        
        f.write("\n## ì¿¼ë¦¬ íƒ€ì…ë³„ ì„±ëŠ¥ ìš”ì•½\n\n")
        for qtype in results["query_types"]:
            f.write(f"### {qtype}\n\n")
            f.write("| ì—”ì§„ | ì„±ê³µë¥  | í‰ê·  ì‹ ë¢°ë„ | í‰ê·  ì‘ë‹µì‹œê°„ |\n")
            f.write("|------|--------|-------------|---------------|\n")
            
            for engine_name in results["engines"]:
                if qtype in results["detailed_metrics"][engine_name]["query_type_breakdown"]:
                    breakdown = results["detailed_metrics"][engine_name]["query_type_breakdown"][qtype]
                    f.write(f"| {engine_name} | {breakdown['success_rate']:.1%} | {breakdown['avg_confidence']:.3f} | {breakdown['avg_response_time']:.3f}s |\n")
            f.write("\n")
    
    print(f"  âœ… ê²°ê³¼ ì €ì¥ ì™„ë£Œ:")
    print(f"    â€¢ ì¢…í•© ê²°ê³¼: {comprehensive_file}")
    print(f"    â€¢ ìƒì„¸ ê²°ê³¼: {detailed_file}")
    print(f"    â€¢ ìš”ì•½ ë³´ê³ ì„œ: {summary_file}")
    
    return {
        "comprehensive_file": str(comprehensive_file),
        "detailed_file": str(detailed_file),
        "summary_file": str(summary_file)
    }


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    print("ğŸš€ í™•ì¥ ì¢…í•© í‰ê°€ ì‹œì‘")
    print("=" * 60)
    
    # 1. í†µí•© ê·¸ë˜í”„ ë¡œë“œ
    graph = load_integrated_graph()
    if not graph:
        return
    
    print("\n" + "=" * 60)
    
    # 2. í™•ì¥ Gold Standard ë¡œë“œ
    queries, query_types = load_gold_standard()
    if not queries:
        return
    
    print("\n" + "=" * 60)
    
    # 3. ì—”ì§„ ì´ˆê¸°í™”
    engines = initialize_engines(graph)
    if not engines:
        return
    
    print("\n" + "=" * 60)
    
    # 4. ì¢…í•© í‰ê°€ ì‹¤í–‰
    results = run_comprehensive_evaluation(engines, queries, query_types)
    
    print("\n" + "=" * 60)
    
    # 5. ê²°ê³¼ ì €ì¥
    saved_files = save_evaluation_results(results)
    
    print("\n" + "=" * 60)
    print("ğŸ‰ í™•ì¥ ì¢…í•© í‰ê°€ ì™„ë£Œ!")
    print(f"\nğŸ“Š í‰ê°€ ê²°ê³¼ ìš”ì•½:")
    
    for engine_name, metrics in results["detailed_metrics"].items():
        print(f"  â€¢ {engine_name}: {metrics['success_rate']:.1%} ì„±ê³µë¥ , {metrics['avg_confidence']:.3f} ì‹ ë¢°ë„, {metrics['avg_response_time']:.3f}s ì‘ë‹µì‹œê°„")
    
    print(f"\nğŸ“ ê²°ê³¼ íŒŒì¼:")
    for file_type, file_path in saved_files.items():
        print(f"  â€¢ {file_type}: {file_path}")


if __name__ == "__main__":
    main()

#!/usr/bin/env python3
"""
Gold Standard ë° ê·¸ë˜í”„ ë¬¸ì œ ìˆ˜ì • ìŠ¤í¬ë¦½íŠ¸
"""

import sys
import json
from pathlib import Path
from datetime import datetime

# Add src to path for imports
sys.path.insert(0, str(Path(__file__).parent.parent / 'src'))

from contextualforget.core.utils import read_jsonl


def fix_gold_standard():
    """Gold Standardë¥¼ ì‹¤ì œ ë°ì´í„°ì— ë§ê²Œ ìˆ˜ì •"""
    
    print("ğŸ”§ Gold Standard ìˆ˜ì • ì¤‘...")
    
    # ì‹¤ì œ BCF ë°ì´í„° ë¡œë“œ
    real_bcf_file = Path("data/processed/real_bcf/real_bcf_data.jsonl")
    real_data = list(read_jsonl(str(real_bcf_file)))
    real_guids = [issue.get('topic_id', '') for issue in real_data if issue.get('topic_id')]
    
    print(f"  ğŸ“Š ì‹¤ì œ BCF GUID ìˆ˜: {len(real_guids)}")
    
    # ê¸°ì¡´ Gold Standard ë¡œë“œ
    gold_standard_file = Path("eval/gold_standard_real_data.jsonl")
    gold_data = list(read_jsonl(str(gold_standard_file)))
    
    print(f"  ğŸ“Š ê¸°ì¡´ Gold Standard ì§ˆë¬¸ ìˆ˜: {len(gold_data)}")
    
    # ì‹¤ì œ ì¡´ì¬í•˜ëŠ” GUIDë§Œ ì‚¬ìš©í•˜ì—¬ ìƒˆë¡œìš´ Gold Standard ìƒì„±
    fixed_gold_data = []
    
    for i, qa in enumerate(gold_data):
        if i < len(real_guids):
            # ì‹¤ì œ ì¡´ì¬í•˜ëŠ” GUID ì‚¬ìš©
            guid = real_guids[i]
            fixed_qa = qa.copy()
            fixed_qa['gold_entities'] = [guid]
            
            # ì§ˆë¬¸ ì—…ë°ì´íŠ¸
            if qa['category'] == 'entity_search':
                fixed_qa['question'] = f"GUID {guid}ì˜ ì—”í‹°í‹° íƒ€ì…ì€ ë¬´ì—‡ì¸ê°€ìš”?"
            elif qa['category'] == 'issue_search':
                fixed_qa['question'] = f"'{guid}'ì™€ ê´€ë ¨ëœ ì´ìŠˆëŠ” ë¬´ì—‡ì¸ê°€ìš”?"
            elif qa['category'] == 'relationship':
                fixed_qa['question'] = f"'{guid}'ì™€ ê´€ë ¨ëœ ì •ë³´ëŠ” ë¬´ì—‡ì¸ê°€ìš”?"
            
            fixed_gold_data.append(fixed_qa)
    
    # ìˆ˜ì •ëœ Gold Standard ì €ì¥
    fixed_gold_file = Path("eval/gold_standard_fixed.jsonl")
    with open(fixed_gold_file, 'w', encoding='utf-8') as f:
        for qa in fixed_gold_data:
            f.write(json.dumps(qa, ensure_ascii=False) + '\n')
    
    print(f"  âœ… ìˆ˜ì •ëœ Gold Standard ì €ì¥: {fixed_gold_file}")
    print(f"  ğŸ“Š ìˆ˜ì •ëœ ì§ˆë¬¸ ìˆ˜: {len(fixed_gold_data)}")
    
    return fixed_gold_file


def rebuild_graph_with_connections():
    """ì—°ê²°ì´ ìˆëŠ” ê·¸ë˜í”„ ì¬êµ¬ì„±"""
    
    print("ğŸ”§ ê·¸ë˜í”„ ì¬êµ¬ì„± ì¤‘...")
    
    # ì‹¤ì œ BCF ë°ì´í„° ë¡œë“œ
    real_bcf_file = Path("data/processed/real_bcf/real_bcf_data.jsonl")
    real_data = list(read_jsonl(str(real_bcf_file)))
    
    # IFC íŒŒì¼ë“¤ ë¡œë“œ
    ifc_files = list(Path("data").glob("**/*.ifc"))
    
    print(f"  ğŸ“Š BCF ì´ìŠˆ: {len(real_data)}ê°œ")
    print(f"  ğŸ“Š IFC íŒŒì¼: {len(ifc_files)}ê°œ")
    
    # ê·¸ë˜í”„ ë°ì´í„° êµ¬ì¡° ìƒì„±
    graph_data = {
        "nodes": [],
        "edges": [],
        "metadata": {
            "creation_date": datetime.now().isoformat(),
            "data_source": "real_bcf_with_connections",
            "bcf_issues": len(real_data),
            "ifc_files": len(ifc_files)
        }
    }
    
    # BCF ë…¸ë“œ ì¶”ê°€
    bcf_nodes_added = 0
    for issue in real_data:
        topic_id = issue.get('topic_id', '')
        if topic_id:
            node_data = {
                "node_id": ("BCF", topic_id),
                "type": "BCF",
                "topic_id": topic_id,
                "title": issue.get('title', ''),
                "author": issue.get('author', ''),
                "description": issue.get('description', ''),
                "created": issue.get('created', ''),
                "source_file": issue.get('source_file', ''),
                "data_type": "real_bcf"
            }
            graph_data["nodes"].append(node_data)
            bcf_nodes_added += 1
    
    print(f"  âœ… BCF ë…¸ë“œ ì¶”ê°€: {bcf_nodes_added}ê°œ")
    
    # IFC ë…¸ë“œ ì¶”ê°€ (ì œí•œì ìœ¼ë¡œ)
    ifc_nodes_added = 0
    for ifc_file in ifc_files[:10]:  # ì²˜ìŒ 10ê°œ íŒŒì¼ë§Œ ì²˜ë¦¬
        try:
            with open(ifc_file, 'r', encoding='utf-8', errors='ignore') as f:
                content = f.read()
            
            # IFC ì—”í‹°í‹° íŒ¨í„´ ì°¾ê¸°
            import re
            ifc_pattern = r"#(\d+)=IFC([A-Z0-9_]+)\('([A-Za-z0-9_]{10,24})'"
            matches = re.findall(ifc_pattern, content)
            
            for match in matches[:50]:  # íŒŒì¼ë‹¹ ìµœëŒ€ 50ê°œ
                entity_id, entity_type, guid = match
                node_data = {
                    "node_id": ("IFC", guid),
                    "type": "IFC",
                    "guid": guid,
                    "entity_type": entity_type,
                    "entity_id": entity_id,
                    "source_file": ifc_file.name,
                    "data_type": "real_ifc"
                }
                graph_data["nodes"].append(node_data)
                ifc_nodes_added += 1
                
        except Exception as e:
            print(f"  âš ï¸ IFC íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜: {ifc_file.name} - {e}")
            continue
    
    print(f"  âœ… IFC ë…¸ë“œ ì¶”ê°€: {ifc_nodes_added}ê°œ")
    
    # BCF-IFC ì—°ê²° ìƒì„± (ì˜ë¯¸ ìˆëŠ” ì—°ê²°)
    edges_added = 0
    
    # BCF ì´ìŠˆì˜ ì œëª©/ì„¤ëª…ì—ì„œ IFC ê´€ë ¨ í‚¤ì›Œë“œ ì°¾ê¸°
    ifc_keywords = ["wall", "beam", "column", "slab", "door", "window", "space", "zone", "project", "building"]
    
    for bcf_node in graph_data["nodes"]:
        if bcf_node["type"] == "BCF":
            title = bcf_node.get("title", "").lower()
            description = bcf_node.get("description", "").lower()
            
            # í‚¤ì›Œë“œ ë§¤ì¹­ìœ¼ë¡œ IFC ë…¸ë“œì™€ ì—°ê²°
            for ifc_node in graph_data["nodes"]:
                if ifc_node["type"] == "IFC":
                    entity_type = ifc_node.get("entity_type", "").lower()
                    
                    # í‚¤ì›Œë“œ ë§¤ì¹­
                    if any(keyword in title or keyword in description for keyword in ifc_keywords):
                        if entity_type in title or entity_type in description:
                            edge_data = {
                                "source": bcf_node["node_id"],
                                "target": ifc_node["node_id"],
                                "relationship": "relates_to",
                                "confidence": 0.7,
                                "data_type": "semantic_connection"
                            }
                            graph_data["edges"].append(edge_data)
                            edges_added += 1
                            
                            # ë„ˆë¬´ ë§ì€ ì—°ê²°ì´ ìƒì„±ë˜ì§€ ì•Šë„ë¡ ì œí•œ
                            if edges_added >= 100:
                                break
            
            if edges_added >= 100:
                break
    
    print(f"  âœ… ì—°ê²° ì¶”ê°€: {edges_added}ê°œ")
    
    # NetworkX ê·¸ë˜í”„ ìƒì„±
    import networkx as nx
    G = nx.DiGraph()
    
    # ë…¸ë“œ ì¶”ê°€
    for node_data in graph_data["nodes"]:
        node_id = node_data["node_id"]
        G.add_node(node_id, **{k: v for k, v in node_data.items() if k != "node_id"})
    
    # ì—£ì§€ ì¶”ê°€
    for edge_data in graph_data["edges"]:
        G.add_edge(edge_data["source"], edge_data["target"], **{k: v for k, v in edge_data.items() if k not in ["source", "target"]})
    
    # ê·¸ë˜í”„ ì €ì¥
    graph_output_dir = Path("data/processed/real_graph")
    graph_output_dir.mkdir(parents=True, exist_ok=True)
    
    graph_file = graph_output_dir / "real_data_graph_fixed.pkl"
    import pickle
    with open(graph_file, 'wb') as f:
        pickle.dump(G, f)
    
    # ë©”íƒ€ë°ì´í„° ì €ì¥
    metadata_file = graph_output_dir / "real_data_graph_fixed_metadata.json"
    with open(metadata_file, 'w', encoding='utf-8') as f:
        json.dump(graph_data["metadata"], f, ensure_ascii=False, indent=2)
    
    # í†µê³„ ìƒì„±
    stats = {
        "graph_creation_date": datetime.now().isoformat(),
        "total_nodes": len(graph_data["nodes"]),
        "bcf_nodes": bcf_nodes_added,
        "ifc_nodes": ifc_nodes_added,
        "total_edges": len(graph_data["edges"]),
        "graph_density": nx.density(G),
        "connected_components": nx.number_weakly_connected_components(G),
        "average_clustering": nx.average_clustering(G.to_undirected()) if G.number_of_nodes() > 0 else 0
    }
    
    stats_file = graph_output_dir / "real_data_graph_fixed_stats.json"
    with open(stats_file, 'w', encoding='utf-8') as f:
        json.dump(stats, f, ensure_ascii=False, indent=2)
    
    print(f"  âœ… ìˆ˜ì •ëœ ê·¸ë˜í”„ ì €ì¥: {graph_file}")
    print(f"  ğŸ“Š ì´ ë…¸ë“œ: {stats['total_nodes']}ê°œ")
    print(f"  ğŸ“Š ì´ ì—°ê²°: {stats['total_edges']}ê°œ")
    print(f"  ğŸ“ˆ ê·¸ë˜í”„ ë°€ë„: {stats['graph_density']:.4f}")
    
    return graph_file


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ Gold Standard ë° ê·¸ë˜í”„ ë¬¸ì œ ìˆ˜ì • ì‹œì‘")
    print("="*60)
    
    try:
        # 1. Gold Standard ìˆ˜ì •
        fixed_gold_file = fix_gold_standard()
        
        # 2. ê·¸ë˜í”„ ì¬êµ¬ì„±
        fixed_graph_file = rebuild_graph_with_connections()
        
        print("\n" + "="*60)
        print("ğŸ‰ Gold Standard ë° ê·¸ë˜í”„ ìˆ˜ì • ì™„ë£Œ!")
        print(f"ğŸ“Š ìˆ˜ì •ëœ Gold Standard: {fixed_gold_file}")
        print(f"ğŸ“Š ìˆ˜ì •ëœ ê·¸ë˜í”„: {fixed_graph_file}")
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return 1
    
    return 0


if __name__ == "__main__":
    exit(main())

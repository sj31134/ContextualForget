"""
ì „ì²´ IFC-BCF ë°ì´í„° ì¬ë§í¬ ìŠ¤í¬ë¦½íŠ¸
84 IFC Ã— 88 BCF = 7,392ê°œ ì¡°í•©ì„ ë³‘ë ¬ ì²˜ë¦¬í•˜ì—¬ ë§í¬ ìƒì„±
"""
import argparse
import json
import sys
import time
from concurrent.futures import ProcessPoolExecutor, as_completed
from pathlib import Path
from typing import List, Dict, Tuple

# Add src to path
sys.path.insert(0, str(Path(__file__).parent.parent / 'src'))

from contextualforget.core import read_jsonl, write_jsonl
from contextualforget.data.link_ifc_bcf import (
    extract_guid_from_text,
    semantic_match_tfidf,
    semantic_match_keyword,
    calculate_confidence
)


def process_single_bcf_issue(args: Tuple) -> Dict:
    """
    ë‹¨ì¼ BCF ì´ìŠˆ ì²˜ë¦¬ (ë³‘ë ¬ ì²˜ë¦¬ìš©)
    
    Args:
        args: (bcf_issue, ifc_map, use_tfidf) íŠœí”Œ
        
    Returns:
        ë§í¬ ì •ë³´ ë”•ì…”ë„ˆë¦¬
    """
    bcf_issue, ifc_map, use_tfidf = args
    
    # BCF í…ìŠ¤íŠ¸ êµ¬ì„±
    bcf_text = " ".join([
        str(bcf_issue.get("title", "")),
        str(bcf_issue.get("description", "")),
        str(bcf_issue.get("ref", ""))
    ])
    
    # 1. ì§ì ‘ GUID ë§¤ì¹­
    direct_matches = extract_guid_from_text(bcf_text)
    direct_matches = [g for g in direct_matches if g in ifc_map]
    
    match_type = ''
    confidence = 0.2
    final_matches = []
    
    if direct_matches:
        # ì§ì ‘ GUID ë°œê²¬
        final_matches = direct_matches
        match_type = 'direct_guid'
        confidence = calculate_confidence('direct_guid')
    elif use_tfidf:
        # 2. TF-IDF ì˜ë¯¸ì  ë§¤ì¹­
        tfidf_matches = semantic_match_tfidf(bcf_text, ifc_map)
        if tfidf_matches:
            final_matches = [guid for guid, score in tfidf_matches]
            avg_score = sum(score for _, score in tfidf_matches) / len(tfidf_matches)
            match_type = 'tfidf'
            confidence = calculate_confidence('tfidf', avg_score)
        else:
            # 3. í‚¤ì›Œë“œ ë§¤ì¹­ (fallback)
            keyword_scores = []
            for guid, ifc_item in ifc_map.items():
                score = semantic_match_keyword(bcf_text, ifc_item)
                if score > 0:
                    keyword_scores.append((guid, score))
            
            if keyword_scores:
                keyword_scores.sort(key=lambda x: x[1], reverse=True)
                final_matches = [guid for guid, score in keyword_scores[:3]]
                avg_score = sum(score for _, score in keyword_scores[:3]) / len(keyword_scores[:3])
                match_type = 'keyword'
                confidence = calculate_confidence('keyword', avg_score)
    else:
        # TF-IDF ë¯¸ì‚¬ìš©: í‚¤ì›Œë“œ ë§¤ì¹­ë§Œ
        keyword_scores = []
        for guid, ifc_item in ifc_map.items():
            score = semantic_match_keyword(bcf_text, ifc_item)
            if score > 0:
                keyword_scores.append((guid, score))
        
        if keyword_scores:
            keyword_scores.sort(key=lambda x: x[1], reverse=True)
            final_matches = [guid for guid, score in keyword_scores[:3]]
            avg_score = sum(score for _, score in keyword_scores[:3]) / len(keyword_scores[:3])
            match_type = 'keyword'
            confidence = calculate_confidence('keyword', avg_score)
    
    return {
        "topic_id": bcf_issue["topic_id"],
        "guid_matches": final_matches,
        "confidence": confidence,
        "match_type": match_type,
        "evidence": bcf_text[:100] + "..." if len(bcf_text) > 100 else bcf_text
    }


def main():
    ap = argparse.ArgumentParser(description='ì „ì²´ IFC-BCF ë°ì´í„° ì¬ë§í¬')
    ap.add_argument("--data-dir", default="data/processed", help="ë°ì´í„° ë””ë ‰í† ë¦¬")
    ap.add_argument("--out", default="data/processed/all_links.jsonl", help="ì¶œë ¥ íŒŒì¼")
    ap.add_argument("--use-tfidf", action="store_true", help="TF-IDF ë§¤ì¹­ ì‚¬ìš©")
    ap.add_argument("--workers", type=int, default=4, help="ë³‘ë ¬ ì›Œì»¤ ìˆ˜")
    ap.add_argument("--limit-ifc", type=int, help="IFC íŒŒì¼ ìˆ˜ ì œí•œ (í…ŒìŠ¤íŠ¸ìš©)")
    ap.add_argument("--limit-bcf", type=int, help="BCF íŒŒì¼ ìˆ˜ ì œí•œ (í…ŒìŠ¤íŠ¸ìš©)")
    a = ap.parse_args()
    
    data_dir = Path(a.data_dir)
    
    # IFC íŒŒì¼ ë¡œë“œ
    print("ğŸ“‚ IFC íŒŒì¼ ë¡œë“œ ì¤‘...")
    ifc_files = sorted(data_dir.glob('*_ifc.jsonl'))
    if a.limit_ifc:
        ifc_files = ifc_files[:a.limit_ifc]
    
    all_ifc_data = {}
    for ifc_file in ifc_files:
        for entity in read_jsonl(str(ifc_file)):
            all_ifc_data[entity['guid']] = entity
    
    print(f"âœ… IFC ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(all_ifc_data):,}ê°œ ì—”í‹°í‹°")
    
    # BCF íŒŒì¼ ë¡œë“œ
    print("ğŸ“‚ BCF íŒŒì¼ ë¡œë“œ ì¤‘...")
    bcf_files = sorted(data_dir.glob('*_bcf.jsonl'))
    if a.limit_bcf:
        bcf_files = bcf_files[:a.limit_bcf]
    
    all_bcf_data = []
    for bcf_file in bcf_files:
        all_bcf_data.extend(read_jsonl(str(bcf_file)))
    
    print(f"âœ… BCF ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(all_bcf_data):,}ê°œ ì´ìŠˆ")
    
    # ë³‘ë ¬ ì²˜ë¦¬ ì¤€ë¹„
    print(f"\nğŸ”— ë§í¬ ìƒì„± ì‹œì‘ ({a.workers}ê°œ ì›Œì»¤)...")
    tasks = [
        (bcf_issue, all_ifc_data, a.use_tfidf)
        for bcf_issue in all_bcf_data
    ]
    
    all_links = []
    start_time = time.time()
    
    with ProcessPoolExecutor(max_workers=a.workers) as executor:
        futures = {
            executor.submit(process_single_bcf_issue, task): i
            for i, task in enumerate(tasks)
        }
        
        completed = 0
        for future in as_completed(futures):
            try:
                link = future.result()
                all_links.append(link)
                completed += 1
                
                if completed % 10 == 0 or completed == len(tasks):
                    elapsed = time.time() - start_time
                    rate = completed / elapsed if elapsed > 0 else 0
                    remaining = (len(tasks) - completed) / rate if rate > 0 else 0
                    print(f"   ì§„í–‰: {completed}/{len(tasks)} ({completed/len(tasks)*100:.1f}%) "
                          f"- {rate:.1f} ì´ìŠˆ/ì´ˆ, ë‚¨ì€ ì‹œê°„: {remaining:.0f}ì´ˆ", end='\\r')
            except Exception as e:
                print(f"\\nâš ï¸  ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
    
    print()  # ì¤„ë°”ê¿ˆ
    
    # ê²°ê³¼ ì €ì¥
    write_jsonl(a.out, all_links)
    
    # í†µê³„ ì¶œë ¥
    elapsed = time.time() - start_time
    print(f"\\nğŸ“Š ë§í¬ ìƒì„± í†µê³„:")
    print(f"   ì²˜ë¦¬ ì‹œê°„: {elapsed:.1f}ì´ˆ")
    print(f"   ì²˜ë¦¬ ì†ë„: {len(all_links)/elapsed:.1f} ì´ìŠˆ/ì´ˆ")
    print(f"   ì´ BCF ì´ìŠˆ: {len(all_links):,}ê°œ")
    
    with_matches = [r for r in all_links if r['guid_matches']]
    print(f"   ë§¤ì¹­ ì„±ê³µ: {len(with_matches):,}ê°œ ({len(with_matches)/len(all_links)*100:.1f}%)")
    
    total_links = sum(len(r['guid_matches']) for r in all_links)
    print(f"   ì´ ë§í¬ ìˆ˜: {total_links:,}ê°œ")
    
    avg_confidence = sum(r['confidence'] for r in all_links) / len(all_links) if all_links else 0
    print(f"   í‰ê·  ì‹ ë¢°ë„: {avg_confidence:.3f}")
    
    # ì‹ ë¢°ë„ë³„ ë¶„í¬
    high_conf = len([r for r in all_links if r['confidence'] >= 0.7])
    medium_conf = len([r for r in all_links if 0.4 <= r['confidence'] < 0.7])
    low_conf = len([r for r in all_links if r['confidence'] < 0.4])
    
    print(f"   ì‹ ë¢°ë„ ë¶„í¬:")
    print(f"      ë†’ìŒ (â‰¥0.7): {high_conf}ê°œ ({high_conf/len(all_links)*100:.1f}%)")
    print(f"      ì¤‘ê°„ (0.4-0.7): {medium_conf}ê°œ ({medium_conf/len(all_links)*100:.1f}%)")
    print(f"      ë‚®ìŒ (<0.4): {low_conf}ê°œ ({low_conf/len(all_links)*100:.1f}%)")
    
    # ë§¤ì¹­ íƒ€ì…ë³„ ë¶„í¬
    match_types = {}
    for r in with_matches:
        mt = r.get('match_type', 'unknown')
        match_types[mt] = match_types.get(mt, 0) + 1
    
    print(f"   ë§¤ì¹­ íƒ€ì…:")
    for mt, count in sorted(match_types.items(), key=lambda x: x[1], reverse=True):
        print(f"      {mt}: {count}ê°œ ({count/len(with_matches)*100:.1f}%)")
    
    print(f"\\nâœ… ë§í¬ íŒŒì¼ ì €ì¥ ì™„ë£Œ: {a.out}")
    
    # ê²€ì¦
    if total_links >= 500:
        print(f"âœ… ë§í¬ ìˆ˜ ëª©í‘œ ë‹¬ì„±: {total_links} >= 500")
    else:
        print(f"âš ï¸  ë§í¬ ìˆ˜ ëª©í‘œ ë¯¸ë‹¬: {total_links} < 500")
    
    if avg_confidence >= 0.6:
        print(f"âœ… í‰ê·  ì‹ ë¢°ë„ ëª©í‘œ ë‹¬ì„±: {avg_confidence:.3f} >= 0.6")
    else:
        print(f"âš ï¸  í‰ê·  ì‹ ë¢°ë„ ëª©í‘œ ë¯¸ë‹¬: {avg_confidence:.3f} < 0.6")


if __name__ == "__main__":
    main()


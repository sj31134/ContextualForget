#!/usr/bin/env python3
"""
ì‹¤ì œ BCF ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê·¸ë˜í”„ ì¬êµ¬ì„±
"""

import sys
import json
from pathlib import Path
from datetime import datetime
import networkx as nx

# Add src to path for imports
sys.path.insert(0, str(Path(__file__).parent.parent / 'src'))

from contextualforget.core.utils import read_jsonl
from contextualforget.data.build_graph import build_graph_from_files


def rebuild_graph_with_real_data():
    """ì‹¤ì œ BCF ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê·¸ë˜í”„ ì¬êµ¬ì„±"""
    
    print("ğŸ”„ ì‹¤ì œ ë°ì´í„° ê¸°ë°˜ ê·¸ë˜í”„ ì¬êµ¬ì„± ì‹œì‘...")
    
    # 1. ì‹¤ì œ BCF ë°ì´í„° ë¡œë“œ
    print("ğŸ“Š ì‹¤ì œ BCF ë°ì´í„° ë¡œë“œ ì¤‘...")
    real_bcf_file = Path("data/processed/real_bcf/real_bcf_data.jsonl")
    real_bcf_data = list(read_jsonl(str(real_bcf_file)))
    
    print(f"  âœ… ë¡œë“œëœ ì‹¤ì œ BCF ì´ìŠˆ: {len(real_bcf_data)}ê°œ")
    
    # 2. ê¸°ì¡´ IFC ë°ì´í„° ë¡œë“œ (ì‹¤ì œ IFC íŒŒì¼ë“¤)
    print("ğŸ“Š IFC ë°ì´í„° ë¡œë“œ ì¤‘...")
    ifc_files = list(Path("data").glob("**/*.ifc"))
    print(f"  ğŸ“ ë°œê²¬ëœ IFC íŒŒì¼: {len(ifc_files)}ê°œ")
    
    # 3. ê·¸ë˜í”„ ë°ì´í„° êµ¬ì¡° ìƒì„±
    print("ğŸ—ï¸ ê·¸ë˜í”„ ë°ì´í„° êµ¬ì¡° ìƒì„± ì¤‘...")
    graph_data = {
        "nodes": [],
        "edges": [],
        "metadata": {
            "creation_date": datetime.now().isoformat(),
            "data_source": "real_bcf_integrated",
            "bcf_issues": len(real_bcf_data),
            "ifc_files": len(ifc_files)
        }
    }
    
    # 4. BCF ë…¸ë“œ ì¶”ê°€
    print("ğŸ“‹ BCF ë…¸ë“œ ì¶”ê°€ ì¤‘...")
    bcf_nodes_added = 0
    
    for issue in real_bcf_data:
        topic_id = issue.get('topic_id', '')
        if topic_id:
            node_data = {
                "node_id": ("BCF", topic_id),
                "type": "BCF",
                "topic_id": topic_id,
                "title": issue.get('title', ''),
                "author": issue.get('author', ''),
                "description": issue.get('description', ''),
                "created": issue.get('created', ''),
                "source_file": issue.get('source_file', ''),
                "data_type": "real_bcf"
            }
            graph_data["nodes"].append(node_data)
            bcf_nodes_added += 1
    
    print(f"  âœ… ì¶”ê°€ëœ BCF ë…¸ë“œ: {bcf_nodes_added}ê°œ")
    
    # 5. IFC ë…¸ë“œ ì¶”ê°€ (ê¸°ì¡´ IFC íŒŒì¼ë“¤ì—ì„œ)
    print("ğŸ—ï¸ IFC ë…¸ë“œ ì¶”ê°€ ì¤‘...")
    ifc_nodes_added = 0
    
    for ifc_file in ifc_files:
        try:
            # ê°„ë‹¨í•œ IFC ì—”í‹°í‹° ì¶”ì¶œ (ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ íŒŒì‹± í•„ìš”)
            with open(ifc_file, 'r', encoding='utf-8', errors='ignore') as f:
                content = f.read()
            
            # IFC ì—”í‹°í‹° íŒ¨í„´ ì°¾ê¸°
            import re
            ifc_pattern = r"#(\d+)=IFC([A-Z0-9_]+)\('([A-Za-z0-9_]{10,24})'"
            matches = re.findall(ifc_pattern, content)
            
            for match in matches:
                entity_id, entity_type, guid = match
                node_data = {
                    "node_id": ("IFC", guid),
                    "type": "IFC",
                    "guid": guid,
                    "entity_type": entity_type,
                    "entity_id": entity_id,
                    "source_file": ifc_file.name,
                    "data_type": "real_ifc"
                }
                graph_data["nodes"].append(node_data)
                ifc_nodes_added += 1
                
                # ë„ˆë¬´ ë§ì€ ë…¸ë“œê°€ ìƒì„±ë˜ì§€ ì•Šë„ë¡ ì œí•œ
                if ifc_nodes_added >= 1000:
                    break
            
            if ifc_nodes_added >= 1000:
                break
                
        except Exception as e:
            print(f"  âš ï¸ IFC íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜: {ifc_file.name} - {e}")
            continue
    
    print(f"  âœ… ì¶”ê°€ëœ IFC ë…¸ë“œ: {ifc_nodes_added}ê°œ")
    
    # 6. BCF-IFC ì—°ê²° ìƒì„± (ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±)
    print("ğŸ”— BCF-IFC ì—°ê²° ìƒì„± ì¤‘...")
    edges_added = 0
    
    # BCF ì´ìŠˆì™€ IFC ì—”í‹°í‹° ê°„ì˜ ì—°ê²° ìƒì„±
    for bcf_node in graph_data["nodes"]:
        if bcf_node["type"] == "BCF":
            # BCF ì´ìŠˆì˜ ì œëª©ì´ë‚˜ ì„¤ëª…ì—ì„œ IFC ê´€ë ¨ í‚¤ì›Œë“œ ì°¾ê¸°
            title = bcf_node.get("title", "").lower()
            description = bcf_node.get("description", "").lower()
            
            # IFC ê´€ë ¨ í‚¤ì›Œë“œë“¤
            ifc_keywords = ["wall", "beam", "column", "slab", "door", "window", "space", "zone"]
            
            for ifc_node in graph_data["nodes"]:
                if ifc_node["type"] == "IFC":
                    entity_type = ifc_node.get("entity_type", "").lower()
                    
                    # í‚¤ì›Œë“œ ë§¤ì¹­ìœ¼ë¡œ ì—°ê²° ìƒì„±
                    if any(keyword in title or keyword in description for keyword in ifc_keywords):
                        if entity_type in title or entity_type in description:
                            edge_data = {
                                "source": bcf_node["node_id"],
                                "target": ifc_node["node_id"],
                                "relationship": "relates_to",
                                "confidence": 0.7,
                                "data_type": "real_connection"
                            }
                            graph_data["edges"].append(edge_data)
                            edges_added += 1
                            
                            # ë„ˆë¬´ ë§ì€ ì—°ê²°ì´ ìƒì„±ë˜ì§€ ì•Šë„ë¡ ì œí•œ
                            if edges_added >= 500:
                                break
            
            if edges_added >= 500:
                break
    
    print(f"  âœ… ì¶”ê°€ëœ ì—°ê²°: {edges_added}ê°œ")
    
    # 7. ê·¸ë˜í”„ ì €ì¥
    print("ğŸ’¾ ê·¸ë˜í”„ ì €ì¥ ì¤‘...")
    graph_output_dir = Path("data/processed/real_graph")
    graph_output_dir.mkdir(parents=True, exist_ok=True)
    
    # NetworkX ê·¸ë˜í”„ ìƒì„±
    G = nx.DiGraph()
    
    # ë…¸ë“œ ì¶”ê°€
    for node_data in graph_data["nodes"]:
        node_id = node_data["node_id"]
        G.add_node(node_id, **{k: v for k, v in node_data.items() if k != "node_id"})
    
    # ì—£ì§€ ì¶”ê°€
    for edge_data in graph_data["edges"]:
        G.add_edge(edge_data["source"], edge_data["target"], **{k: v for k, v in edge_data.items() if k not in ["source", "target"]})
    
    # ê·¸ë˜í”„ ì €ì¥
    graph_file = graph_output_dir / "real_data_graph.pkl"
    import pickle
    with open(graph_file, 'wb') as f:
        pickle.dump(G, f)
    
    # ë©”íƒ€ë°ì´í„° ì €ì¥
    metadata_file = graph_output_dir / "real_data_graph_metadata.json"
    with open(metadata_file, 'w', encoding='utf-8') as f:
        json.dump(graph_data["metadata"], f, ensure_ascii=False, indent=2)
    
    # 8. í†µê³„ ìƒì„±
    stats = {
        "graph_creation_date": datetime.now().isoformat(),
        "total_nodes": len(graph_data["nodes"]),
        "bcf_nodes": bcf_nodes_added,
        "ifc_nodes": ifc_nodes_added,
        "total_edges": len(graph_data["edges"]),
        "graph_density": nx.density(G),
        "connected_components": nx.number_weakly_connected_components(G),
        "average_clustering": nx.average_clustering(G.to_undirected()) if G.number_of_nodes() > 0 else 0
    }
    
    stats_file = graph_output_dir / "real_data_graph_stats.json"
    with open(stats_file, 'w', encoding='utf-8') as f:
        json.dump(stats, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… ê·¸ë˜í”„ ì¬êµ¬ì„± ì™„ë£Œ!")
    print(f"  ğŸ“Š ì´ ë…¸ë“œ: {stats['total_nodes']}ê°œ")
    print(f"  ğŸ“‹ BCF ë…¸ë“œ: {stats['bcf_nodes']}ê°œ")
    print(f"  ğŸ—ï¸ IFC ë…¸ë“œ: {stats['ifc_nodes']}ê°œ")
    print(f"  ğŸ”— ì´ ì—°ê²°: {stats['total_edges']}ê°œ")
    print(f"  ğŸ“ˆ ê·¸ë˜í”„ ë°€ë„: {stats['graph_density']:.4f}")
    print(f"  ğŸ’¾ ì €ì¥ ìœ„ì¹˜: {graph_file}")
    
    return stats


def update_gold_standard_with_real_data():
    """ì‹¤ì œ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ Gold Standard ì—…ë°ì´íŠ¸"""
    
    print("\nğŸ¯ ì‹¤ì œ ë°ì´í„° ê¸°ë°˜ Gold Standard ì—…ë°ì´íŠ¸...")
    
    # 1. ì‹¤ì œ BCF ë°ì´í„°ì—ì„œ ì‹¤ì œ GUID/í‚¤ì›Œë“œ ì¶”ì¶œ
    real_bcf_file = Path("data/processed/real_bcf/real_bcf_data.jsonl")
    real_bcf_data = list(read_jsonl(str(real_bcf_file)))
    
    # 2. ì‹¤ì œ topic_idë“¤ ì¶”ì¶œ
    real_topic_ids = [issue.get('topic_id', '') for issue in real_bcf_data if issue.get('topic_id')]
    real_titles = [issue.get('title', '') for issue in real_bcf_data if issue.get('title', '').strip()]
    
    print(f"  ğŸ“Š ì‹¤ì œ topic_id: {len(real_topic_ids)}ê°œ")
    print(f"  ğŸ“ ì‹¤ì œ ì œëª©: {len(real_titles)}ê°œ")
    
    # 3. ê¸°ì¡´ Gold Standard ë¡œë“œ
    gold_standard_file = Path("eval/gold_standard_v3_fixed.jsonl")
    if not gold_standard_file.exists():
        print("  âš ï¸ ê¸°ì¡´ Gold Standard íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    gold_standard = list(read_jsonl(str(gold_standard_file)))
    
    # 4. ì‹¤ì œ ë°ì´í„°ë¡œ Gold Standard ì—…ë°ì´íŠ¸
    updated_gold_standard = []
    
    for i, qa in enumerate(gold_standard):
        updated_qa = qa.copy()
        
        # ì‹¤ì œ topic_id ì‚¬ìš©
        if i < len(real_topic_ids) and real_topic_ids[i]:
            updated_qa['gold_entities'] = [real_topic_ids[i]]
            
            # ì§ˆë¬¸ ì—…ë°ì´íŠ¸
            if qa['category'] == 'entity_search':
                updated_qa['question'] = f"GUID {real_topic_ids[i]}ì˜ ì—”í‹°í‹° íƒ€ì…ì€ ë¬´ì—‡ì¸ê°€ìš”?"
            elif qa['category'] == 'issue_search':
                updated_qa['question'] = f"'{real_topic_ids[i]}'ì™€ ê´€ë ¨ëœ ì´ìŠˆëŠ” ë¬´ì—‡ì¸ê°€ìš”?"
            elif qa['category'] == 'relationship':
                updated_qa['question'] = f"'{real_topic_ids[i]}'ì™€ ê´€ë ¨ëœ ì •ë³´ëŠ” ë¬´ì—‡ì¸ê°€ìš”?"
        
        # ì‹¤ì œ ì œëª© ì‚¬ìš©
        elif i < len(real_titles) and real_titles[i]:
            updated_qa['gold_entities'] = [real_titles[i]]
            
            if qa['category'] == 'entity_search':
                updated_qa['question'] = f"'{real_titles[i]}'ì™€ ê´€ë ¨ëœ ì •ë³´ëŠ” ë¬´ì—‡ì¸ê°€ìš”?"
            elif qa['category'] == 'issue_search':
                updated_qa['question'] = f"'{real_titles[i]}'ì™€ ê´€ë ¨ëœ ì´ìŠˆëŠ” ë¬´ì—‡ì¸ê°€ìš”?"
            elif qa['category'] == 'relationship':
                updated_qa['question'] = f"'{real_titles[i]}'ì™€ ê´€ë ¨ëœ ì •ë³´ëŠ” ë¬´ì—‡ì¸ê°€ìš”?"
        
        updated_gold_standard.append(updated_qa)
    
    # 5. ì—…ë°ì´íŠ¸ëœ Gold Standard ì €ì¥
    updated_gold_file = Path("eval/gold_standard_real_data.jsonl")
    with open(updated_gold_file, 'w', encoding='utf-8') as f:
        for qa in updated_gold_standard:
            f.write(json.dumps(qa, ensure_ascii=False) + '\n')
    
    print(f"  âœ… ì—…ë°ì´íŠ¸ëœ Gold Standard ì €ì¥: {updated_gold_file}")
    print(f"  ğŸ“Š ì´ ì§ˆë¬¸ ìˆ˜: {len(updated_gold_standard)}ê°œ")
    
    return updated_gold_file


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ ì‹¤ì œ ë°ì´í„° ê¸°ë°˜ ê·¸ë˜í”„ ì¬êµ¬ì„± ì‹œì‘")
    print("="*60)
    
    try:
        # 1. ê·¸ë˜í”„ ì¬êµ¬ì„±
        stats = rebuild_graph_with_real_data()
        
        # 2. Gold Standard ì—…ë°ì´íŠ¸
        update_gold_standard_with_real_data()
        
        print("\n" + "="*60)
        print("ğŸ‰ ì‹¤ì œ ë°ì´í„° ê¸°ë°˜ ì‹œìŠ¤í…œ ì¬êµ¬ì„± ì™„ë£Œ!")
        print("ğŸ“ˆ ì—°êµ¬ ì‹ ë¢°ë„ ëŒ€í­ í–¥ìƒ!")
        print("ğŸ”§ ë§¤ì¹­ ì‹¤íŒ¨ ë¬¸ì œ í•´ê²°ì„ ìœ„í•œ ê¸°ë°˜ ë§ˆë ¨!")
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return 1
    
    return 0


if __name__ == "__main__":
    exit(main())

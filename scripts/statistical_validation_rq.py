#!/usr/bin/env python3
"""
RQ ê²€ì¦ì„ ìœ„í•œ í†µê³„ì  ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸
t-test, ANOVA, Cohen's d, íˆìŠ¤í† ê·¸ë¨, Box-Whisker í”Œë¡¯ ìƒì„±
"""

import sys
import json
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
# import seaborn as sns  # seaborn ì—†ì´ ì‹¤í–‰
from pathlib import Path
from datetime import datetime
from collections import defaultdict
from scipy import stats
from scipy.stats import ttest_ind, f_oneway, levene, shapiro
import warnings
warnings.filterwarnings('ignore')

# Add src to path for imports
sys.path.insert(0, str(Path(__file__).parent.parent / 'src'))


def load_evaluation_results():
    """í‰ê°€ ê²°ê³¼ ë¡œë“œ"""
    
    print("ğŸ“‚ í‰ê°€ ê²°ê³¼ ë¡œë“œ ì¤‘...")
    
    # ìµœì‹  í‰ê°€ ê²°ê³¼ íŒŒì¼ ì°¾ê¸°
    results_dir = Path("results/evaluation_extended")
    if not results_dir.exists():
        print("âŒ í‰ê°€ ê²°ê³¼ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None
    
    # ê°€ì¥ ìµœê·¼ íŒŒì¼ ì°¾ê¸°
    result_files = list(results_dir.glob("evaluation_extended_*_comprehensive.json"))
    if not result_files:
        print("âŒ í‰ê°€ ê²°ê³¼ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None
    
    latest_file = max(result_files, key=lambda x: x.stat().st_mtime)
    print(f"  ğŸ“ ë¡œë“œ ì¤‘: {latest_file}")
    
    with open(latest_file, 'r', encoding='utf-8') as f:
        results = json.load(f)
    
    print(f"  âœ… í‰ê°€ ê²°ê³¼ ë¡œë“œ ì™„ë£Œ: {results['total_queries']}ê°œ ì§ˆì˜")
    return results


def prepare_data_for_analysis(results):
    """í†µê³„ ë¶„ì„ì„ ìœ„í•œ ë°ì´í„° ì¤€ë¹„"""
    
    print("ğŸ“Š í†µê³„ ë¶„ì„ìš© ë°ì´í„° ì¤€ë¹„ ì¤‘...")
    
    # ë°ì´í„°í”„ë ˆì„ ìƒì„±
    data_rows = []
    
    for engine_name, engine_data in results["engine_results"].items():
        for qtype, query_results in engine_data.items():
            for result in query_results:
                data_rows.append({
                    'engine': engine_name,
                    'query_type': qtype,
                    'query_id': result['query_id'],
                    'success': result['success'],
                    'confidence': result['confidence'],
                    'response_time': result['response_time'],
                    'memory_delta_mb': result['memory_delta_mb'],
                    'cpu_delta_percent': result['cpu_delta_percent']
                })
    
    df = pd.DataFrame(data_rows)
    
    print(f"  âœ… ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ: {len(df)}ê°œ ë ˆì½”ë“œ")
    print(f"  ğŸ“Š ì—”ì§„ë³„ ë¶„í¬:")
    for engine in df['engine'].unique():
        count = len(df[df['engine'] == engine])
        print(f"    â€¢ {engine}: {count}ê°œ")
    
    return df


def rq1_contextual_forgetting_effectiveness(df):
    """RQ1: ë§¥ë½ì  ë§ê° ë©”ì»¤ë‹ˆì¦˜ì˜ íš¨ê³¼ì„± ê²€ì¦"""
    
    print("ğŸ”¬ RQ1: ë§¥ë½ì  ë§ê° ë©”ì»¤ë‹ˆì¦˜ì˜ íš¨ê³¼ì„± ê²€ì¦ ì¤‘...")
    
    # ContextualForget vs BM25, Vector ë¹„êµ
    contextualforget = df[df['engine'] == 'ContextualForget']
    bm25 = df[df['engine'] == 'BM25']
    vector = df[df['engine'] == 'Vector']
    
    rq1_results = {
        "hypothesis": "ContextualForgetì´ BM25, Vectorë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì¼ ê²ƒì´ë‹¤",
        "comparisons": {}
    }
    
    # ContextualForget vs BM25
    if len(contextualforget) > 0 and len(bm25) > 0:
        # ì„±ê³µë¥  ë¹„êµ
        cf_success_rate = contextualforget['success'].mean()
        bm25_success_rate = bm25['success'].mean()
        
        # ì‹ ë¢°ë„ ë¹„êµ
        cf_confidence = contextualforget['confidence'].mean()
        bm25_confidence = bm25['confidence'].mean()
        
        # t-test (ì‹ ë¢°ë„)
        t_stat_cf_bm25, p_val_cf_bm25 = ttest_ind(
            contextualforget['confidence'], 
            bm25['confidence']
        )
        
        # Cohen's d (íš¨ê³¼ í¬ê¸°)
        pooled_std = np.sqrt(((len(contextualforget)-1) * contextualforget['confidence'].std()**2 + 
                             (len(bm25)-1) * bm25['confidence'].std()**2) / 
                            (len(contextualforget) + len(bm25) - 2))
        cohens_d_cf_bm25 = (cf_confidence - bm25_confidence) / pooled_std if pooled_std > 0 else 0
        
        rq1_results["comparisons"]["ContextualForget_vs_BM25"] = {
            "success_rate": {
                "ContextualForget": cf_success_rate,
                "BM25": bm25_success_rate,
                "difference": cf_success_rate - bm25_success_rate
            },
            "confidence": {
                "ContextualForget": cf_confidence,
                "BM25": bm25_confidence,
                "difference": cf_confidence - bm25_confidence
            },
            "t_test": {
                "t_statistic": t_stat_cf_bm25,
                "p_value": p_val_cf_bm25,
                "significant": bool(p_val_cf_bm25 < 0.01)
            },
            "effect_size": {
                "cohens_d": cohens_d_cf_bm25,
                "interpretation": "large" if abs(cohens_d_cf_bm25) > 0.8 else "medium" if abs(cohens_d_cf_bm25) > 0.5 else "small"
            }
        }
    
    # ContextualForget vs Vector
    if len(contextualforget) > 0 and len(vector) > 0:
        vector_success_rate = vector['success'].mean()
        vector_confidence = vector['confidence'].mean()
        
        t_stat_cf_vector, p_val_cf_vector = ttest_ind(
            contextualforget['confidence'], 
            vector['confidence']
        )
        
        pooled_std = np.sqrt(((len(contextualforget)-1) * contextualforget['confidence'].std()**2 + 
                             (len(vector)-1) * vector['confidence'].std()**2) / 
                            (len(contextualforget) + len(vector) - 2))
        cohens_d_cf_vector = (cf_confidence - vector_confidence) / pooled_std if pooled_std > 0 else 0
        
        rq1_results["comparisons"]["ContextualForget_vs_Vector"] = {
            "success_rate": {
                "ContextualForget": cf_success_rate,
                "Vector": vector_success_rate,
                "difference": cf_success_rate - vector_success_rate
            },
            "confidence": {
                "ContextualForget": cf_confidence,
                "Vector": vector_confidence,
                "difference": cf_confidence - vector_confidence
            },
            "t_test": {
                "t_statistic": t_stat_cf_vector,
                "p_value": p_val_cf_vector,
                "significant": bool(p_val_cf_vector < 0.01)
            },
            "effect_size": {
                "cohens_d": cohens_d_cf_vector,
                "interpretation": "large" if abs(cohens_d_cf_vector) > 0.8 else "medium" if abs(cohens_d_cf_vector) > 0.5 else "small"
            }
        }
    
    print(f"  âœ… RQ1 ê²€ì¦ ì™„ë£Œ")
    return rq1_results


def rq2_adaptive_retrieval_superiority(df):
    """RQ2: ì ì‘ì  ê²€ìƒ‰ ì „ëµì˜ ìš°ìˆ˜ì„± ê²€ì¦"""
    
    print("ğŸ”¬ RQ2: ì ì‘ì  ê²€ìƒ‰ ì „ëµì˜ ìš°ìˆ˜ì„± ê²€ì¦ ì¤‘...")
    
    # ì¿¼ë¦¬ íƒ€ì…ë³„ ì„±ëŠ¥ ë¶„ì„
    query_types = df['query_type'].unique()
    
    rq2_results = {
        "hypothesis": "ì ì‘ì  ê²€ìƒ‰ ì „ëµì´ ì¿¼ë¦¬ íƒ€ì…ë³„ë¡œ ìµœì ì˜ ì„±ëŠ¥ì„ ë³´ì¼ ê²ƒì´ë‹¤",
        "query_type_analysis": {},
        "anova_results": {}
    }
    
    # ê° ì¿¼ë¦¬ íƒ€ì…ë³„ë¡œ ì—”ì§„ ì„±ëŠ¥ ë¹„êµ
    for qtype in query_types:
        qtype_data = df[df['query_type'] == qtype]
        
        if len(qtype_data) == 0:
            continue
        
        # ì—”ì§„ë³„ ì„±ëŠ¥
        engine_performance = {}
        for engine in qtype_data['engine'].unique():
            engine_data = qtype_data[qtype_data['engine'] == engine]
            engine_performance[engine] = {
                "success_rate": engine_data['success'].mean(),
                "avg_confidence": engine_data['confidence'].mean(),
                "avg_response_time": engine_data['response_time'].mean(),
                "count": len(engine_data)
            }
        
        rq2_results["query_type_analysis"][qtype] = engine_performance
    
    # ANOVA: ì¿¼ë¦¬ íƒ€ì…ë³„ ì ì‘ì  ì „ëµ íš¨ê³¼
    if len(query_types) > 1:
        # ê° ì¿¼ë¦¬ íƒ€ì…ë³„ë¡œ Hybrid ì—”ì§„ì˜ ì„±ëŠ¥
        hybrid_performance_by_type = []
        for qtype in query_types:
            qtype_hybrid = df[(df['query_type'] == qtype) & (df['engine'] == 'Hybrid')]
            if len(qtype_hybrid) > 0:
                hybrid_performance_by_type.append(qtype_hybrid['confidence'].values)
        
        if len(hybrid_performance_by_type) > 1:
            # ANOVA ìˆ˜í–‰
            f_stat, p_val = f_oneway(*hybrid_performance_by_type)
            
            rq2_results["anova_results"] = {
                "f_statistic": f_stat,
                "p_value": p_val,
                "significant": bool(p_val < 0.01),
                "interpretation": "ì¿¼ë¦¬ íƒ€ì…ë³„ ì ì‘ì  ì „ëµ íš¨ê³¼ê°€ í†µê³„ì ìœ¼ë¡œ ìœ ì˜í•¨" if p_val < 0.01 else "ì¿¼ë¦¬ íƒ€ì…ë³„ íš¨ê³¼ê°€ í†µê³„ì ìœ¼ë¡œ ìœ ì˜í•˜ì§€ ì•ŠìŒ"
            }
    
    print(f"  âœ… RQ2 ê²€ì¦ ì™„ë£Œ")
    return rq2_results


def rq3_system_universality(df):
    """RQ3: í†µí•© ì‹œìŠ¤í…œì˜ ë²”ìš©ì„± ê²€ì¦"""
    
    print("ğŸ”¬ RQ3: í†µí•© ì‹œìŠ¤í…œì˜ ë²”ìš©ì„± ê²€ì¦ ì¤‘...")
    
    rq3_results = {
        "hypothesis": "í†µí•© ì‹œìŠ¤í…œì´ ë‹¤ì–‘í•œ ì¿¼ë¦¬ íƒ€ì…ê³¼ ì—”ì§„ì—ì„œ ì¼ê´€ë˜ê²Œ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì¼ ê²ƒì´ë‹¤",
        "universality_analysis": {}
    }
    
    # ì—”ì§„ë³„ ì „ì²´ ì„±ëŠ¥
    engine_performance = {}
    for engine in df['engine'].unique():
        engine_data = df[df['engine'] == engine]
        engine_performance[engine] = {
            "success_rate": engine_data['success'].mean(),
            "avg_confidence": engine_data['confidence'].mean(),
            "avg_response_time": engine_data['response_time'].mean(),
            "total_queries": len(engine_data)
        }
    
    rq3_results["universality_analysis"]["engine_performance"] = engine_performance
    
    # ì¿¼ë¦¬ íƒ€ì…ë³„ ì„±ëŠ¥ ì¼ê´€ì„±
    query_type_consistency = {}
    for qtype in df['query_type'].unique():
        qtype_data = df[df['query_type'] == qtype]
        
        # ê° ì—”ì§„ì˜ ì„±ëŠ¥
        engine_scores = []
        for engine in qtype_data['engine'].unique():
            engine_data = qtype_data[qtype_data['engine'] == engine]
            if len(engine_data) > 0:
                engine_scores.append(engine_data['confidence'].mean())
        
        if len(engine_scores) > 1:
            query_type_consistency[qtype] = {
                "mean_performance": np.mean(engine_scores),
                "std_performance": np.std(engine_scores),
                "coefficient_of_variation": np.std(engine_scores) / np.mean(engine_scores) if np.mean(engine_scores) > 0 else 0,
                "performance_range": max(engine_scores) - min(engine_scores)
            }
    
    rq3_results["universality_analysis"]["query_type_consistency"] = query_type_consistency
    
    # ì „ì²´ ì‹œìŠ¤í…œ ì•ˆì •ì„± (ëª¨ë“  ì—”ì§„ì˜ ì„±ëŠ¥ ë³€ë™ì„±)
    all_confidences = df['confidence'].values
    rq3_results["universality_analysis"]["system_stability"] = {
        "overall_mean_confidence": np.mean(all_confidences),
        "overall_std_confidence": np.std(all_confidences),
        "confidence_range": max(all_confidences) - min(all_confidences),
        "coefficient_of_variation": np.std(all_confidences) / np.mean(all_confidences) if np.mean(all_confidences) > 0 else 0
    }
    
    print(f"  âœ… RQ3 ê²€ì¦ ì™„ë£Œ")
    return rq3_results


def create_visualizations(df, rq1_results, rq2_results, rq3_results):
    """ì‹œê°í™” ìƒì„±"""
    
    print("ğŸ“Š ì‹œê°í™” ìƒì„± ì¤‘...")
    
    # ê²°ê³¼ ë””ë ‰í† ë¦¬ ìƒì„±
    viz_dir = Path("visualizations/statistical_analysis")
    viz_dir.mkdir(parents=True, exist_ok=True)
    
    # 1. ì‹ ë¢°ë„ ë¶„í¬ íˆìŠ¤í† ê·¸ë¨
    plt.figure(figsize=(12, 8))
    for i, engine in enumerate(df['engine'].unique()):
        plt.subplot(2, 2, i+1)
        engine_data = df[df['engine'] == engine]['confidence']
        plt.hist(engine_data, bins=20, alpha=0.7, edgecolor='black')
        plt.title(f'{engine} - Confidence Distribution')
        plt.xlabel('Confidence')
        plt.ylabel('Frequency')
        plt.axvline(engine_data.mean(), color='red', linestyle='--', label=f'Mean: {engine_data.mean():.3f}')
        plt.legend()
    
    plt.tight_layout()
    plt.savefig(viz_dir / 'confidence_distribution_histogram.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    # 2. Box-Whisker í”Œë¡¯ (ì—”ì§„ë³„ ì„±ëŠ¥)
    plt.figure(figsize=(10, 6))
    df_melted = df.melt(id_vars=['engine'], value_vars=['confidence', 'response_time'], 
                       var_name='metric', value_name='value')
    
    # sns.boxplot(data=df_melted, x='engine', y='value', hue='metric')  # seaborn ëŒ€ì‹  matplotlib ì‚¬ìš©
    df_melted.boxplot(column='value', by=['engine', 'metric'], ax=plt.gca())
    plt.title('Engine Performance Comparison (Box-Whisker Plot)')
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.savefig(viz_dir / 'engine_performance_boxplot.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    # 3. ì¿¼ë¦¬ íƒ€ì…ë³„ ì„±ëŠ¥ íˆíŠ¸ë§µ
    plt.figure(figsize=(12, 8))
    
    # ì„±ê³µë¥  íˆíŠ¸ë§µ
    plt.subplot(1, 2, 1)
    success_pivot = df.groupby(['engine', 'query_type'])['success'].mean().unstack()
    # sns.heatmap(success_pivot, annot=True, fmt='.2f', cmap='YlOrRd')  # matplotlibìœ¼ë¡œ ëŒ€ì²´
    plt.imshow(success_pivot.values, cmap='YlOrRd', aspect='auto')
    plt.colorbar()
    plt.xticks(range(len(success_pivot.columns)), success_pivot.columns)
    plt.yticks(range(len(success_pivot.index)), success_pivot.index)
    plt.title('Success Rate by Engine and Query Type')
    
    # ì‹ ë¢°ë„ íˆíŠ¸ë§µ
    plt.subplot(1, 2, 2)
    confidence_pivot = df.groupby(['engine', 'query_type'])['confidence'].mean().unstack()
    # sns.heatmap(confidence_pivot, annot=True, fmt='.3f', cmap='Blues')  # matplotlibìœ¼ë¡œ ëŒ€ì²´
    plt.imshow(confidence_pivot.values, cmap='Blues', aspect='auto')
    plt.colorbar()
    plt.xticks(range(len(confidence_pivot.columns)), confidence_pivot.columns)
    plt.yticks(range(len(confidence_pivot.index)), confidence_pivot.index)
    plt.title('Confidence by Engine and Query Type')
    
    plt.tight_layout()
    plt.savefig(viz_dir / 'performance_heatmap.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    # 4. í™•ì¥ì„± ê·¸ë˜í”„ (ì‘ë‹µ ì‹œê°„ vs ì—”ì§„)
    plt.figure(figsize=(10, 6))
    # sns.boxplot(data=df, x='engine', y='response_time')  # matplotlibìœ¼ë¡œ ëŒ€ì²´
    df.boxplot(column='response_time', by='engine', ax=plt.gca())
    plt.title('Response Time Distribution by Engine')
    plt.ylabel('Response Time (seconds)')
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.savefig(viz_dir / 'response_time_scalability.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    # 5. ë§ê° ì ìˆ˜ ì‹œë®¬ë ˆì´ì…˜ (ì‹œê°„ ê²½ê³¼ë³„)
    plt.figure(figsize=(10, 6))
    
    # ë§ê° ì ìˆ˜ ì‹œë®¬ë ˆì´ì…˜
    time_points = np.linspace(0, 365, 100)  # 1ë…„ê°„
    forgetting_scores = np.exp(-0.1 * time_points / 365)  # ì§€ìˆ˜ì  ê°ì†Œ
    
    plt.plot(time_points, forgetting_scores, linewidth=2, color='red')
    plt.title('Simulated Forgetting Score Over Time')
    plt.xlabel('Days Since Last Access')
    plt.ylabel('Forgetting Score')
    plt.grid(True, alpha=0.3)
    plt.axhline(y=0.5, color='gray', linestyle='--', alpha=0.7, label='Threshold (0.5)')
    plt.legend()
    plt.tight_layout()
    plt.savefig(viz_dir / 'forgetting_score_timeline.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"  âœ… ì‹œê°í™” ìƒì„± ì™„ë£Œ: {viz_dir}")
    return viz_dir


def save_statistical_results(rq1_results, rq2_results, rq3_results, viz_dir):
    """í†µê³„ì  ê²°ê³¼ ì €ì¥"""
    
    print("ğŸ’¾ í†µê³„ì  ê²°ê³¼ ì €ì¥ ì¤‘...")
    
    # ê²°ê³¼ ë””ë ‰í† ë¦¬ ìƒì„±
    results_dir = Path("results/statistical_validation")
    results_dir.mkdir(parents=True, exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # í†µí•© ê²°ê³¼
    statistical_results = {
        "analysis_date": datetime.now().isoformat(),
        "rq1_contextual_forgetting": rq1_results,
        "rq2_adaptive_retrieval": rq2_results,
        "rq3_system_universality": rq3_results,
        "visualization_directory": str(viz_dir)
    }
    
    # JSON ì €ì¥
    results_file = results_dir / f"statistical_validation_{timestamp}.json"
    with open(results_file, 'w', encoding='utf-8') as f:
        json.dump(statistical_results, f, indent=2, ensure_ascii=False)
    
    # ìš”ì•½ ë³´ê³ ì„œ ìƒì„±
    summary_file = results_dir / f"statistical_summary_{timestamp}.md"
    with open(summary_file, 'w', encoding='utf-8') as f:
        f.write("# í†µê³„ì  ê²€ì¦ ê²°ê³¼ ìš”ì•½\n\n")
        f.write(f"**ë¶„ì„ ì¼ì‹œ**: {statistical_results['analysis_date']}\n\n")
        
        f.write("## RQ1: ë§¥ë½ì  ë§ê° ë©”ì»¤ë‹ˆì¦˜ì˜ íš¨ê³¼ì„±\n\n")
        f.write(f"**ê°€ì„¤**: {rq1_results['hypothesis']}\n\n")
        
        for comparison, data in rq1_results['comparisons'].items():
            f.write(f"### {comparison}\n\n")
            f.write(f"- **ì„±ê³µë¥  ì°¨ì´**: {data['success_rate']['difference']:.3f}\n")
            f.write(f"- **ì‹ ë¢°ë„ ì°¨ì´**: {data['confidence']['difference']:.3f}\n")
            f.write(f"- **t-test p-value**: {data['t_test']['p_value']:.6f}\n")
            f.write(f"- **í†µê³„ì  ìœ ì˜ì„±**: {'ìœ ì˜í•¨' if data['t_test']['significant'] else 'ìœ ì˜í•˜ì§€ ì•ŠìŒ'}\n")
            f.write(f"- **íš¨ê³¼ í¬ê¸° (Cohen's d)**: {data['effect_size']['cohens_d']:.3f} ({data['effect_size']['interpretation']})\n\n")
        
        f.write("## RQ2: ì ì‘ì  ê²€ìƒ‰ ì „ëµì˜ ìš°ìˆ˜ì„±\n\n")
        f.write(f"**ê°€ì„¤**: {rq2_results['hypothesis']}\n\n")
        
        if rq2_results['anova_results']:
            f.write("### ANOVA ê²°ê³¼\n\n")
            f.write(f"- **F-statistic**: {rq2_results['anova_results']['f_statistic']:.3f}\n")
            f.write(f"- **p-value**: {rq2_results['anova_results']['p_value']:.6f}\n")
            f.write(f"- **í•´ì„**: {rq2_results['anova_results']['interpretation']}\n\n")
        
        f.write("## RQ3: í†µí•© ì‹œìŠ¤í…œì˜ ë²”ìš©ì„±\n\n")
        f.write(f"**ê°€ì„¤**: {rq3_results['hypothesis']}\n\n")
        
        f.write("### ì—”ì§„ë³„ ì„±ëŠ¥\n\n")
        for engine, perf in rq3_results['universality_analysis']['engine_performance'].items():
            f.write(f"- **{engine}**: ì„±ê³µë¥  {perf['success_rate']:.1%}, ì‹ ë¢°ë„ {perf['avg_confidence']:.3f}\n")
        
        f.write(f"\n### ì‹œìŠ¤í…œ ì•ˆì •ì„±\n\n")
        stability = rq3_results['universality_analysis']['system_stability']
        f.write(f"- **ì „ì²´ í‰ê·  ì‹ ë¢°ë„**: {stability['overall_mean_confidence']:.3f}\n")
        f.write(f"- **ì‹ ë¢°ë„ í‘œì¤€í¸ì°¨**: {stability['overall_std_confidence']:.3f}\n")
        f.write(f"- **ë³€ë™ê³„ìˆ˜**: {stability['coefficient_of_variation']:.3f}\n")
    
    print(f"  âœ… ê²°ê³¼ ì €ì¥ ì™„ë£Œ:")
    print(f"    â€¢ í†µê³„ ê²°ê³¼: {results_file}")
    print(f"    â€¢ ìš”ì•½ ë³´ê³ ì„œ: {summary_file}")
    
    return {
        "results_file": str(results_file),
        "summary_file": str(summary_file)
    }


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    print("ğŸš€ í†µê³„ì  ê²€ì¦ ì‹œì‘")
    print("=" * 60)
    
    # 1. í‰ê°€ ê²°ê³¼ ë¡œë“œ
    results = load_evaluation_results()
    if not results:
        return
    
    print("\n" + "=" * 60)
    
    # 2. ë°ì´í„° ì¤€ë¹„
    df = prepare_data_for_analysis(results)
    
    print("\n" + "=" * 60)
    
    # 3. RQ1 ê²€ì¦
    rq1_results = rq1_contextual_forgetting_effectiveness(df)
    
    print("\n" + "=" * 60)
    
    # 4. RQ2 ê²€ì¦
    rq2_results = rq2_adaptive_retrieval_superiority(df)
    
    print("\n" + "=" * 60)
    
    # 5. RQ3 ê²€ì¦
    rq3_results = rq3_system_universality(df)
    
    print("\n" + "=" * 60)
    
    # 6. ì‹œê°í™” ìƒì„±
    viz_dir = create_visualizations(df, rq1_results, rq2_results, rq3_results)
    
    print("\n" + "=" * 60)
    
    # 7. ê²°ê³¼ ì €ì¥
    saved_files = save_statistical_results(rq1_results, rq2_results, rq3_results, viz_dir)
    
    print("\n" + "=" * 60)
    print("ğŸ‰ í†µê³„ì  ê²€ì¦ ì™„ë£Œ!")
    print(f"\nğŸ“Š ì£¼ìš” ê²°ê³¼:")
    
    # RQ1 ìš”ì•½
    if rq1_results['comparisons']:
        for comparison, data in rq1_results['comparisons'].items():
            print(f"  â€¢ {comparison}: p={data['t_test']['p_value']:.6f}, Cohen's d={data['effect_size']['cohens_d']:.3f}")
    
    # RQ2 ìš”ì•½
    if rq2_results['anova_results']:
        print(f"  â€¢ RQ2 ANOVA: F={rq2_results['anova_results']['f_statistic']:.3f}, p={rq2_results['anova_results']['p_value']:.6f}")
    
    # RQ3 ìš”ì•½
    stability = rq3_results['universality_analysis']['system_stability']
    print(f"  â€¢ RQ3 ì‹œìŠ¤í…œ ì•ˆì •ì„±: í‰ê·  ì‹ ë¢°ë„ {stability['overall_mean_confidence']:.3f}, ë³€ë™ê³„ìˆ˜ {stability['coefficient_of_variation']:.3f}")
    
    print(f"\nğŸ“ ê²°ê³¼ íŒŒì¼:")
    for file_type, file_path in saved_files.items():
        print(f"  â€¢ {file_type}: {file_path}")


if __name__ == "__main__":
    main()

#!/usr/bin/env python3
"""
ê¸°ì¡´ ìˆ˜ì§‘ ë°ì´í„° ë³µêµ¬ ìŠ¤í¬ë¦½íŠ¸
515ê°œ íŒŒì¼ (428 IFC + 87 BCF)ì„ ë¶„ì„í•˜ê³  í†µí•© ë°ì´í„°ì…‹ êµ¬ì¶•ì„ ìœ„í•œ ì¤€ë¹„
"""

import sys
import json
import shutil
from pathlib import Path
from datetime import datetime
from collections import defaultdict

# Add src to path for imports
sys.path.insert(0, str(Path(__file__).parent.parent / 'src'))

from contextualforget.core.utils import parse_bcf_zip, write_jsonl, read_jsonl


def analyze_downloaded_data():
    """ë‹¤ìš´ë¡œë“œëœ ë°ì´í„° ë¶„ì„"""
    
    downloaded_dir = Path("data/raw/downloaded")
    if not downloaded_dir.exists():
        print("âŒ data/raw/downloaded ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return None
    
    print("ğŸ” ë‹¤ìš´ë¡œë“œëœ ë°ì´í„° ë¶„ì„ ì¤‘...")
    
    # íŒŒì¼ íƒ€ì…ë³„ ë¶„ë¥˜
    ifc_files = list(downloaded_dir.glob("*.ifc"))
    bcf_files = list(downloaded_dir.glob("*.bcfzip"))
    
    print(f"  ğŸ“Š ë°œê²¬ëœ íŒŒì¼:")
    print(f"    â€¢ IFC íŒŒì¼: {len(ifc_files)}ê°œ")
    print(f"    â€¢ BCF íŒŒì¼: {len(bcf_files)}ê°œ")
    print(f"    â€¢ ì´ íŒŒì¼: {len(ifc_files) + len(bcf_files)}ê°œ")
    
    # íŒŒì¼ í¬ê¸° ë¶„ì„
    total_size = sum(f.stat().st_size for f in ifc_files + bcf_files)
    print(f"  ğŸ’¾ ì´ í¬ê¸°: {total_size / (1024*1024):.1f} MB")
    
    # BCF íŒŒì¼ë³„ ì´ìŠˆ ìˆ˜ ë¶„ì„
    bcf_analysis = {}
    total_bcf_issues = 0
    
    for bcf_file in bcf_files[:10]:  # ì²˜ìŒ 10ê°œë§Œ ìƒ˜í”Œ ë¶„ì„
        try:
            issues = parse_bcf_zip(bcf_file)
            bcf_analysis[bcf_file.name] = len(issues)
            total_bcf_issues += len(issues)
        except Exception as e:
            print(f"    âš ï¸ {bcf_file.name} íŒŒì‹± ì‹¤íŒ¨: {e}")
            bcf_analysis[bcf_file.name] = 0
    
    print(f"  ğŸ“‹ BCF ì´ìŠˆ ë¶„ì„ (ìƒ˜í”Œ 10ê°œ):")
    for filename, count in bcf_analysis.items():
        print(f"    â€¢ {filename}: {count}ê°œ ì´ìŠˆ")
    
    return {
        "ifc_files": len(ifc_files),
        "bcf_files": len(bcf_files),
        "total_files": len(ifc_files) + len(bcf_files),
        "total_size_mb": total_size / (1024*1024),
        "bcf_analysis": bcf_analysis,
        "estimated_total_bcf_issues": total_bcf_issues * len(bcf_files) // 10
    }


def restore_bcf_data():
    """BCF ë°ì´í„° ë³µêµ¬ ë° í†µí•©"""
    
    print("ğŸ”„ BCF ë°ì´í„° ë³µêµ¬ ì¤‘...")
    
    downloaded_dir = Path("data/raw/downloaded")
    output_dir = Path("data/processed/restored_bcf")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # BCF íŒŒì¼ ì²˜ë¦¬
    bcf_files = list(downloaded_dir.glob("*.bcfzip"))
    all_bcf_data = []
    
    for i, bcf_file in enumerate(bcf_files):
        print(f"  ğŸ“„ ì²˜ë¦¬ ì¤‘: {bcf_file.name} ({i+1}/{len(bcf_files)})")
        
        try:
            issues = parse_bcf_zip(bcf_file)
            
            for issue in issues:
                # ë©”íƒ€ë°ì´í„° ì¶”ê°€
                issue["source_file"] = bcf_file.name
                issue["source_type"] = "downloaded"
                issue["restored_date"] = datetime.now().isoformat()
                
                all_bcf_data.append(issue)
                
        except Exception as e:
            print(f"    âš ï¸ {bcf_file.name} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            continue
    
    # ë³µêµ¬ëœ BCF ë°ì´í„° ì €ì¥
    output_file = output_dir / "restored_bcf_data.jsonl"
    write_jsonl(str(output_file), all_bcf_data)
    
    print(f"  âœ… ë³µêµ¬ ì™„ë£Œ: {len(all_bcf_data)}ê°œ BCF ì´ìŠˆ")
    print(f"  ğŸ“ ì €ì¥ ìœ„ì¹˜: {output_file}")
    
    return {
        "total_issues": len(all_bcf_data),
        "output_file": str(output_file),
        "source_files": len(bcf_files)
    }


def restore_ifc_data():
    """IFC ë°ì´í„° ë³µêµ¬ ë° ë¶„ì„"""
    
    print("ğŸ”„ IFC ë°ì´í„° ë³µêµ¬ ì¤‘...")
    
    downloaded_dir = Path("data/raw/downloaded")
    output_dir = Path("data/processed/restored_ifc")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # IFC íŒŒì¼ ë¶„ì„
    ifc_files = list(downloaded_dir.glob("*.ifc"))
    ifc_analysis = []
    
    for i, ifc_file in enumerate(ifc_files[:50]):  # ì²˜ìŒ 50ê°œë§Œ ìƒ˜í”Œ ë¶„ì„
        print(f"  ğŸ“„ ë¶„ì„ ì¤‘: {ifc_file.name} ({i+1}/50)")
        
        try:
            # íŒŒì¼ í¬ê¸°ì™€ ê¸°ë³¸ ì •ë³´ë§Œ ìˆ˜ì§‘
            file_info = {
                "filename": ifc_file.name,
                "size_bytes": ifc_file.stat().st_size,
                "size_mb": ifc_file.stat().st_size / (1024*1024),
                "source_type": "downloaded",
                "restored_date": datetime.now().isoformat()
            }
            
            # IFC íŒŒì¼ì˜ ì²« ëª‡ ì¤„ì„ ì½ì–´ì„œ ë²„ì „ ì •ë³´ ì¶”ì¶œ
            with open(ifc_file, 'r', encoding='utf-8', errors='ignore') as f:
                first_lines = [f.readline().strip() for _ in range(10)]
                
            # IFC ë²„ì „ ì¶”ì¶œ
            for line in first_lines:
                if line.startswith("ISO-10303-21"):
                    file_info["ifc_version"] = line
                    break
                elif "IFC" in line.upper():
                    file_info["ifc_version"] = line
                    break
            
            ifc_analysis.append(file_info)
            
        except Exception as e:
            print(f"    âš ï¸ {ifc_file.name} ë¶„ì„ ì‹¤íŒ¨: {e}")
            continue
    
    # IFC ë¶„ì„ ê²°ê³¼ ì €ì¥
    output_file = output_dir / "ifc_analysis.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(ifc_analysis, f, indent=2, ensure_ascii=False)
    
    print(f"  âœ… ë¶„ì„ ì™„ë£Œ: {len(ifc_analysis)}ê°œ IFC íŒŒì¼")
    print(f"  ğŸ“ ì €ì¥ ìœ„ì¹˜: {output_file}")
    
    return {
        "analyzed_files": len(ifc_analysis),
        "total_files": len(ifc_files),
        "output_file": str(output_file)
    }


def update_sources_json():
    """sources.json ì—…ë°ì´íŠ¸"""
    
    print("ğŸ“ sources.json ì—…ë°ì´íŠ¸ ì¤‘...")
    
    sources_file = Path("data/sources.json")
    
    # ê¸°ì¡´ sources.json ì½ê¸°
    if sources_file.exists():
        with open(sources_file, 'r', encoding='utf-8') as f:
            sources = json.load(f)
    else:
        sources = {"ifc_files": [], "bcf_files": []}
    
    # ë³µêµ¬ëœ ë°ì´í„° ì •ë³´ ì¶”ê°€
    restored_bcf_info = {
        "path": "data/processed/restored_bcf/restored_bcf_data.jsonl",
        "name": "Restored BCF Data",
        "description": "ë³µêµ¬ëœ buildingSMART ë° ê¸°íƒ€ BCF í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤",
        "type": "restored_bcf",
        "source_type": "downloaded",
        "data_quality": "high",
        "added_date": datetime.now().isoformat(),
        "usage": "primary_training_data"
    }
    
    restored_ifc_info = {
        "path": "data/processed/restored_ifc/ifc_analysis.json",
        "name": "Restored IFC Data",
        "description": "ë³µêµ¬ëœ OpenBIM IDS, IfcOpenShell ë° ê¸°íƒ€ IFC íŒŒì¼",
        "type": "restored_ifc",
        "source_type": "downloaded",
        "data_quality": "high",
        "added_date": datetime.now().isoformat(),
        "usage": "primary_training_data"
    }
    
    # ê¸°ì¡´ í•­ëª©ê³¼ ì¤‘ë³µë˜ì§€ ì•Šë„ë¡ ì¶”ê°€
    existing_paths = {item.get("path", "") for item in sources.get("bcf_files", [])}
    
    if restored_bcf_info["path"] not in existing_paths:
        sources["bcf_files"].append(restored_bcf_info)
    
    if restored_ifc_info["path"] not in existing_paths:
        sources["ifc_files"].append(restored_ifc_info)
    
    # ì—…ë°ì´íŠ¸ëœ sources.json ì €ì¥
    with open(sources_file, 'w', encoding='utf-8') as f:
        json.dump(sources, f, indent=2, ensure_ascii=False)
    
    print(f"  âœ… sources.json ì—…ë°ì´íŠ¸ ì™„ë£Œ")
    
    return sources


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    print("ğŸš€ ê¸°ì¡´ ìˆ˜ì§‘ ë°ì´í„° ë³µêµ¬ ì‹œì‘")
    print("=" * 50)
    
    # 1. ë‹¤ìš´ë¡œë“œëœ ë°ì´í„° ë¶„ì„
    analysis = analyze_downloaded_data()
    if not analysis:
        return
    
    print("\n" + "=" * 50)
    
    # 2. BCF ë°ì´í„° ë³µêµ¬
    bcf_result = restore_bcf_data()
    
    print("\n" + "=" * 50)
    
    # 3. IFC ë°ì´í„° ë³µêµ¬
    ifc_result = restore_ifc_data()
    
    print("\n" + "=" * 50)
    
    # 4. sources.json ì—…ë°ì´íŠ¸
    sources = update_sources_json()
    
    print("\n" + "=" * 50)
    print("ğŸ‰ ë°ì´í„° ë³µêµ¬ ì™„ë£Œ!")
    print("\nğŸ“Š ë³µêµ¬ ê²°ê³¼ ìš”ì•½:")
    print(f"  â€¢ BCF ì´ìŠˆ: {bcf_result['total_issues']}ê°œ")
    print(f"  â€¢ IFC íŒŒì¼: {ifc_result['total_files']}ê°œ (ë¶„ì„: {ifc_result['analyzed_files']}ê°œ)")
    print(f"  â€¢ ì´ íŒŒì¼: {analysis['total_files']}ê°œ")
    print(f"  â€¢ ì´ í¬ê¸°: {analysis['total_size_mb']:.1f} MB")
    
    # ë³µêµ¬ ê²°ê³¼ë¥¼ JSONìœ¼ë¡œ ì €ì¥
    result_summary = {
        "restoration_date": datetime.now().isoformat(),
        "analysis": analysis,
        "bcf_result": bcf_result,
        "ifc_result": ifc_result,
        "sources_updated": True
    }
    
    summary_file = Path("data/analysis/restoration_summary.json")
    summary_file.parent.mkdir(parents=True, exist_ok=True)
    
    with open(summary_file, 'w', encoding='utf-8') as f:
        json.dump(result_summary, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ“ ìƒì„¸ ê²°ê³¼: {summary_file}")


if __name__ == "__main__":
    main()

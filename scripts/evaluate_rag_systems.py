#!/usr/bin/env python3
"""
RAG ì‹œìŠ¤í…œ í‰ê°€ ìŠ¤í¬ë¦½íŠ¸
BM25, Vector RAG, ContextualForget ë¹„êµ í‰ê°€
"""

import json
import time
from pathlib import Path
from typing import Dict, List, Any
import pandas as pd
from datetime import datetime

# Add src to path
import sys
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

from contextualforget.baselines.bm25_engine import BM25QueryEngine
from contextualforget.baselines.vector_engine import VectorQueryEngine
from contextualforget.query.advanced_query import AdvancedQueryEngine


class RAGEvaluator:
    """RAG ì‹œìŠ¤í…œ í‰ê°€ í´ë˜ìŠ¤"""
    
    def __init__(self, graph_path: str = "data/processed/graph.gpickle"):
        self.graph_path = graph_path
        self.engines = {}
        self.results = []
        
    def initialize_engines(self):
        """ëª¨ë“  RAG ì—”ì§„ ì´ˆê¸°í™”"""
        print("ğŸ”§ RAG ì—”ì§„ë“¤ ì´ˆê¸°í™” ì¤‘...")
        
        # Load graph data
        import pickle
        with open(self.graph_path, 'rb') as f:
            graph = pickle.load(f)
        
        # Extract data for engines
        bcf_data = []
        ifc_data = []
        
        for node, data in graph.nodes(data=True):
            if node[0] == "BCF":
                bcf_data.append(data)
            elif node[0] == "IFC":
                ifc_data.append(data)
        
        graph_data = {
            'bcf_data': bcf_data,
            'ifc_data': ifc_data,
            'graph': graph
        }
        
        # Initialize BM25 engine with sample data for testing
        print("  ğŸ“š BM25 ì—”ì§„ ì´ˆê¸°í™”...")
        # Remove existing index to force rebuild
        import shutil
        bm25_index_dir = Path("data/processed/bm25_index")
        if bm25_index_dir.exists():
            shutil.rmtree(bm25_index_dir)
        
        self.engines['BM25'] = BM25QueryEngine()
        # Use sample data for BM25 testing
        sample_data = {
            'bcf_data': [
                {
                    'guid': 'bcf_sample_001',
                    'topic_id': 'bcf_sample_001', 
                    'title': 'Sample BCF Issue',
                    'description': 'This is a sample BCF issue for testing',
                    'author': 'test_user',
                    'created': '2025-01-01T00:00:00Z',
                    'status': 'open',
                    'priority': 'medium',
                    'assigned_to': 'engineer_1',
                    'viewpoint_guid': '1kTvXnbbzCWw8lcMd1dR4o',
                    'topic_guid': 'bcf_sample_001',
                    'comment': 'Initial issue report',
                    'modified_date': '2025-01-01T00:00:00Z'
                }
            ]
        }
        self.engines['BM25'].initialize(sample_data)
        
        # Initialize Vector engine with actual graph data
        print("  ğŸ§  Vector ì—”ì§„ ì´ˆê¸°í™”...")
        self.engines['Vector'] = VectorQueryEngine()
        self.engines['Vector'].initialize(graph_data)
        
        # Initialize ContextualForget engine
        print("  ğŸ¯ ContextualForget ì—”ì§„ ì´ˆê¸°í™”...")
        self.engines['ContextualForget'] = AdvancedQueryEngine(graph)
        
        print("âœ… ëª¨ë“  ì—”ì§„ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def create_test_queries(self) -> List[Dict[str, Any]]:
        """í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ìƒì„±"""
        return [
            {
                "id": "q1",
                "question": "ë¬´ê· ì‹¤ ê´€ë ¨ ì´ìŠˆê°€ ìˆë‚˜ìš”?",
                "type": "general",
                "expected_entities": ["ë¬´ê· ì‹¤", "ë§ˆê°"],
                "expected_guid": None
            },
            {
                "id": "q2", 
                "question": "vvqxIxPgyNdRG6WySVJWDPì™€ ê´€ë ¨ëœ ë¬¸ì œëŠ”?",
                "type": "guid",
                "expected_entities": ["vvqxIxPgyNdRG6WySVJWDP"],
                "expected_guid": "vvqxIxPgyNdRG6WySVJWDP"
            },
            {
                "id": "q3",
                "question": "ìµœê·¼ 7ì¼ê°„ ë°œìƒí•œ ì´ìŠˆëŠ”?",
                "type": "temporal",
                "expected_entities": [],
                "expected_guid": None
            },
            {
                "id": "q4",
                "question": "engineer_aê°€ ì‘ì„±í•œ ì´ìŠˆëŠ”?",
                "type": "author",
                "expected_entities": ["engineer_a"],
                "expected_guid": None
            },
            {
                "id": "q5",
                "question": "ë§ˆê° ê´€ë ¨ ë¬¸ì œì ì€?",
                "type": "general",
                "expected_entities": ["ë§ˆê°", "ì‚¬ì–‘"],
                "expected_guid": None
            }
        ]
    
    def evaluate_engine(self, engine_name: str, query: Dict[str, Any]) -> Dict[str, Any]:
        """ë‹¨ì¼ ì—”ì§„ í‰ê°€"""
        start_time = time.time()
        
        try:
            if engine_name == "ContextualForget":
                # ContextualForgetëŠ” ë‹¤ë¥¸ ì¸í„°í˜ì´ìŠ¤ ì‚¬ìš©
                if query["type"] == "guid" and query["expected_guid"]:
                    results = self.engines[engine_name].find_by_guid(query["expected_guid"])
                elif query["type"] == "author":
                    author = query["expected_entities"][0] if query["expected_entities"] else ""
                    results = self.engines[engine_name].find_by_author(author)
                else:
                    results = self.engines[engine_name].find_by_keywords(
                        query["expected_entities"]
                    )
                
                answer = f"Found {len(results)} results"
                confidence = 0.8 if results else 0.0
                
            else:
                # BM25ì™€ Vector ì—”ì§„
                result = self.engines[engine_name].process_query(query["question"])
                answer = result.get("answer", "")
                confidence = result.get("confidence", 0.0)
                results = result.get("details", {})
            
            response_time = time.time() - start_time
            
            return {
                "engine": engine_name,
                "query_id": query["id"],
                "question": query["question"],
                "answer": answer,
                "confidence": confidence,
                "response_time": response_time,
                "results_count": len(results) if isinstance(results, list) else results.get("total_results", 0),
                "success": True,
                "error": None
            }
            
        except Exception as e:
            response_time = time.time() - start_time
            return {
                "engine": engine_name,
                "query_id": query["id"],
                "question": query["question"],
                "answer": "",
                "confidence": 0.0,
                "response_time": response_time,
                "results_count": 0,
                "success": False,
                "error": str(e)
            }
    
    def run_evaluation(self) -> pd.DataFrame:
        """ì „ì²´ í‰ê°€ ì‹¤í–‰"""
        print("ğŸš€ RAG ì‹œìŠ¤í…œ í‰ê°€ ì‹œì‘...")
        
        # ì—”ì§„ ì´ˆê¸°í™”
        self.initialize_engines()
        
        # í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ìƒì„±
        test_queries = self.create_test_queries()
        
        # ê° ì—”ì§„ë³„ë¡œ í‰ê°€ ì‹¤í–‰
        all_results = []
        
        for query in test_queries:
            print(f"\nğŸ“ ì¿¼ë¦¬ í‰ê°€: {query['question']}")
            
            for engine_name in self.engines.keys():
                print(f"  ğŸ” {engine_name} ì—”ì§„ í‰ê°€ ì¤‘...")
                result = self.evaluate_engine(engine_name, query)
                all_results.append(result)
                
                if result["success"]:
                    print(f"    âœ… ì„±ê³µ - ì‹ ë¢°ë„: {result['confidence']:.3f}, ì‘ë‹µì‹œê°„: {result['response_time']:.3f}s")
                else:
                    print(f"    âŒ ì‹¤íŒ¨ - ì˜¤ë¥˜: {result['error']}")
        
        # ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
        df = pd.DataFrame(all_results)
        
        # ê²°ê³¼ ì €ì¥
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = f"evaluation_results_{timestamp}.csv"
        df.to_csv(output_file, index=False)
        
        print(f"\nğŸ“Š í‰ê°€ ê²°ê³¼ ì €ì¥: {output_file}")
        
        return df
    
    def generate_report(self, df: pd.DataFrame) -> str:
        """í‰ê°€ ë³´ê³ ì„œ ìƒì„±"""
        report = []
        report.append("# RAG ì‹œìŠ¤í…œ í‰ê°€ ë³´ê³ ì„œ")
        report.append(f"í‰ê°€ ì¼ì‹œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append("")
        
        # ì „ì²´ í†µê³„
        report.append("## ì „ì²´ í†µê³„")
        report.append("")
        
        for engine in df['engine'].unique():
            engine_df = df[df['engine'] == engine]
            success_rate = engine_df['success'].mean() * 100
            avg_confidence = engine_df[engine_df['success']]['confidence'].mean()
            avg_response_time = engine_df['response_time'].mean()
            
            report.append(f"### {engine}")
            report.append(f"- ì„±ê³µë¥ : {success_rate:.1f}%")
            report.append(f"- í‰ê·  ì‹ ë¢°ë„: {avg_confidence:.3f}")
            report.append(f"- í‰ê·  ì‘ë‹µì‹œê°„: {avg_response_time:.3f}ì´ˆ")
            report.append("")
        
        # ì¿¼ë¦¬ë³„ ìƒì„¸ ê²°ê³¼
        report.append("## ì¿¼ë¦¬ë³„ ìƒì„¸ ê²°ê³¼")
        report.append("")
        
        for query_id in df['query_id'].unique():
            query_df = df[df['query_id'] == query_id]
            question = query_df.iloc[0]['question']
            
            report.append(f"### {query_id}: {question}")
            report.append("")
            
            for _, row in query_df.iterrows():
                status = "âœ…" if row['success'] else "âŒ"
                report.append(f"- **{row['engine']}** {status}")
                if row['success']:
                    report.append(f"  - ì‹ ë¢°ë„: {row['confidence']:.3f}")
                    report.append(f"  - ì‘ë‹µì‹œê°„: {row['response_time']:.3f}ì´ˆ")
                    report.append(f"  - ê²°ê³¼ ìˆ˜: {row['results_count']}")
                else:
                    report.append(f"  - ì˜¤ë¥˜: {row['error']}")
                report.append("")
        
        return "\n".join(report)


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    evaluator = RAGEvaluator()
    
    # í‰ê°€ ì‹¤í–‰
    results_df = evaluator.run_evaluation()
    
    # ë³´ê³ ì„œ ìƒì„±
    report = evaluator.generate_report(results_df)
    
    # ë³´ê³ ì„œ ì €ì¥
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    report_file = f"evaluation_report_{timestamp}.md"
    
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(report)
    
    print(f"ğŸ“‹ í‰ê°€ ë³´ê³ ì„œ ì €ì¥: {report_file}")
    
    # ê°„ë‹¨í•œ ìš”ì•½ ì¶œë ¥
    print("\n" + "="*50)
    print("ğŸ“Š í‰ê°€ ìš”ì•½")
    print("="*50)
    
    for engine in results_df['engine'].unique():
        engine_df = results_df[results_df['engine'] == engine]
        success_rate = engine_df['success'].mean() * 100
        avg_confidence = engine_df[engine_df['success']]['confidence'].mean()
        
        print(f"{engine:15} | ì„±ê³µë¥ : {success_rate:5.1f}% | í‰ê· ì‹ ë¢°ë„: {avg_confidence:.3f}")


if __name__ == "__main__":
    main()

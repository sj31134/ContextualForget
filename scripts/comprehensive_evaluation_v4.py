"""
ì¢…í•© í‰ê°€ ì‹œìŠ¤í…œ v4 - ê°œì„  ë²„ì „
- ì¤‘ê°„ ì €ì¥ ê¸°ëŠ¥
- ì§„í–‰ ìƒí™© ë¡œê¹…
- ì¬ê°œ ê¸°ëŠ¥
- ì˜ˆìƒ ì™„ë£Œ ì‹œê°„ í‘œì‹œ
"""

import json
import pickle
import sys
import time
from datetime import datetime, timedelta
from pathlib import Path
from typing import Any, Dict, List
import numpy as np
from tqdm import tqdm

# í”„ë¡œì íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.insert(0, str(Path(__file__).parent.parent / 'src'))

from contextualforget.query.contextual_forget_engine import ContextualForgetEngine
from contextualforget.query.adaptive_retrieval import HybridRetrievalEngine
from contextualforget.baselines.bm25_engine import BM25QueryEngine
from contextualforget.baselines.vector_engine import VectorQueryEngine
from contextualforget.core.utils import read_jsonl

# metrics_v2 ì„í¬íŠ¸
sys.path.insert(0, str(Path(__file__).parent.parent / 'eval'))
from metrics_v2 import EvaluationMetricsV2


class ComprehensiveEvaluatorV4:
    """ì¢…í•© RAG ì‹œìŠ¤í…œ í‰ê°€ì v4 - ê°œì„  ë²„ì „"""
    
    def __init__(self, graph_path: str, gold_standard_path: str, output_dir: str):
        self.graph_path = graph_path
        self.gold_standard_path = gold_standard_path
        self.output_dir = Path(output_dir)
        self.graph = None
        self.engines = {}
        self.gold_standard = []
        self.results = []
        
        # ì¤‘ê°„ ì €ì¥ ì„¤ì •
        self.checkpoint_interval = 10  # 10ë²ˆë§ˆë‹¤ ì €ì¥
        self.checkpoint_file = self.output_dir / 'checkpoint.json'
        self.progress_file = self.output_dir / 'progress.log'
        
        # ì„±ëŠ¥ ì¶”ì 
        self.start_time = None
        self.evaluation_times = []
        
    def load_graph(self):
        """ê·¸ë˜í”„ ë¡œë“œ"""
        print("ğŸ“Š ê·¸ë˜í”„ ë¡œë“œ ì¤‘...")
        with open(self.graph_path, 'rb') as f:
            self.graph = pickle.load(f)
        print(f"âœ… ê·¸ë˜í”„ ë¡œë“œ ì™„ë£Œ: {self.graph.number_of_nodes():,}ê°œ ë…¸ë“œ, {self.graph.number_of_edges():,}ê°œ ì—£ì§€")
    
    def load_gold_standard(self, limit: int = None):
        """Gold Standard ë¡œë“œ"""
        print("\nğŸ“‹ Gold Standard ë¡œë“œ ì¤‘...")
        all_data = list(read_jsonl(self.gold_standard_path))
        
        if limit and limit > 0:
            self.gold_standard = all_data[:limit]
            print(f"âœ… Gold Standard ë¡œë“œ ì™„ë£Œ: {len(self.gold_standard)}ê°œ QA (ì „ì²´ {len(all_data)}ê°œ ì¤‘ ìƒ˜í”Œ)")
        else:
            self.gold_standard = all_data
            print(f"âœ… Gold Standard ë¡œë“œ ì™„ë£Œ: {len(self.gold_standard)}ê°œ QA")
    
    def initialize_engines(self):
        """ëª¨ë“  ì—”ì§„ ì´ˆê¸°í™”"""
        print("\nğŸ”§ ì—”ì§„ë“¤ ì´ˆê¸°í™” ì¤‘...")
        
        # ë°ì´í„° ì¶”ì¶œ
        bcf_data = []
        ifc_data = []
        for node, data in self.graph.nodes(data=True):
            if isinstance(node, tuple):
                if node[0] == 'BCF':
                    bcf_data.append(data)
                elif node[0] == 'IFC':
                    ifc_data.append(data)
        
        print(f"   ğŸ“‹ BCF ë°ì´í„°: {len(bcf_data)}ê°œ")
        print(f"   ğŸ“‹ IFC ë°ì´í„°: {len(ifc_data)}ê°œ")
        
        # BM25 ì—”ì§„
        print("   ğŸ” BM25 ì—”ì§„ ì´ˆê¸°í™”...")
        bm25_engine = BM25QueryEngine()
        bm25_engine.initialize({'bcf_data': bcf_data, 'ifc_data': ifc_data, 'graph': self.graph})
        self.engines['BM25'] = bm25_engine
        
        # Vector ì—”ì§„
        print("   ğŸ” Vector ì—”ì§„ ì´ˆê¸°í™”...")
        vector_engine = VectorQueryEngine()
        vector_engine.initialize({
            'bcf_data': bcf_data,
            'ifc_data': ifc_data,
            'graph': self.graph
        })
        self.engines['Vector'] = vector_engine
        
        # ContextualForget ì—”ì§„
        print("   ğŸ” ContextualForget ì—”ì§„ ì´ˆê¸°í™”...")
        contextual_engine = ContextualForgetEngine(
            self.graph, 
            enable_contextual_forgetting=True
        )
        self.engines['ContextualForget'] = contextual_engine
        
        # í•˜ì´ë¸Œë¦¬ë“œ ì—”ì§„
        print("   ğŸ” í•˜ì´ë¸Œë¦¬ë“œ ì—”ì§„ ì´ˆê¸°í™”...")
        hybrid_engine = HybridRetrievalEngine(
            base_engines={
                'BM25': bm25_engine,
                'Vector': vector_engine,
                'ContextualForget': contextual_engine
            },
            fusion_strategy='basic',
            enable_adaptation=False
        )
        self.engines['Hybrid'] = hybrid_engine
        
        print("âœ… ëª¨ë“  ì—”ì§„ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def evaluate_single_query(self, engine_name: str, qa: Dict[str, Any]) -> Dict[str, Any]:
        """ë‹¨ì¼ ì¿¼ë¦¬ í‰ê°€"""
        engine = self.engines[engine_name]
        question = qa['question']
        
        try:
            # ì¿¼ë¦¬ ì‹¤í–‰
            start_time = time.perf_counter()
            
            if engine_name == 'ContextualForget':
                result = engine.contextual_query(question)
            elif engine_name == 'Hybrid':
                result = engine.query(question)
            else:
                result = engine.process_query(question)
            
            response_time = max(time.perf_counter() - start_time, 0.0001)
            
            # í‘œì¤€ í˜•ì‹ ê²€ì¦ ë° ë³´ì •
            if 'entities' not in result:
                result['entities'] = []
                if 'details' in result and 'results' in result['details']:
                    for doc in result['details']['results']:
                        entity_id = doc.get('doc_id') or doc.get('topic_id') or doc.get('guid', '')
                        if entity_id:
                            result['entities'].append(entity_id)
            
            if 'result_count' not in result:
                result['result_count'] = len(result.get('entities', []))
            
            if 'confidence' not in result:
                result['confidence'] = 0.0
            
            # ë©”íŠ¸ë¦­ ê³„ì‚°
            metrics = EvaluationMetricsV2.compute_metrics(result, qa)
            metrics['response_time'] = response_time
            
            return {
                'qa_id': qa.get('id', ''),
                'question': question,
                'category': qa.get('category', 'unknown'),
                'engine': engine_name,
                'result': result,
                'metrics': metrics,
                'status': 'success'
            }
            
        except Exception as e:
            return {
                'qa_id': qa.get('id', ''),
                'question': question,
                'category': qa.get('category', 'unknown'),
                'engine': engine_name,
                'result': {},
                'metrics': {
                    'success': False,
                    'precision': 0.0,
                    'recall': 0.0,
                    'f1': 0.0,
                    'ndcg@10': 0.0,
                    'mrr': 0.0,
                    'confidence': 0.0,
                    'response_time': 0.0,
                    'result_count': 0
                },
                'status': 'error',
                'error': str(e)
            }
    
    def save_checkpoint(self):
        """ì¤‘ê°„ ì €ì¥"""
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        checkpoint_data = {
            'results': self.results,
            'completed_evaluations': len(self.results),
            'timestamp': datetime.now().isoformat()
        }
        
        with open(self.checkpoint_file, 'w') as f:
            json.dump(checkpoint_data, f, indent=2, ensure_ascii=False, default=str)
    
    def load_checkpoint(self) -> bool:
        """ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ"""
        if self.checkpoint_file.exists():
            with open(self.checkpoint_file, 'r') as f:
                checkpoint_data = json.load(f)
                self.results = checkpoint_data['results']
                print(f"ğŸ“‚ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ: {len(self.results)}ê°œ í‰ê°€ ì™„ë£Œë¨")
                return True
        return False
    
    def log_progress(self, current: int, total: int, avg_time: float):
        """ì§„í–‰ ìƒí™© ë¡œê¹…"""
        remaining = total - current
        eta_seconds = remaining * avg_time
        eta = timedelta(seconds=int(eta_seconds))
        
        log_message = (
            f"[{datetime.now().strftime('%H:%M:%S')}] "
            f"ì§„í–‰: {current}/{total} ({current/total*100:.1f}%) | "
            f"í‰ê·  ì†ë„: {avg_time:.2f}ì´ˆ/í‰ê°€ | "
            f"ì˜ˆìƒ ì™„ë£Œ: {eta}\n"
        )
        
        print(log_message.strip())
        
        # íŒŒì¼ì—ë„ ì €ì¥
        with open(self.progress_file, 'a') as f:
            f.write(log_message)
    
    def evaluate_all(self, resume: bool = False):
        """ì „ì²´ í‰ê°€ ì‹¤í–‰"""
        print("\n" + "="*60)
        print("ğŸ“Š ì „ì²´ í‰ê°€ ì‹œì‘")
        print("="*60)
        print(f"   Gold Standard: {len(self.gold_standard)}ê°œ")
        print(f"   ì—”ì§„: {len(self.engines)}ê°œ")
        print(f"   ì´ í‰ê°€ íšŸìˆ˜: {len(self.gold_standard) * len(self.engines)}íšŒ")
        
        # ì¬ê°œ ì—¬ë¶€ í™•ì¸
        start_idx = 0
        if resume and self.load_checkpoint():
            start_idx = len(self.results) // len(self.engines)
            print(f"   â–¶ï¸  {start_idx}ë²ˆì§¸ ì§ˆë¬¸ë¶€í„° ì¬ê°œ")
        
        print()
        
        total_evaluations = len(self.gold_standard) * len(self.engines)
        self.start_time = time.time()
        self.evaluation_times = []
        
        with tqdm(total=total_evaluations, initial=len(self.results), desc="í‰ê°€ ì§„í–‰") as pbar:
            for qa_idx, qa in enumerate(self.gold_standard[start_idx:], start=start_idx):
                for engine_name in self.engines.keys():
                    eval_start = time.time()
                    
                    result = self.evaluate_single_query(engine_name, qa)
                    self.results.append(result)
                    
                    # ì‹œê°„ ì¶”ì 
                    eval_time = time.time() - eval_start
                    self.evaluation_times.append(eval_time)
                    
                    pbar.update(1)
                    
                    # ì¤‘ê°„ ì €ì¥
                    if len(self.results) % self.checkpoint_interval == 0:
                        self.save_checkpoint()
                
                # ì§„í–‰ ìƒí™© ë¡œê¹… (ë§¤ 5ê°œ QAë§ˆë‹¤)
                if (qa_idx + 1) % 5 == 0:
                    avg_time = np.mean(self.evaluation_times[-20:]) if self.evaluation_times else 0
                    self.log_progress(len(self.results), total_evaluations, avg_time)
        
        # ìµœì¢… ì €ì¥
        self.save_checkpoint()
        print("\nâœ… ì „ì²´ í‰ê°€ ì™„ë£Œ")
    
    def analyze_results(self):
        """ê²°ê³¼ ë¶„ì„"""
        print("\n" + "="*60)
        print("ğŸ“ˆ ê²°ê³¼ ë¶„ì„")
        print("="*60)
        
        # ì—”ì§„ë³„ ì§‘ê³„
        engine_stats = {}
        
        for engine_name in self.engines.keys():
            engine_results = [r for r in self.results if r['engine'] == engine_name]
            
            if not engine_results:
                continue
            
            # ë©”íŠ¸ë¦­ ì§‘ê³„
            metrics_list = [r['metrics'] for r in engine_results]
            aggregated = EvaluationMetricsV2.aggregate_metrics(metrics_list)
            
            # ì„±ê³µ/ì‹¤íŒ¨ ì¹´ìš´íŠ¸
            success_count = sum(1 for r in engine_results if r['status'] == 'success')
            error_count = sum(1 for r in engine_results if r['status'] == 'error')
            
            engine_stats[engine_name] = {
                'total_queries': len(engine_results),
                'success_count': success_count,
                'error_count': error_count,
                'aggregated_metrics': aggregated
            }
        
        # ì¶œë ¥
        print("\nì—”ì§„ë³„ ì„±ëŠ¥:")
        print("-" * 60)
        
        for engine_name, stats in engine_stats.items():
            print(f"\n{engine_name}:")
            print(f"  ì„±ê³µ: {stats['success_count']}/{stats['total_queries']}")
            print(f"  ì˜¤ë¥˜: {stats['error_count']}/{stats['total_queries']}")
            
            if 'aggregated_metrics' in stats and stats['aggregated_metrics']:
                agg = stats['aggregated_metrics']
                print(f"  ì„±ê³µë¥ : {agg.get('success_rate_rate', 0.0):.2%}")
                print(f"  í‰ê·  F1: {agg.get('avg_f1', 0.0):.3f}")
                print(f"  í‰ê·  Precision: {agg.get('avg_precision', 0.0):.3f}")
                print(f"  í‰ê·  Recall: {agg.get('avg_recall', 0.0):.3f}")
                print(f"  í‰ê·  NDCG@10: {agg.get('avg_ndcg@10', 0.0):.3f}")
                print(f"  í‰ê·  MRR: {agg.get('avg_mrr', 0.0):.3f}")
                print(f"  í‰ê·  ì‹ ë¢°ë„: {agg.get('avg_confidence', 0.0):.3f}")
                print(f"  í‰ê·  ì‘ë‹µì‹œê°„: {agg.get('avg_response_time', 0.0):.4f}ì´ˆ")
        
        return engine_stats
    
    def save_results(self):
        """ê²°ê³¼ ì €ì¥"""
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥ ì¤‘: {self.output_dir}")
        
        # ìƒì„¸ ê²°ê³¼ ì €ì¥
        detailed_file = self.output_dir / 'evaluation_v4_detailed.json'
        with open(detailed_file, 'w') as f:
            json.dump(self.results, f, indent=2, ensure_ascii=False, default=str)
        print(f"   âœ… ìƒì„¸ ê²°ê³¼: {detailed_file}")
        
        # ìš”ì•½ ê²°ê³¼
        engine_stats = self.analyze_results()
        summary_file = self.output_dir / 'evaluation_v4_summary.json'
        with open(summary_file, 'w') as f:
            json.dump(engine_stats, f, indent=2, ensure_ascii=False, default=str)
        print(f"   âœ… ìš”ì•½ ê²°ê³¼: {summary_file}")
        
        # CSV ë¹„êµ í…Œì´ë¸”
        import csv
        csv_file = self.output_dir / 'evaluation_v4_comparison.csv'
        with open(csv_file, 'w', newline='') as f:
            writer = csv.writer(f)
            writer.writerow(['Engine', 'Success Rate', 'F1', 'Precision', 'Recall', 
                           'NDCG@10', 'MRR', 'Confidence', 'Response Time (s)'])
            
            for engine_name, stats in engine_stats.items():
                if 'aggregated_metrics' in stats and stats['aggregated_metrics']:
                    agg = stats['aggregated_metrics']
                    writer.writerow([
                        engine_name,
                        f"{agg.get('success_rate_rate', 0.0):.2%}",
                        f"{agg.get('avg_f1', 0.0):.3f}",
                        f"{agg.get('avg_precision', 0.0):.3f}",
                        f"{agg.get('avg_recall', 0.0):.3f}",
                        f"{agg.get('avg_ndcg@10', 0.0):.3f}",
                        f"{agg.get('avg_mrr', 0.0):.3f}",
                        f"{agg.get('avg_confidence', 0.0):.3f}",
                        f"{agg.get('avg_response_time', 0.0):.4f}"
                    ])
        print(f"   âœ… CSV ë¹„êµ: {csv_file}")
        
        # ì²´í¬í¬ì¸íŠ¸ ì‚­ì œ
        if self.checkpoint_file.exists():
            self.checkpoint_file.unlink()
            print(f"   ğŸ—‘ï¸  ì²´í¬í¬ì¸íŠ¸ ì‚­ì œ")


def main():
    import argparse
    
    parser = argparse.ArgumentParser(description='ì¢…í•© í‰ê°€ ì‹œìŠ¤í…œ v4 - ê°œì„  ë²„ì „')
    parser.add_argument('--graph', default='data/processed/graph.gpickle', help='ê·¸ë˜í”„ íŒŒì¼')
    parser.add_argument('--gold-standard', default='eval/gold_standard_v3.jsonl', help='Gold Standard íŒŒì¼')
    parser.add_argument('--output', default='results/evaluation_v4', help='ì¶œë ¥ ë””ë ‰í† ë¦¬')
    parser.add_argument('--limit', type=int, default=None, help='ìƒ˜í”Œ í¬ê¸° ì œí•œ (í…ŒìŠ¤íŠ¸ìš©)')
    parser.add_argument('--resume', action='store_true', help='ì¤‘ë‹¨ëœ í‰ê°€ ì¬ê°œ')
    args = parser.parse_args()
    
    print("\n" + "ğŸ¯ " + "="*58)
    print("   ì¢…í•© í‰ê°€ ì‹œìŠ¤í…œ v4 - ê°œì„  ë²„ì „")
    print("="*60)
    
    # í‰ê°€ì ì´ˆê¸°í™”
    evaluator = ComprehensiveEvaluatorV4(args.graph, args.gold_standard, args.output)
    
    # ë°ì´í„° ë¡œë“œ
    evaluator.load_graph()
    evaluator.load_gold_standard(limit=args.limit)
    
    # ì—”ì§„ ì´ˆê¸°í™”
    evaluator.initialize_engines()
    
    # í‰ê°€ ì‹¤í–‰
    evaluator.evaluate_all(resume=args.resume)
    
    # ê²°ê³¼ ë¶„ì„ ë° ì €ì¥
    evaluator.save_results()
    
    total_time = time.time() - evaluator.start_time
    print("\n" + "="*60)
    print(f"âœ… ì¢…í•© í‰ê°€ v4 ì™„ë£Œ!")
    print(f"   ì´ ì†Œìš” ì‹œê°„: {timedelta(seconds=int(total_time))}")
    print("="*60)


if __name__ == "__main__":
    main()


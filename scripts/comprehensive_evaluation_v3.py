"""
ì¢…í•© í‰ê°€ ì‹œìŠ¤í…œ v3
200ê°œ Gold Standard QAë¡œ 4ê°œ ì—”ì§„ 800íšŒ í‰ê°€
"""

import json
import pickle
import sys
import time
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List
import numpy as np
from tqdm import tqdm

# í”„ë¡œì íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.insert(0, str(Path(__file__).parent.parent / 'src'))

from contextualforget.query.contextual_forget_engine import ContextualForgetEngine
from contextualforget.query.adaptive_retrieval import HybridRetrievalEngine
from contextualforget.baselines.bm25_engine import BM25QueryEngine
from contextualforget.baselines.vector_engine import VectorQueryEngine
from contextualforget.core.utils import read_jsonl

# metrics_v2 ì„í¬íŠ¸
sys.path.insert(0, str(Path(__file__).parent.parent / 'eval'))
from metrics_v2 import EvaluationMetricsV2


class ComprehensiveEvaluatorV3:
    """ì¢…í•© RAG ì‹œìŠ¤í…œ í‰ê°€ì v3"""
    
    def __init__(self, graph_path: str, gold_standard_path: str):
        self.graph_path = graph_path
        self.gold_standard_path = gold_standard_path
        self.graph = None
        self.engines = {}
        self.gold_standard = []
        self.results = []
        
    def load_graph(self):
        """ê·¸ë˜í”„ ë¡œë“œ"""
        print("ğŸ“Š ê·¸ë˜í”„ ë¡œë“œ ì¤‘...")
        with open(self.graph_path, 'rb') as f:
            self.graph = pickle.load(f)
        print(f"âœ… ê·¸ë˜í”„ ë¡œë“œ ì™„ë£Œ: {self.graph.number_of_nodes():,}ê°œ ë…¸ë“œ, {self.graph.number_of_edges():,}ê°œ ì—£ì§€")
    
    def load_gold_standard(self):
        """Gold Standard ë¡œë“œ"""
        print("\nğŸ“‹ Gold Standard ë¡œë“œ ì¤‘...")
        self.gold_standard = list(read_jsonl(self.gold_standard_path))
        print(f"âœ… Gold Standard ë¡œë“œ ì™„ë£Œ: {len(self.gold_standard)}ê°œ QA")
    
    def initialize_engines(self):
        """ëª¨ë“  ì—”ì§„ ì´ˆê¸°í™”"""
        print("\nğŸ”§ ì—”ì§„ë“¤ ì´ˆê¸°í™” ì¤‘...")
        
        # ë°ì´í„° ì¶”ì¶œ
        bcf_data = []
        ifc_data = []
        for node, data in self.graph.nodes(data=True):
            if isinstance(node, tuple):
                if node[0] == 'BCF':
                    bcf_data.append(data)
                elif node[0] == 'IFC':
                    ifc_data.append(data)
        
        print(f"   ğŸ“‹ BCF ë°ì´í„°: {len(bcf_data)}ê°œ")
        print(f"   ğŸ“‹ IFC ë°ì´í„°: {len(ifc_data)}ê°œ")
        
        # BM25 ì—”ì§„
        print("   ğŸ” BM25 ì—”ì§„ ì´ˆê¸°í™”...")
        bm25_engine = BM25QueryEngine()
        bm25_engine.initialize({'bcf_data': bcf_data, 'ifc_data': ifc_data, 'graph': self.graph})
        self.engines['BM25'] = bm25_engine
        
        # Vector ì—”ì§„
        print("   ğŸ” Vector ì—”ì§„ ì´ˆê¸°í™”...")
        vector_engine = VectorQueryEngine()
        vector_engine.initialize({
            'bcf_data': bcf_data,
            'ifc_data': ifc_data,
            'graph': self.graph
        })
        self.engines['Vector'] = vector_engine
        
        # ContextualForget ì—”ì§„
        print("   ğŸ” ContextualForget ì—”ì§„ ì´ˆê¸°í™”...")
        contextual_engine = ContextualForgetEngine(
            self.graph, 
            enable_contextual_forgetting=True
        )
        self.engines['ContextualForget'] = contextual_engine
        
        # í•˜ì´ë¸Œë¦¬ë“œ ì—”ì§„
        print("   ğŸ” í•˜ì´ë¸Œë¦¬ë“œ ì—”ì§„ ì´ˆê¸°í™”...")
        hybrid_engine = HybridRetrievalEngine(
            base_engines={
                'BM25': bm25_engine,
                'Vector': vector_engine,
                'ContextualForget': contextual_engine
            },
            fusion_strategy='basic',
            enable_adaptation=False
        )
        self.engines['Hybrid'] = hybrid_engine
        
        print("âœ… ëª¨ë“  ì—”ì§„ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def evaluate_single_query(self, engine_name: str, qa: Dict[str, Any]) -> Dict[str, Any]:
        """ë‹¨ì¼ ì¿¼ë¦¬ í‰ê°€"""
        engine = self.engines[engine_name]
        question = qa['question']
        
        try:
            # ì¿¼ë¦¬ ì‹¤í–‰
            start_time = time.perf_counter()
            
            if engine_name == 'ContextualForget':
                result = engine.contextual_query(question)
            elif engine_name == 'Hybrid':
                result = engine.query(question)
            else:
                result = engine.process_query(question)
            
            response_time = max(time.perf_counter() - start_time, 0.0001)
            
            # í‘œì¤€ í˜•ì‹ ê²€ì¦ ë° ë³´ì •
            if 'entities' not in result:
                # detailsë‚˜ resultsì—ì„œ entities ì¶”ì¶œ ì‹œë„
                result['entities'] = []
                if 'details' in result and 'results' in result['details']:
                    for doc in result['details']['results']:
                        entity_id = doc.get('doc_id') or doc.get('topic_id') or doc.get('guid', '')
                        if entity_id:
                            result['entities'].append(entity_id)
            
            if 'result_count' not in result:
                result['result_count'] = len(result.get('entities', []))
            
            if 'confidence' not in result:
                result['confidence'] = 0.0
            
            # ë©”íŠ¸ë¦­ ê³„ì‚°
            metrics = EvaluationMetricsV2.compute_metrics(result, qa)
            metrics['response_time'] = response_time
            
            return {
                'qa_id': qa.get('id', ''),
                'question': question,
                'category': qa.get('category', 'unknown'),
                'engine': engine_name,
                'result': result,
                'metrics': metrics,
                'status': 'success'
            }
            
        except Exception as e:
            print(f"      âš ï¸  {engine_name} ì˜¤ë¥˜: {str(e)[:50]}")
            return {
                'qa_id': qa.get('id', ''),
                'question': question,
                'category': qa.get('category', 'unknown'),
                'engine': engine_name,
                'result': {},
                'metrics': {
                    'success': False,
                    'precision': 0.0,
                    'recall': 0.0,
                    'f1': 0.0,
                    'ndcg@10': 0.0,
                    'mrr': 0.0,
                    'confidence': 0.0,
                    'response_time': 0.0,
                    'result_count': 0
                },
                'status': 'error',
                'error': str(e)
            }
    
    def evaluate_all(self):
        """ì „ì²´ í‰ê°€ ì‹¤í–‰"""
        print("\n" + "="*60)
        print("ğŸ“Š ì „ì²´ í‰ê°€ ì‹œì‘")
        print("="*60)
        print(f"   Gold Standard: {len(self.gold_standard)}ê°œ")
        print(f"   ì—”ì§„: {len(self.engines)}ê°œ")
        print(f"   ì´ í‰ê°€ íšŸìˆ˜: {len(self.gold_standard) * len(self.engines)}íšŒ")
        print()
        
        total_evaluations = len(self.gold_standard) * len(self.engines)
        
        with tqdm(total=total_evaluations, desc="í‰ê°€ ì§„í–‰") as pbar:
            for qa in self.gold_standard:
                for engine_name in self.engines.keys():
                    result = self.evaluate_single_query(engine_name, qa)
                    self.results.append(result)
                    pbar.update(1)
        
        print("\nâœ… ì „ì²´ í‰ê°€ ì™„ë£Œ")
    
    def analyze_results(self):
        """ê²°ê³¼ ë¶„ì„"""
        print("\n" + "="*60)
        print("ğŸ“ˆ ê²°ê³¼ ë¶„ì„")
        print("="*60)
        
        # ì—”ì§„ë³„ ì§‘ê³„
        engine_stats = {}
        
        for engine_name in self.engines.keys():
            engine_results = [r for r in self.results if r['engine'] == engine_name]
            
            if not engine_results:
                continue
            
            # ë©”íŠ¸ë¦­ ì§‘ê³„
            metrics_list = [r['metrics'] for r in engine_results]
            aggregated = EvaluationMetricsV2.aggregate_metrics(metrics_list)
            
            # ì„±ê³µ/ì‹¤íŒ¨ ì¹´ìš´íŠ¸
            success_count = sum(1 for r in engine_results if r['status'] == 'success')
            error_count = sum(1 for r in engine_results if r['status'] == 'error')
            
            engine_stats[engine_name] = {
                'total_queries': len(engine_results),
                'success_count': success_count,
                'error_count': error_count,
                'aggregated_metrics': aggregated
            }
        
        # ì¶œë ¥
        print("\nì—”ì§„ë³„ ì„±ëŠ¥:")
        print("-" * 60)
        
        for engine_name, stats in engine_stats.items():
            print(f"\n{engine_name}:")
            print(f"  ì„±ê³µ: {stats['success_count']}/{stats['total_queries']}")
            print(f"  ì˜¤ë¥˜: {stats['error_count']}/{stats['total_queries']}")
            
            if 'aggregated_metrics' in stats and stats['aggregated_metrics']:
                agg = stats['aggregated_metrics']
                print(f"  ì„±ê³µë¥ : {agg.get('success_rate_rate', 0.0):.2%}")
                print(f"  í‰ê·  F1: {agg.get('avg_f1', 0.0):.3f}")
                print(f"  í‰ê·  Precision: {agg.get('avg_precision', 0.0):.3f}")
                print(f"  í‰ê·  Recall: {agg.get('avg_recall', 0.0):.3f}")
                print(f"  í‰ê·  NDCG@10: {agg.get('avg_ndcg@10', 0.0):.3f}")
                print(f"  í‰ê·  MRR: {agg.get('avg_mrr', 0.0):.3f}")
                print(f"  í‰ê·  ì‹ ë¢°ë„: {agg.get('avg_confidence', 0.0):.3f}")
                print(f"  í‰ê·  ì‘ë‹µì‹œê°„: {agg.get('avg_response_time', 0.0):.4f}ì´ˆ")
        
        return engine_stats
    
    def save_results(self, output_dir: str):
        """ê²°ê³¼ ì €ì¥"""
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥ ì¤‘: {output_dir}")
        
        # ìƒì„¸ ê²°ê³¼ ì €ì¥
        detailed_file = output_path / 'evaluation_v3_detailed.json'
        with open(detailed_file, 'w') as f:
            json.dump(self.results, f, indent=2, ensure_ascii=False, default=str)
        print(f"   âœ… ìƒì„¸ ê²°ê³¼: {detailed_file}")
        
        # ìš”ì•½ ê²°ê³¼
        engine_stats = self.analyze_results()
        summary_file = output_path / 'evaluation_v3_summary.json'
        with open(summary_file, 'w') as f:
            json.dump(engine_stats, f, indent=2, ensure_ascii=False, default=str)
        print(f"   âœ… ìš”ì•½ ê²°ê³¼: {summary_file}")
        
        # CSV ë¹„êµ í…Œì´ë¸”
        import csv
        csv_file = output_path / 'evaluation_v3_comparison.csv'
        with open(csv_file, 'w', newline='') as f:
            writer = csv.writer(f)
            writer.writerow(['Engine', 'Success Rate', 'F1', 'Precision', 'Recall', 
                           'NDCG@10', 'MRR', 'Confidence', 'Response Time (s)'])
            
            for engine_name, stats in engine_stats.items():
                if 'aggregated_metrics' in stats and stats['aggregated_metrics']:
                    agg = stats['aggregated_metrics']
                    writer.writerow([
                        engine_name,
                        f"{agg.get('success_rate_rate', 0.0):.2%}",
                        f"{agg.get('avg_f1', 0.0):.3f}",
                        f"{agg.get('avg_precision', 0.0):.3f}",
                        f"{agg.get('avg_recall', 0.0):.3f}",
                        f"{agg.get('avg_ndcg@10', 0.0):.3f}",
                        f"{agg.get('avg_mrr', 0.0):.3f}",
                        f"{agg.get('avg_confidence', 0.0):.3f}",
                        f"{agg.get('avg_response_time', 0.0):.4f}"
                    ])
        print(f"   âœ… CSV ë¹„êµ: {csv_file}")


def main():
    import argparse
    
    parser = argparse.ArgumentParser(description='ì¢…í•© í‰ê°€ ì‹œìŠ¤í…œ v3')
    parser.add_argument('--graph', default='data/processed/graph.gpickle', help='ê·¸ë˜í”„ íŒŒì¼')
    parser.add_argument('--gold-standard', default='eval/gold_standard_v3.jsonl', help='Gold Standard íŒŒì¼')
    parser.add_argument('--output', default='results/evaluation_v3', help='ì¶œë ¥ ë””ë ‰í† ë¦¬')
    args = parser.parse_args()
    
    print("\n" + "ğŸ¯ " + "="*58)
    print("   ì¢…í•© í‰ê°€ ì‹œìŠ¤í…œ v3")
    print("="*60)
    
    # í‰ê°€ì ì´ˆê¸°í™”
    evaluator = ComprehensiveEvaluatorV3(args.graph, args.gold_standard)
    
    # ë°ì´í„° ë¡œë“œ
    evaluator.load_graph()
    evaluator.load_gold_standard()
    
    # ì—”ì§„ ì´ˆê¸°í™”
    evaluator.initialize_engines()
    
    # í‰ê°€ ì‹¤í–‰
    evaluator.evaluate_all()
    
    # ê²°ê³¼ ë¶„ì„ ë° ì €ì¥
    evaluator.save_results(args.output)
    
    print("\n" + "="*60)
    print("âœ… ì¢…í•© í‰ê°€ v3 ì™„ë£Œ!")
    print("="*60)


if __name__ == "__main__":
    main()

